#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass book
\begin_preamble
\bibpunct{(}{)}{,}{a}{,}{,}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage[font=small]{caption}
\usepackage{colortbl}
\usepackage{enumitem}
\usepackage{caption}




\date{}
%\fancyhf{}               % Clear fancy header/footer
%\fancyhead[R]{Submitted: Conceptual Foundations of Language Science}
%\fancyhead[L]{7/6/18}  
%\fancyfoot[R]{\thepage}
%\makeatletter

\setenumerate[0]{label=(\alph*)}

\renewcommand{\multirowsetup}{\centering}
\setlength{\textfloatsep}{10pt plus 1.0pt minus 2.0pt}
\end_preamble
\use_default_options true
\begin_modules
linguistics
\end_modules
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "times" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family rmdefault
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement H
\paperfontsize 12
\spacing onehalf
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plainnat
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 1in
\topmargin 1in
\rightmargin 1in
\bottommargin 1in
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle plain
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
Representational Considerations in Models of Language Change and Stability
\end_layout

\begin_layout Author
Rebecca L.
 Morley
\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Chapter
Introduction
\end_layout

\begin_layout Standard
Synchronic and diachronic linguistics are typically pursued as separate
 disciplines, with little to no overlap.
 Nevertheless, it is not possible for either to be truly agnostic about
 the form of the other.
 This is necessarily true because synchronic theories are not theories about
 attested languages, but theories about possible languages.
 Therefore, any possible language, undergoing any series of diachronic changes,
 must always end up as another member of the set of possible synchronic
 languages.
 Conversely, a theory of the end state of diachronic change is necessarily
 a theory of a synchronic grammar at some point in time.
 The actuators of change must also be latently present in some way within
 synchronic states, just as the speakers of daughter languages must have
 been learners of mother languages.
\end_layout

\begin_layout Standard
In this work I will demonstrate how deeply held assumptions about the correct
 representations of synchronic grammars delimit an associated theory of
 diachrony, and how standard assumptions about the units of change disallow
 certain synchronic states.
 I will argue that it is necessary to reconsider the operative units within
 both domains, and that in doing so we are likely to gain new insights into
 how linguistic structures change and, by extension, how they fail to change,
 i.e., exhibit stable variation.
\end_layout

\begin_layout Standard
The goal of this work is to determine what types of mental structures may
 be sufficient, and possibly, necessary in order to capture certain linguistic
 phenomena at a computational level of description (in the sense of 
\begin_inset CommandInset citation
LatexCommand citet
key "marr1982vision"

\end_inset

).
 The approach is twofold: 1) to test a number of proposed structures and
 mechanisms by implementing them in simple computational models; and 2)
 to test the explanatory adequacy of a number of existing models by transforming
 their implementations into theoretical constructs.
 The first method is likely to be familiar to many readers; the second,
 however, is somewhat novel, and requires some explanation.
 In the simplest terms, it is the reverse of the first: taking implemented
 functions and deriving the theoretical linguistic entity that the function
 implements.
 This requires determining whether a specific implementational detail is
 incidental (with no repercussions beyond the implementational level), or
 whether there are hidden ramifications to that choice at the level of linguisti
c theory (the computational level).
 There is a second aspect to this analysis as well.
 Models, to be useful, as well as tractable, must be simplifications of
 what are extremely complex systems.
 The simplifications, however, must be of the right kind to ensure that
 the model is still informative about the phenomenon of interest.
 That is, the model must be able to 
\begin_inset Quotes eld
\end_inset

scale up
\begin_inset Quotes erd
\end_inset

.
 There is no algorithm, however, by which we can establish scalability ahead
 of time in building our toy models.
 Thus, it is critical that any model be made consistent with what is known
 more globally about the phenomenon of interest (what I will call imposing
 boundary conditions), and not just the small piece it was designed to explain.
 
\end_layout

\begin_layout Standard
As a series of models are developed and tested, they will be assessed as
 to whether or not they meet the relevant boundary conditions in an internally
 consistent, theoretically motivated way.
 This higher-level model analysis will reveal the covert representational
 corollaries of various modeling choices, providing insight, in turn, into
 both sufficient and necessary components of a working theory of language
 stability and change.
 This approach is also an illustration of the utility of an intensively
 fine-grained local analysis in approaching the largest and most general
 of theoretical questions.
 Although the phenomena modeled are phonetic and phonological ones, the
 methodology is applicable to any domain of linguistics.
 
\end_layout

\begin_layout Standard
This book is organized as follows.
 In the remainder of this chapter the representational issues that apply
 in both the synchronic and the diachronic domains are introduced.
 Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:The-Exemplar-Model"

\end_inset

 describes the basic architecture from which the models are built.
 To begin with, three general types of phenomena are modeled: a gradient
 context-free process, a gradient context-dependent process, and a categorical
 context-dependent process.
 Simulations for all three demonstrate that iterated processes without check
 lead to collapse, or unbounded category shift.
 Furthermore, production modeled as random selection of unnormalized perceptual
 inputs leads to sub-category mismatch.
 Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:The-Linguistic-Phenomena"

\end_inset

 makes explicit links between these general model types and specific linguistic
 phenomena, namely, word frequency effects, vowel lengthening, and vowel
 nasalization.
 In Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Models-of-Change"

\end_inset

, articulatory targets are introduced to the basic model in order to check
 unbounded shift.
 A set of models with targets of various kinds are analyzed in depth.
 The set is generated by selecting parameters along two dimensions: whether
 production tokens are stored or generated (
\noun on
state/process
\noun default
); and whether more than one level of representation is used (category/sub-categ
ory).
 In this chapter it is shown that only two models from this set satisfy
 the criteria of being both representationally consistent and bounded.
 The possible states for each of the two models is then fully derived.
 These results are related to existing models, and the types of sound change
 that they are capable of capturing.
 In Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Perception-Production"

\end_inset

 it is shown that the typical implementational simplification, in which
 perception and production tokens are equated, is not only implausible,
 but obscures a fundamental flaw in the mechanism for change.
 Iterative change no longer follows once an explicit mapping between acoustic
 values and articulatory gestures is required.
 Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Phoneme-Split"

\end_inset

 is devoted to a type of change no previously modeled: the genesis of a
 new phoneme category.
 Adopting a theory in which the mapping from perception to production is
 taken to be inherently ambiguous, I offer a proposal for an implemented
 model in which variable sub-lexical segmentation results in mixed representatio
ns.
 Change in the model is taken to be change in the distribution of already
 existing variants.
 The work is summarized in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Summary-&-Conclusions"

\end_inset

, where other types of sound change, and future avenues of research are
 briefly discussed.
 
\end_layout

\begin_layout Section
Abstract Representations
\end_layout

\begin_layout Standard
One of the basic representational divisions that can be made in a theory
 of cognition is between what is stored in memory versus what is not stored,
 and thus, must be computed (or generated).
 The choice about what aspects of a linguistic pattern to treat as stored
 versus generated will determine, to quite a large extent, what we take
 to be the possible dimensions of synchronic variation cross-linguistically,
 as well as the possible diachronic outcomes.
 This will be the focus of Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Models-of-Change"

\end_inset

, where we will also see that this choice can imply a number of other representa
tional assumptions.
 In this section, we preview that analysis by deconstructing some of the
 most basic units of phonological theory.
\end_layout

\begin_layout Standard
It should be noted that mainstream synchronic linguistics is heavily biased
 towards conceptualizing phenomena as generating processes: 
\begin_inset Quotes eld
\end_inset

vowel nasalization
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

final de-voicing
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

initial aspiration
\begin_inset Quotes erd
\end_inset

, etc.
\begin_inset Foot
status open

\begin_layout Plain Layout
Even if these are merely terminological conveniences, they color the way
 we think about, and model, these phenomena.
\end_layout

\end_inset

.
 This is directly linked to a conception of mental representations as maximally
 abstract.
 In other words, only unpredictable information should be stored (such as
 the arbitrary sound units associated with a given lexical item), while
 all predictable information should be derived.
 Although this view may have originated with 
\begin_inset CommandInset citation
LatexCommand citet
key "Chomsky1968"

\end_inset

, it has also been explicitly advocated for much more recently in various
 theories of underspecification (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "archangeli1988aspects,Steriade1995a"

\end_inset

).
 More commonly, however, it is an unexpressed assumption that the analysis
 that maximizes the predictive power of the grammar is the preferred one
\begin_inset Foot
status open

\begin_layout Plain Layout
Within Optimality Theory, this pressure is, in a sense, even stronger, because
 all possible words must be filtered through the grammar (not just the selected
 URs).
 However, Lexicon Optimization allows for known lexical items to be generated
 from faithful inputs allowing for some predictability to be retained in
 the lexicon (
\begin_inset CommandInset citation
LatexCommand citealt
key "Prince2004"

\end_inset

: Ch.
 9).
 
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
For example, the pronunciation of the word 
\begin_inset Quotes eld
\end_inset

lamb
\begin_inset Quotes erd
\end_inset

 in English can be written with the following series of phonetic symbols:
 
\begin_inset IPA

\begin_layout Standard
[læ̃m]
\end_layout

\end_inset

, where the diacritic over the vowel indicates nasalization.
 The property of nasalization, however, is predictable in English, and only
 occurs when vowels are produced in proximity to nasal consonants, like
 [m].
 The lexical entry for 
\begin_inset Quotes eld
\end_inset

lamb
\begin_inset Quotes erd
\end_inset

 is therefore denoted as 
\begin_inset IPA

\begin_layout Standard
/læm/
\end_layout

\end_inset

, without the nasalization.
 Concomitantly, a pronunciation rule must be internalized by the native
 English speaker, a rule that stipulates that any vowels adjacent to nasal
 consonants must become nasalized.
 Under this theory, the lexical item 
\begin_inset IPA

\begin_layout Standard
/læm/
\end_layout

\end_inset

 is first retrieved, and then transformed to 
\begin_inset IPA

\begin_layout Standard
[læ̃m]
\end_layout

\end_inset

 via the application of this rule.
 
\end_layout

\begin_layout Standard
This hypothesis in fact, implies that the lexical entry is comprised of
 a string of smaller units, the phonemes 
\begin_inset IPA

\begin_layout Standard
/l/
\end_layout

\end_inset

, 
\begin_inset IPA

\begin_layout Standard
/æ/
\end_layout

\end_inset

, and 
\begin_inset IPA

\begin_layout Standard
/m/
\end_layout

\end_inset

, that are concatenated together in order to produce the word.
 The currently standard view of phonological structure is that there exists
 an entire hierarchy of abstract units wherein larger units are successively
 built from smaller ones: phonemes from features, syllables from phonemes,
 words from syllables, etc.
 At each level, the units of the previous level undergo rules affecting
 their realization.
 The unit of interest in a particular analysis will depend on the phenomenon
 of interest.
 But that unit cannot exist independently of the rest of the hierarchy.
 Consider the dual nature of the phoneme 
\begin_inset IPA

\begin_layout Standard
/æ/
\end_layout

\end_inset

 as part of an abstract category 
\begin_inset IPA

\begin_layout Standard
/æ/
\end_layout

\end_inset

, but also as part of the word 
\begin_inset Quotes eld
\end_inset

lamb
\begin_inset Quotes erd
\end_inset

.
 The variant, or allophone, of the phoneme that occurs in that word is nasalized.
 However, the rule that nasalizes the 
\begin_inset IPA

\begin_layout Standard
/æ/
\end_layout

\end_inset

 is assumed to operate at a more abstract level, i.e., before any nasal, in
 any word and, in fact, to apply to any vowel.
 See (
\begin_inset CommandInset ref
LatexCommand ref
reference "allophony:nasal vowel"

\end_inset

).
 
\end_layout

\begin_layout Numbered Example (multiline)

\emph on
\begin_inset CommandInset label
LatexCommand label
name "allophony:nasal vowel"

\end_inset

/vowel/ 
\begin_inset Formula $\rightarrow$
\end_inset

 [nasalized vowel]/__ [nasal]
\end_layout

\begin_layout Standard
Many phonemes can be said to have multiple phonological allophones, and
 all phonemes have at least multiple phonetic allophones.
 In the word 
\begin_inset IPA

\begin_layout Standard
[tʰæˑɡ˺
\end_layout

\end_inset

] (
\begin_inset Quotes eld
\end_inset

tag
\begin_inset Quotes erd
\end_inset

), for example, the first sound can be characterized as the aspirated allophone
 of 
\begin_inset IPA

\begin_layout Standard
/t/
\end_layout

\end_inset

 that is generated whenever a voiceless plosive occurs in the onset of a
 stressed syllable; the second sound is the lengthened allophone of 
\begin_inset IPA

\begin_layout Standard
/æ/
\end_layout

\end_inset

 that is generated whenever a vowel precedes a voiced obstruent; and the
 third sound is the unreleased allophone of 
\begin_inset IPA

\begin_layout Standard
/ɡ/
\end_layout

\end_inset

 that is generated whenever a plosive occurs in word-final position.
 
\end_layout

\begin_layout Standard
A consequence of abstract representations that do not match produced surface
 forms is that a normalization procedure is required on the perception side
 for successful recognition and retrieval.
 The actually heard 
\begin_inset IPA

\begin_layout Standard
[læ̃m]
\end_layout

\end_inset

 does not match the stored representation 
\begin_inset IPA

\begin_layout Standard
/læm/
\end_layout

\end_inset

, and must be converted by somehow subtracting out, or 
\begin_inset Quotes eld
\end_inset

compensating
\begin_inset Quotes erd
\end_inset

 for, the predictable nasality.
 As far as I am aware, there is no standard notation for formalizing the
 input (or perception) side of the allophonic relationship.
 Therefore, I use the special symbol 
\begin_inset Formula $\hookrightarrow$
\end_inset

 to denote the inference of the underlying form in (
\begin_inset CommandInset ref
LatexCommand ref
reference "Normalization"

\end_inset

), the inverse of (
\begin_inset CommandInset ref
LatexCommand ref
reference "allophony:nasal vowel"

\end_inset

).
 
\end_layout

\begin_layout Numbered Examples (consecutive)
\begin_inset CommandInset label
LatexCommand label
name "Normalization"

\end_inset

 
\emph on
[[nasalized vowel][nasal]]
\emph default
 
\begin_inset Formula $\hookrightarrow$
\end_inset

 
\emph on
//vowel//nasal//
\end_layout

\begin_layout Standard
Once performed, the recovered form should be identical to the stored category.
 Thus, from the generative perspective, category matching is trivial, and
 the difficult part of speech recognition is the normalization process.
 Note that the more rules there are, and the more complex their interaction,
 the more complicated the normalization procedure becomes
\begin_inset Foot
status open

\begin_layout Plain Layout
The real speech perception problem, of course, is much more difficult than
 simply accounting for all the phonetic and phonological predictability.
 There are numerous other factors that affect the realization of a given
 utterance, such as vocal tract length, speaking rate, ambient noise, speaker
 sex, speech community, register, etc.
 At minimum, normalization of all these factors requires a complex non-linear
 function, and is unlikely to have a unique solution.
 
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Actuation-1"

\end_inset

Actuation 
\end_layout

\begin_layout Standard
A commonly described sound change is one in which a sound that was previously
 an allophone becomes a phoneme in its own right (phoneme split).
 Vowel nasalization is considered to be allophonic in English, and was also
 allophonic at some point in the history of French.
 The allophonic rule entailed that a word like 
\begin_inset IPA

\begin_layout Standard
/bɔn/
\end_layout

\end_inset

 would be pronounced as 
\begin_inset IPA

\begin_layout Standard
[bɔ̃n]
\end_layout

\end_inset

.
 According to the classical view, loss of nasal consonants like the one
 in 
\begin_inset IPA

\begin_layout Standard
[bɔ̃n]
\end_layout

\end_inset

, resulted in words like 
\begin_inset IPA

\begin_layout Standard
[bɔ̃]
\end_layout

\end_inset

, where the nasalized variant was no longer predictable (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "Hajek1997a"

\end_inset

).
 In theory, a minimal pair was now possible where the only difference between
 the word pairs was whether the vowel was oral or nasal, e.g., 
\begin_inset IPA

\begin_layout Standard
[bɔ̃]
\end_layout

\end_inset

 versus 
\begin_inset IPA

\begin_layout Standard
[bɔ]
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
This story creates a paradox within the constraints of the representational
 framework just described.
 If nasalization is predictable, then it is added by rule to an abstract
 underlying form, such as 
\begin_inset IPA

\begin_layout Standard
/bɔn/
\end_layout

\end_inset

.
 If the final nasal is dropped by the speaker, then there should be no nasalizat
ion on the vowel, and no way to arrive at a phonemically nasalized vowel
\begin_inset Foot
status open

\begin_layout Plain Layout
In fact, it is possible to achieve the necessary outcome if the nasal is
 dropped by the speaker only 
\emph on
after
\emph default
 the allophonic rule has been applied.
 This move, however, requires a theory of serially ordered rules in the
 first place, and, in the second, raises other difficulties in terms of
 theoretical constraints on the ordering of those rules, and what types
 of rules are allowed to occur before or after others.
\end_layout

\end_inset

.
 If the final nasal is not dropped by the speaker, but fails to be heard
 by the listener, a different problem arises.
 A listener provided with the input sequence 
\begin_inset IPA

\begin_layout Standard
[bɔ̃]
\end_layout

\end_inset

 ought to infer, based on their native language competence, that they failed
 to hear a nasal consonant that was actually produced, given that vowels
 are only ever nasalized preceding a nasal consonant.
 In fact, they ought to be able to infer, based on the conversational context
 and their knowledge of lexical items, that the target was 
\begin_inset Quotes eld
\end_inset

bon
\begin_inset Quotes erd
\end_inset

, and thus correct for any performance errors in production or perception.
\end_layout

\begin_layout Standard
The causality in this story can be reversed, where the loss of the nasal,
 rather than being the actuating event, merely reveals (to the linguist)
 that the nasalized vowel has already become phonemic (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "Janda2003"

\end_inset

).
 This `covert change' approach, however, merely pushes the explanation back
 a step – how did the vowel become phonemically nasal? And in either story
 the Actuation Problem (
\begin_inset CommandInset citation
LatexCommand citealt
key "Labov1968"

\end_inset

) remains unsolved.
 What is required is a mechanism by which predictability can be lost at
 the allophonic level.
 Furthermore, the mechanism itself must be predictable; that is, it must
 either always apply (yet only occasionally lead to sound change), or it
 must apply under specific well-understood conditions.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Less-abstract-Representations"

\end_inset

Less-abstract Representations
\end_layout

\begin_layout Standard
Even within a maximally abstract system it will be necessary to deal with
 multiple representational levels in a way that is obscured by the notational
 conventions used above.
 For example, a phoneme split was said to require predictability to be lost
 at the allophonic level.
 But, in fact, what is really needed is the loss of predictability at a
 hyper-allophonic level, such as that expressed in (
\begin_inset CommandInset ref
LatexCommand ref
reference "allophony:nasal vowel"

\end_inset

) – which will be symbolized as 
\begin_inset Formula $[\tilde{V}]$
\end_inset

 going forward.
 And because neither phonemes, allophones, or hyper-allophones exist in
 isolation, whatever mechanism is proposed must act through the medium of
 actual words.
 Furthermore, sound change has been observed to be gradual from a phonetic
 point of view, such that relatively small differences in pronunciation
 can be seen to incrementally increase across speakers of different ages
 in a 
\begin_inset Quotes eld
\end_inset

sound change in progress
\begin_inset Quotes erd
\end_inset

.
 These small differences are reflected in what may be stable differences
 between different dialects, between male and female speakers, between speakers
 of higher socioeconomic and lower socioeconomic status, etc..
 It is now widely accepted, in fact, that the pool of phonetic variants
 that exists across a heterogeneous population of speakers provides the
 basis for future sound changes (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "Guy2008"

\end_inset

).
\end_layout

\begin_layout Standard
For these reasons, an alternative framework in which mental representations
 are far closer to actually produced forms, retaining significant detail
 at both the acoustic and phonetic levels, has arisen in the study of sound
 change.
 Exemplar models were first developed in the field of psychology, in order
 to reflect a number of insights about human memory and categorization.
 Rather than having clear, definable boundaries, many mental categories
 seemed to function much more as though they were a reflection of their
 current members (e.g., 
\begin_inset CommandInset citation
LatexCommand citet
key "Rosch1977"

\end_inset

).
 Categorization of novel items was less a question of logical inference,
 than of similarity to known instances.
 Furthermore, many dimensions of similarity were potentially implicated,
 not all of which were relevant from a taxonomic point of view (
\begin_inset CommandInset citation
LatexCommand citealt
key "Nosofsky1988,Luce1986"

\end_inset

).
 Within linguistics, exemplar models have been invoked to account for a
 host of factors known to affect both word recognition and production, but
 which are not expressable within a maximally abstract generative framework.
 Among these are the pervasive effects of word frequency (
\begin_inset CommandInset citation
LatexCommand citealt
key "Bybee2001"

\end_inset

), the familiar-speaker effect in word priming, as well as the persistence
 of sub-phonemic detail (
\begin_inset CommandInset citation
LatexCommand citealt
key "tilsen2009subphonemic"

\end_inset

), and the influence of socio-indexical variables on what are typically
 assumed to be more abstract, grammatical levels of processing (see 
\begin_inset CommandInset citation
LatexCommand citealt
key "docherty2014evaluation"

\end_inset

 for review).
 
\end_layout

\begin_layout Standard
The term `exemplar' is meant to indicate that representations being stored
 in memory are of individual, specific experiences.
 For example, each time you hear the word 
\begin_inset Quotes eld
\end_inset

lamb
\begin_inset Quotes erd
\end_inset

 over the course of your lifetime, spoken by any of a number of different
 people, in any number of different contexts, an exemplar that resides within
 the category associated with the word 
\begin_inset Quotes eld
\end_inset

lamb
\begin_inset Quotes erd
\end_inset

 is created.
 Just as a minimal representational framework implies the necessity of a
 normalization procedure, a 
\begin_inset Quotes eld
\end_inset

maximal
\begin_inset Quotes erd
\end_inset

 representational framework suggests that normalization of the acoustic
 signal may not be necessary at all.
 Since previous experiences of the word 
\begin_inset Quotes eld
\end_inset

lamb
\begin_inset Quotes erd
\end_inset

 share many similarities, among them that fact that that there is some degree
 of nasalization on the vowel, they are likely to provide the closest matches
 to any new token that also contains a nasalized vowel of this type.
 No reversal of nasalization is required (cf.
 
\begin_inset CommandInset citation
LatexCommand citealt
key "Johnson1997a"

\end_inset

).
 Classification occurs by discovering the cloud to which a new token bears
 the closest over-all similarity in this space.
 However, because speech is so variable, in ways that listeners seem quite
 sensitive to, this mental space is a very high-dimensional one.
 As a result, the similarity computation is likely to be quite complex.
 
\end_layout

\begin_layout Standard
Because the relationship between the acoustic speech signal and the structural
 units of language is a non-linear, many-to-many mapping, there must always
 be a theoretical trade-off of this kind.
 For an easy classification algorithm, generative theory requires complex
 pre-processing in the form of a normalization procedure.
 For little, to no, pre-processing, exemplar theory requires a complex classific
ation algorithm.
 In the modeling work that follows we will adopt the non-trivial assumption
 that classification is perfect – that all tokens are recognized as members
 of their intended category.
 The complexity, however, will surface in the transformation between what
 is perceived (and subsequently stored), and what is produced (based on
 what is stored).
 Nominally, all the models in this work are exemplar models.
 However, they are really much more general-purpose models.
 In the limit, all tokens can belong to a single category, or all categories
 contain a single token each.
 The question of normalization will remain central because it depends on
 exactly how abstract the representations are, and there will always be
 a trade-off between what is stored and what is computed.
 
\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "sec:The-Exemplar-Model"

\end_inset

The Basic Model
\end_layout

\begin_layout Standard
One of the earliest exemplar models within linguistics, 
\begin_inset CommandInset citation
LatexCommand citet
key "Goldinger1996"

\end_inset

 (adapting 
\begin_inset CommandInset citation
LatexCommand citet
key "hintzman1984minerva"

\end_inset

), was designed to capture the effect of past experience on current perception.
 In this model, new tokens are experienced and added to memory in the following
 way.
 First, the n-dimensional similarity between a novel (`probe') token and
 all members of a given category, is calculated.
 The overall degree of similarity will determine whether a given probe is
 recognized or not.
 The similarity matrix is, in turn, used to create an `echo' of the probe:
 the average of the values of each stored token, along each dimension, weighted
 by its similarity to the probe.
 This echo, rather than the probe itself, is what is then added to memory.
 These properties allow the model to simulate the phenomenon whereby listeners
 often mistakenly `remember' tokens that are particularly `good', or prototypica
l, members of a category, even when they have never actually experienced
 those tokens.
 Goldinger's model also introduced a production component – a seemingly
 minimal extension in which a stored echo can be selected for `readout'.
 Goldinger is explicit about assuming that the articulations needed to produce
 a given auditory token can be accurately reconstructed from the acoustics
 of that token (and thus directly `read out' from the stored perception
 token).
 This assumption would be implicitly adopted in most of the work that followed.
 
\end_layout

\begin_layout Standard
The standard perception-production loop model, as well as the application
 to sound change 
\emph on
per se
\emph default
, appears to have originated with 
\begin_inset CommandInset citation
LatexCommand citet
key "Pierrehumbert2000"

\end_inset

.
 Production in this model starts with random selection from a store of perceived
 tokens.
 Each production, in turn, is then perceived (either by the original speaker,
 or by an interlocutor with an identical exemplar space) and then stored.
 Then the process begins again.
 In this way, small perturbations (noise or articulatory biases in production;
 perceptual biases or error in perception) accumulate in the exemplar cloud,
 leading to gradual shifts in the category as a whole.
 The perception-production loop that will form the basis for the models
 discussed in this book is schematized in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Feedback Loop"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename P-PLoop.pdf
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Feedback Loop"

\end_inset

Perception-Production Feedback Loop
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The basic exemplar model includes three additional mechanisms that are necessary
 for generating useful results.
 The first of these is what is typically conceptualized as an error term.
 This allows for variation to persist, and provides the necessary stochastic
 element needed for achieving multiple outcomes.
 The second is entrenchment, which prevents categories from losing cohesion
 and dispersing along the dimensions of variation.
 The third mechanism is memory decay, privileging more recent perceptions
 in memory, and preventing the category from simply getting larger and larger.
 Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Baseline-Model-Specification"

\end_inset

 is a schematic of the basic algorithm for the models that will be implemented
 and run below.
 Mathematical details will be provided in the following section and the
 Appendices.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Box Boxed
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
Baseline Perception-Production Model (one dimensional)
\end_layout

\begin_layout Enumerate
Initialize cloud
\end_layout

\begin_deeper
\begin_layout Itemize
assign values to a cloud of 
\emph on
n
\emph default
 tokens (randomly generated from a Normal distribution of mean 
\begin_inset Formula $\mu$
\end_inset

 and variance 
\begin_inset Formula $\sigma^{2}$
\end_inset

)
\end_layout

\begin_layout Itemize
assign each token an age (a time at which it was produced)
\end_layout

\end_deeper
\begin_layout Enumerate
\begin_inset CommandInset label
LatexCommand label
name "enu:step 2"

\end_inset

Randomly select a token for production
\end_layout

\begin_deeper
\begin_layout Itemize
add the production bias, moving the token a small amount in the biasing
 direction
\end_layout

\begin_layout Itemize
add the error term, moving the token a small amount in either direction
\end_layout

\begin_layout Itemize
add entrenchment, moving the token a small amount closer to the category
 mean 
\end_layout

\end_deeper
\begin_layout Enumerate
Store
\end_layout

\begin_deeper
\begin_layout Itemize
add the new token value to the cloud
\end_layout

\begin_layout Itemize
remove one of the oldest tokens from the cloud
\end_layout

\end_deeper
\begin_layout Enumerate
Repeat Step 
\begin_inset CommandInset ref
LatexCommand ref
reference "enu:step 2"

\end_inset


\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Baseline-Model-Specification"

\end_inset

Baseline Model Specification
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
Entrenchment
\end_layout

\begin_layout Standard
Category consolidation, or variance reduction, has been motivated as an
 effect of practice, or motor tuning (e.g., 
\begin_inset CommandInset citation
LatexCommand citealp
key "saltzman1989dynamical"

\end_inset

).
 Implementationally, it is necessary to prevent the category expansion in
 both directions that would result from consistent production error, and
 the additional expansion that would occur in the biasing direction.
 The general equation for entrenchment that will be used in this paper is
 the following (based on 
\begin_inset CommandInset citation
LatexCommand citet
key "Pierrehumbert2000"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E(x_{i})=\epsilon(\bar{x}-x_{i})\label{eq:Entrenchment}
\end{equation}

\end_inset

where 
\begin_inset Formula $\epsilon$
\end_inset

 is a constant between 0 and 1, 
\begin_inset Formula $x_{i}$
\end_inset

 is the current location of token 
\emph on
i
\emph default
 along some dimension 
\emph on
x
\emph default
, and 
\begin_inset Formula $\overline{x}$
\end_inset

 is the current category mean along that dimension.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:First Model param"

\end_inset

 illustrates the evolution of a single exemplar cloud generated from the
 model outlined in 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Baseline-Model-Specification"

\end_inset

.
 In each sub-figure the different colors indicate the same distribution
 at initialization (white), and after a certain fixed number of model iterations
 (black).
 Unless otherwise stated, all models are assumed to be one-dimensional along
 
\emph on
x
\emph default
.
 Individual tokens are given as counts over successively binned 
\emph on
x
\emph default
 values.
 
\end_layout

\begin_layout Standard
Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Basic-Iterative-Model"

\end_inset

 shows how the distribution as a whole shifts in the direction of the production
 bias over time (measured in iterations of the perception-production loop).
 Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:NoEntrenchment"

\end_inset

 shows the result of running the same model, minus the entrenchment term,
 over the same number of iterations.
 The biasing shift still occurs, but with increasing variance along the
 biased dimension.
 See Appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Appendix A"

\end_inset

 for the specific parameter values used in these, and the following, simulations.
 
\end_layout

\begin_layout Section
Memory Decay
\end_layout

\begin_layout Standard
Without memory decay, categories can only spread without shifting.
 The older the tokens, the more times, on average, they will have been chosen
 as production targets, reinforcing the initial conditions of the cloud.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:No-memory-decay:"

\end_inset

 is an illustration of this effect for the same model and starting conditions
 as the previous two simulations, but with the memory decay term removed.
 No tokens discarded.
 The skew in the direction of the production bias (implemented as an incremental
ly decreasing function) can be seen in the left tail of the distribution,
 but older tokens keep the category anchored at the right.
 There are a number of ways in which a memory decay term can be implemented.
 In these, and the following, models the total number of tokens is kept
 constant by removing one of the oldest tokens each time a new token is
 added
\begin_inset Foot
status open

\begin_layout Plain Layout
There are other ways to keep the number of category members constant.
 The token furthest from the mean could be discarded on each iteration,
 for example.
 This would act to increase the entrenchment effect, further reducing variation.
 However, the purpose here is not only to keep the number of tokens constant,
 but to allow a domino effect to develop by increasing the probability that
 a token will be chosen again with each biasing iteration.
 
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename 8000iterwithentrenchment.pdf
	width 25page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Basic-Iterative-Model"

\end_inset

All Forces
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename 8000iternoentrenchment.pdf
	width 25page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:NoEntrenchment"

\end_inset

Without Entrenchment
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename 8000iternomemory.pdf
	width 25page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:No-memory-decay:"

\end_inset

Without Memory Decay
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:First Model param"

\end_inset

Basic Iterative Model: Starting distribution (white); Distribution after
 8000 iterations (black).
 Note that y-axis range in c) is about 10 times larger than in a) and b).
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Iterativity"

\end_inset

The Collapse Problem
\end_layout

\begin_layout Standard
As illustrated in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Basic-Iterative-Model"

\end_inset

, the basic exemplar model with a single unidirectional bias can produce
 cohesive movement of an entire cloud of exemplars in the direction of the
 bias.
 What will be demonstrated in this section is that this shift is unbounded,
 leading ultimately to category collapse and merger.
 This could easily be inferred from the fact that the basic model contains
 only one force that acts in a consistent direction, with nothing to oppose
 it.
 However, it is worthwhile to actually run the simulations for a number
 of reasons.
 Unambiguously establishing the results for general classes of phenomena
 will allow us to see immediately what the model predicts for the linguistic
 phenomena that map to each of those classes.
 Running actual simulations will also force us to consider the question
 of whether exemplar models are to be evaluated only at convergence, and
 what the relationship is between model time and real time, in terms of
 experiences of instances of speech.
 Finally, the specific ways in which the models fail will be informative
 regarding the mental representations that these models are meant to map
 to.
 
\end_layout

\begin_layout Standard
The three classes of phenomena to be modeled in this section are the following:
 a context-free process, and two context-dependent processes: one gradient,
 and one categorical.
 For all of the three basic models, a production bias, 
\emph on
B
\emph default
, will be implemented for a given token 
\emph on
i
\emph default
, as a fixed percentage reduction (
\begin_inset Formula $\alpha$
\end_inset

) in the value of 
\emph on

\begin_inset Formula $x_{i}$
\end_inset


\emph default
 along dimension 
\emph on
x
\emph default
.
 See Eq.(
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Production Bias"

\end_inset

)
\begin_inset Foot
status open

\begin_layout Plain Layout
The production bias in 
\begin_inset CommandInset citation
LatexCommand citet
key "Pierrehumbert2000"

\end_inset

 is a constant that applies regardless of the current token value.
 Making the bias proportional results in less reduction for tokens that
 already have small values, fixing the percentage of reduction, rather than
 the absolute value, for all tokens.
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
B(x_{i})=-x_{i}\alpha\label{eq:Production Bias}
\end{equation}

\end_inset

After the production bias applies, the biased token will be added back to
 the cloud from which it was originally drawn.
 It will be useful to express the value of a given token on any iteration
 as a function of the original non-biased token that gave rise to it.
 For one such original token, 
\begin_inset Formula $x_{i}$
\end_inset

, we can label its biased daughter as 
\begin_inset Formula $x_{i(+1)}$
\end_inset

, and calculate its biased value to be 
\begin_inset Formula $x_{i}\left(1-\alpha\right)$
\end_inset

 along dimension 
\emph on
x
\emph default
.
 If, on some subsequent iteration, this daughter token 
\begin_inset Formula $x_{i(+1)}$
\end_inset

 is chosen for production, it will be subject to the same biasing force,
 resulting in the granddaughter, 
\begin_inset Formula $x_{i(+2)}$
\end_inset

, with value 
\begin_inset Formula $x_{i(+2)}=x_{i(+1)}\left(1-\alpha\right)=x_{i}\left(1-\alpha\right)^{2}$
\end_inset

.
 Proceeding to the general case, we can express the value of any token,
 on any given model iteration, as a function of the value of its originator
 token (
\begin_inset Formula $x_{o}$
\end_inset

), and the number of generations, 
\emph on
n
\emph default
, by which the current token is removed from that originator.
 Eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:linear bias"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
x_{o(+n)}=x_{o}\left(1-\alpha\right)^{n}\label{eq:linear bias}
\end{equation}

\end_inset


\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Model-1:-Context-Free"

\end_inset

Model 1: Context-Free Iterativity
\end_layout

\begin_layout Standard
In Model 1, the bias function applies to all tokens.
 Therefore, the linear bias term in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:linear bias"

\end_inset

) will cause the entire category to shift in the biasing direction over
 time.
 The following simulations compare the behavior of a low-frequency category,
 to a high-frequency category, one whose tokens are produced, and thus experienc
ed, more often.
 All simulations begin with the same starting distributions: a high-frequency
 category with 800 tokens, and a low-frequency category with 200 token.
 Steps were taken to make the distributions of the two categories as close
 as possible
\begin_inset Foot
status open

\begin_layout Plain Layout
The high-frequency category was generated by randomly sampling 800 tokens
 from a normal distribution with mean of 
\begin_inset Formula $50x$
\end_inset

 and a standard deviation of 
\begin_inset Formula $2x$
\end_inset

.
 The low-frequency category was then created by sampling 200 tokens from
 the high-frequency category: 50 tokens from each quartile.
\end_layout

\end_inset

.
 See Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Frequency Starting Dist"

\end_inset

.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename startCon.pdf
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Frequency Starting Dist"

\end_inset

Starting Distribution.
 White bars: High-frequency category.
 Black bars: Low-frequency category.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Because these models rely on random processes, the outcome is not guaranteed
 to be identical each time the model is run.
 To evaluate models of this kind, one conducts a number of independent identical
 `experiments' (trials) that consist of running the model with the same
 starting conditions, and the same parameters, for the same number of iterations.
 Results are then averaged over the set of trials.
 In the first set of simulations, 500 model trials were run for 1000 iterations
 each.
 On each model iteration one token was produced, selected stochastically
 from among all possible tokens (making it 4 times more likely to be chosen
 from the high-frequency than the low-frequency category).
 That token was biased according to Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Production Bias"

\end_inset

) and then added back to the category from which it originated.
 
\end_layout

\begin_layout Standard
The mean category value along 
\emph on
x
\emph default
 for each category was calculated at the end of each of the 500 trials,
 and converted to a z-score.
 A boxplot of plot of the difference between the means of the two categories
 on each trial is shown in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:HvL"

\end_inset

.
 In 44% of trials the difference was negative (low-frequency mean larger
 than high-frequency), and in 56% it was positive.
 The mean difference over all 500 trials was close to zero: .031 (or 16%
 of the initial distribution standard deviation).
 Thus we do not see a consistent difference in the two categories after
 an arbitrarily selected number of iterations.
 Intuitively, we might have expected the higher-frequency category to have
 moved further along 
\emph on
x
\emph default
, and have a lower value, because tokens from that category are produced
 more often, and thus, multiply-biased.
 However, it is also the case that, if frequency of occurrence is expressed
 in number of tokens, and sampling for production is random, then producing
 a token that had undergone biasing 
\emph on
fewer
\emph default
 times is also more likely in high, than low, frequency categories.
 This is simply because there are more tokens, which lowers the probability
 of selecting any individual token, and thus lowers the probability of selecting
 the daughter of any individual token, relative to the low-frequency category.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename FrequencyEffect.pdf
	width 30page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:HvL"

\end_inset

 4:1 Token ratio
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename FrequencyEffectEqNums.pdf
	width 30page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Freq.EqualTokens"

\end_inset

1:1 Token Ratio
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Simulation of Iterative Biasing for H(igh) frequency category versus L(ow)
 frequency category
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The difference in the number of tokens in each category also results in
 a difference in variance across the different trials.
 The variance is larger for the lower-frequency category due to undersampling;
 because fewer tokens are produced from the low-frequency category in a
 given trial, and the tokens are selected randomly, the likelihood that
 the sample will be significantly different from trial to trial is greater
 (
\begin_inset CommandInset citation
LatexCommand citet
key "Soskuthy"

\end_inset

 finds a similar effect using a parameterized exemplar model).
 Variance compounds over iterations, such that the variance between independent
 model runs after 10,000 iterations is greater than after 5000 iterations.
 Fig 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Frequency Catch Up"

\end_inset

 illustrates the across-trial variance for the two categories at successive
 intervals, after: 500, 1000, 1500, and 2000 iterations.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename FrequencyOverTime.pdf
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Frequency Catch Up"

\end_inset

Average z-scored value for High (white) versus Low (gray) frequency categories
 at 4 equally spaced intervals of model time, each 500 model iterations
 long.
 Each boxplot shows the results of 10 independent trials at each of the
 successive stages.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Different implementational choices and assumptions will produce somewhat
 different results.
 If the two categories contain the same constant number of tokens, but the
 high-frequency category is still 4 times more likely to be produced on
 any given iteration, then the high-frequency category will have a consistently
 lower value along 
\emph on
x
\emph default
 than the low-frequency category.
 See Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Freq.EqualTokens"

\end_inset

.
 This is because the inertia from the larger number of few-times-biased
 tokens is missing.
 These specific results also depend on the ratio of frequencies of the two
 categories, as well as a number of other parameter settings.
 Those dependencies will be discussed further in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Word-Frequency"

\end_inset

, when this model is linked to the linguistic phenomenon of frequency-based
 word reduction.
 For now, we turn to the behavior of the model in the limit.
\end_layout

\begin_layout Standard
The means of both categories steadily decrease as a function of the number
 of model iterations.
 Although the amount of biasing becomes steadily smaller as token values
 become smaller, biasing is unbounded.
 That is, the model does not converge on a stable state.
 Convergence can be imposed by specifying a minimum value on 
\emph on
x
\emph default
 beyond which tokens cannot be reduced.
 This results in a skewed distribution with a narrow peak at the threshold
 value and a small rightward tail due to the normally distributed error
 term.
 The thresholded model clearly illustrates that all categories, whether
 high or low frequency, will eventually end up at exactly the same minimum
 value, collapsing any difference between them.
 
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "sec:Context-Dependent-Iterativity"

\end_inset

Context-Dependent Iterativity
\end_layout

\begin_layout Standard
In the previous model all tokens of each category were subjected to the
 same production bias – the context in which the tokens were produced did
 not matter.
 The next two models are context-dependent models.
 In these models the production bias only applies to a subset of tokens,
 those produced in the biasing context.
 As before, production tokens are chosen at random; they are then produced
 in either a biasing or non-biasing context, with a certain fixed probability.
 Regardless of production context, however, all tokens are added back to
 the same originating category.
\end_layout

\begin_layout Subsubsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Phrase-Final Lengthening"

\end_inset

Model 2: Gradient Context-Dependent Bias
\end_layout

\begin_layout Standard
Model 2 implements a gradient production bias, similar to the one used in
 Model 1, but an increasing, rather than decreasing, function of 
\begin_inset Formula $x$
\end_inset

.
 On each iteration, the randomly selected token has probability 
\emph on
p
\emph default
 (< .5) of increasing by a fixed percentage (
\begin_inset Formula $\alpha$
\end_inset

) of its current value.
 As before, the category is initialized by sampling from a Normal distribution,
 and all tokens begin with non-biased values.
 Because there is only one cloud in perception, the only time a difference
 between biased and non-biased tokens can be observed is at the moment of
 production.
 Therefore, model outputs will be given in terms of an observed random sample
 of fixed size at some cycle, 
\emph on
n
\emph default
, of the model.
\end_layout

\begin_layout Standard
The iterativity of the perception-production loop allows for tokens to be
 biased multiple times, but also for tokens to remain persistently non-biased,
 the more so, the larger the category is in terms of stored exemplars, and
 the smaller the value of 
\emph on
p
\emph default
.
 To understand model behavior it is useful to think of each iteration as
 involving four possible outcomes.
 In the first, a relatively low-valued token (the outcome of a series of
 productions occurring more often in non-biasing contexts) is chosen for
 production, but this time in a biasing context, thus increasing its value
 along 
\emph on
x
\emph default
.
 The second possibility is that the same token is chosen for production
 in a non-biasing context, such that its value remains more or less unchanged
 (still relatively low).
 The third and fourth possibilities involve selecting a relatively high-valued
 token (the outcome of a series of productions occurring more often in the
 biasing context) and either producing it in a non-biasing context (no increase
 along 
\emph on
x
\emph default
), or a biasing context (additional increase along 
\emph on
x
\emph default
).
 
\end_layout

\begin_layout Standard
The last type of outcome ensures that a subset of tokens will continue to
 increase without bound.
 Despite the fact that the second type of outcome ensures the persistence
 of low-valued tokens, the category as a whole will move unboundedly rightward
 along 
\emph on
x
\emph default
.
 This is due to the combined effect of memory decay and entrenchment.
 For 
\begin_inset Formula $p<.5$
\end_inset

, the over-all mean of the distribution will always be closer to the lower-value
d side of the distribution, and will initially act to oppose the increase
 due to production bias.
 However, as higher-valued tokens are added to the category, they seed even
 higher-valued daughter tokens, generating an exponentially increasing subset
 of tokens.
 This is the relationship expressed in Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:linear bias"

\end_inset

), reformulated here, for a positive bias, as 
\begin_inset Formula $x_{o(+n)}=x_{o}(1+\alpha)$
\end_inset

.
 As this subset of tokens moves right, it will drag the rest of the distribution
 with it.
 Fig (
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:End context mismatch"

\end_inset

) illustrates this effect via comparison of the observed distribution after
 a model run of 1,000 iterations, versus 5,000 iterations.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align left
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename 1000iter.pdf
	width 30page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
1000 iterations
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename 5000iter.pdf
	width 30page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
5000 iterations
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:End context mismatch"

\end_inset

Observed distribution (800 tokens).
 White: productions in non-biasing context.
 Black: productions in biasing context.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
As expected, the same unboundedness problem arises as was seen in Model
 1.
 With the addition of a threshold (ceiling or floor value along 
\emph on
x
\emph default
) the sub-distributions merge, neutralizing the difference between biased
 and non-biased contexts
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand citet
key "DBLP:journals/corr/Tupper14a"

\end_inset

 attributes a merged outcome such as this to perfect categorization accuracy,
 i.e., failure to discard ambiguous tokens.
 
\end_layout

\end_inset

.
 It will also be shown that keeping all tokens in the same category, regardless
 of history, results in another type of problem – what I will call context
 mismatch.
 Context mismatch will be discussed when this model is linked to the linguistic
 phenomenon of vowel lengthening in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-2:-Lengthening"

\end_inset

.
 
\end_layout

\begin_layout Subsubsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Model-3:-Categorical"

\end_inset

Model 3: Categorical Context-Dependent Bias
\end_layout

\begin_layout Standard
Model 3 implements a binary production bias, albeit with an error term that
 maintains a small amount of variance.
 All tokens produced in the biasing context are initialized with a mean
 at the `+' value on dimension 
\emph on
x
\emph default
, while all tokens produced in the non-biasing context are initialized at
 the `-' value.
 See Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:binary-Starting-Distribution"

\end_inset

.
 As before, all tokens belong to the same category; the different colors
 are for illustrative purposes only, allowing us to track the production
 context during the observation cycle.
 Because the bias is uni-directional, and biasing is categorical, all tokens
 quickly shift to the biased [+] value.
 Once a token has a value of /+/ it cannot be biased further, nor can it
 be `un-biased'.
 This is shown in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:binary-1000iter"

\end_inset

, where we can see that previously biased tokens remain at [+] even if they
 are subsequently produced in a non-biasing context (white bars at [+] location).
 This model is, in fact, bounded, because there is no iterativity for the
 binary feature.
 However, like the previous two models, it results in neutralization of
 the difference between the different contexts.
 Binary-valued features that distinguish between contrastive sound units
 within a language are widely used in phonological theory.
 This connection will be discussed when the model is linked to the linguistic
 phenomenon of vowel nasalization in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-3:-Nasalization"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename NasalizationStart.pdf
	width 30page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:binary-Starting-Distribution"

\end_inset

Starting Distribution
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename Nasalization1000iter.pdf
	width 30page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:binary-1000iter"

\end_inset

1000 iterations
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Context Mismatch Features"

\end_inset

Quasi-binary feature.
 Two variants with equal contextual frequency.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "subsec:The-Linguistic-Phenomena"

\end_inset

The Linguistic Phenomena
\end_layout

\begin_layout Standard
The general context-free and context-dependent processes modeled in the
 previous chapter will now be mapped to specific linguistic phenomena.
 This chapter will show more concretely what the implementational and conceptual
 issues are in developing exemplar models based on tokens of experienced
 speech.
 We will also begin to examine the proper interpretation of model results
 with respect to existing theories and, conversely, the proper implementation
 of specific theoretical hypotheses within an exemplar framework.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "subsec:Word-Frequency"

\end_inset

Model 1: Word Frequency
\end_layout

\begin_layout Standard
In laboratory speech, as well as spoken corpora, it has been repeatedly
 demonstrated that words that are more commonly used are shorter in duration
 than comparable words that are less common (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "Bybee2001,Bybee2002,Bybee2006"

\end_inset

).
 Furthermore, it has been shown that as frequency increases, the average
 duration of a given word monotonically decreases (controlling for other
 factors).
 The diachronic counterpart of this phenomenon is the observation that more
 frequent words tend to 
\begin_inset Quotes eld
\end_inset

lead sound change
\begin_inset Quotes erd
\end_inset

, meaning that a change that will later spread throughout all, or most,
 words of a language is first observed to take place in high-frequency words
 (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "Phillips1984"

\end_inset

).
 Such changes are often themselves reductive in nature, either being the
 direct result of, or influenced by, a reduction in the temporal, and/or
 spatial, extent of the articulation of the given sounds (such as segment
 shortening, segment loss, assimilatory feature changes, or feature centralizati
on).
\end_layout

\begin_layout Standard
Competing explanations for frequency-based reduction can be separated into
 listener-based and speaker-based approaches.
 In the former, more reduced forms are assumed to be easier/more efficient
 for speakers to produce, and are thus hypothesized to be the default production
 mode.
 However, in the case where the meaning is unclear, or there is greater
 than normal ambiguity, the speaker exerts more effort in articulation,
 lengthening and strengthening speech sounds in order to facilitate speech
 recognition for the listener (e.g., 
\begin_inset CommandInset citation
LatexCommand citealp
key "Aylett2004"

\end_inset

).
 Because words that are highly predictable in context are easier to recover,
 such words can be safely reduced, whereas less predictable words must be
 produced more carefully.
 In the absence of other factors, the marginal probability of a given word
 provides an estimate of its likelihood; thus low-frequency words will be
 produced with less reduction in order to facilitate their recovery relative
 to high-frequency words.
 A speaker-based approach, on the other hand, attributes frequency effects
 to automatic consequences of speech production.
 Either high-frequency words have higher resting activation levels on average,
 leading to faster retrieval, and thus more rapid articulation (e.g., 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "gahl2012reduce"

\end_inset

), or increased practice with higher-frequency words leads to greater fluency,
 resulting in shorter, more efficient articulation (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "Bybee2002"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset CommandInset citation
LatexCommand citet
key "Pierrehumbert2000"

\end_inset

 adopts a speaker-based motivation for reduction, modeling the effect as
 a production bias that shortens each token by the same small fixed amount
 whenever it is produced.
 Although a model containing both high and low frequency categories was
 not actually implemented in 
\begin_inset CommandInset citation
LatexCommand citet
key "Pierrehumbert2000"

\end_inset

, the paper suggests that this simple bias can account both for synchronic
 differences in word duration, as well as reductive changes, over time.
 
\end_layout

\begin_layout Standard
The more often tokens are produced from a given category, the more chances
 there will be for initially unreduced tokens to be reduced multiple times.
 Thus, it might seem to follow that higher-frequency categories will shift
 further leftward than lower-frequency categories over the same period of
 time.
 However, as we saw in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-1:-Context-Free"

\end_inset

, the relative average durations of a lower- and higher- frequency word
 category depend on whether the number of tokens in a given category is
 proportional to the frequency of that category.
 Furthermore, given enough time (= number of productions), all categories
 will end up at the same minimum duration.
 In other words, the model will converge on this one stable state from any
 starting point.
 This is the inevitable result of a model with an unopposed force acting,
 and thus is not particularly surprising (
\begin_inset CommandInset citation
LatexCommand citet
key "Baker2011"

\end_inset

 make a similar observation about gradual-accumulation theories of change
 in general).
 However, it raises an important issue regarding the determination of synchronic
 versus diachronic time in exemplar models.
 
\end_layout

\begin_layout Standard
Computational models are typically only evaluated at convergence.
 This is in part because there is usually no explicit theory about how time
 within a model corresponds to real time (or to time in some other model).
 Exemplar models, however, explicitly map iterations to real-world events,
 namely, the perception and storage of speech tokens.
 This requires that literally any stage of the model be a possible synchronic
 state – at least an instantaneous one.
 This property also makes it possible, in principle, that the state of the
 model at convergence (or the fact that the model fails to converge) is
 irrelevant to evaluation.
 This is the case if it can be shown that convergence does not occur within
 the lifetime of the speaker.
 If the model parameters are chosen in a specific way, collapse may be avoidable
 within whatever is taken to be an average lifetime.
 An illustration of what is required to determine these parameters is provided
 in Appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Appendix B"

\end_inset

.
 This line of enquiry uncovers a further prediction of this general model.
 It must be the case that all words become more reduced over time, regardless
 of the specific parameter values.
 
\end_layout

\begin_layout Standard
The iterative model strongly implies that the frequency effect must arise
 in the lifetime of the speaker, and only after they have had sufficient
 exposure to a given (high frequency) category.
 We will define this timespan as the time it takes a category of some frequency
 
\emph on
f,
\emph default
 with a reduction bias of 
\begin_inset Formula $\alpha$
\end_inset

, to reach a degree of reduction, 
\begin_inset Formula $\delta_{n_{f}}$
\end_inset

, that is expressed as a proportion of the original duration.
 We will call this amount of real time an epoch, and we will define the
 number of productions of category 
\emph on
f
\emph default
 during an epoch as 
\begin_inset Formula $n_{f}$
\end_inset

.
 Then, by definition: 
\begin_inset Formula $\overline{d_{n_{f}}}=\overline{d_{0}}-\delta_{n_{f}}\overline{d_{0}}$
\end_inset

.
 Since we know that a frequency effect is observable, at minimum, in young
 adults, this epoch cannot be longer than around 20 years.
 Unless 
\emph on
f 
\emph default
decreases drastically (in fact, we might expect it to increase at this life
 stage), then multiple epochs remain in the lives of these speakers, and
 a decrease comparable to the original frequency effect should be expected
 to occur in each one of them.
 Thus, we should find that word durations should get steadily shorter over
 the lifetime.
 Of course, there is a hard limit on how much a word can be reduced.
 If we predict that some words will hit this limit within the given time
 frame then the frequency effect should actually be lost in the subset of
 words that have reached this limit.
 Although I am not aware of any studies that have specifically investigated
 these questions, I strongly suspect that these predictions would not be
 borne out.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "subsec:Model-2:-Lengthening"

\end_inset

Model 2: Vowel Lengthening
\end_layout

\begin_layout Standard
Context-dependence is the norm in language, especially in the domain of
 sound structure.
 Speech sounds exist in a high-dimensional space, and almost any change
 in context produces some measurable difference in a sound's pronunciation
 along one of those dimensions.
 These effects, however, are usually predictable, and so can be modeled
 using a fixed bias.
 Vowel lengthening before voiced obstruents in word-final position provides
 a simple instantiation of a context-dependent phenomenon that applies to
 the duration dimension.
 It has been noted for well over a 100 years that vowels before final voiced
 obstruents in English are 
\begin_inset Quotes eld
\end_inset

very long
\begin_inset Quotes erd
\end_inset

 (
\begin_inset CommandInset citation
LatexCommand citealt
key "Sweet1880"

\end_inset

: p 59), and laboratory studies have consistently found significant vowel
 duration differences between voiced-voiceless minimal pairs like 
\begin_inset Quotes eld
\end_inset

bad
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

bat
\begin_inset Quotes erd
\end_inset

 (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "peterson1960duration,chen1970vowel"

\end_inset

).
 Furthermore, perceptual experiments find that vowel duration is a sufficient
 cue to the 
\begin_inset Quotes eld
\end_inset

voicing
\begin_inset Quotes erd
\end_inset

 on the final obstruent, whether that segment is actually voiced or not
 (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "raphael1972preceding,Klatt1976"

\end_inset

).
 As it is conventionally described, vowel lengthening is an allophonic process
 whereby vowels produced in the lengthening context (before voiced obstruents)
 are lengthened by some degree, while vowels not produced in the lengthening
 context remain unchanged.
\end_layout

\begin_layout Standard
However, the results of the Model 2 simulation show that, over time, 
\begin_inset Quotes eld
\end_inset

short
\begin_inset Quotes erd
\end_inset

 tokens gets longer, and 
\begin_inset Quotes eld
\end_inset

long
\begin_inset Quotes erd
\end_inset

 tokens get even longer, and eventually all tokens are maximally long whether
 they're produced in pre-voiced or pre-voiceless contexts.
 Furthermore, it is not possible to guarantee, at any intermediate stage,
 that tokens produced in the biasing (voiced) context will be consistently
 longer than tokens produced in the non-biasing (voiceless) context.
 Because of the different possible histories of each token, the single category
 contains both tokens that are very long (all ancestors produced in biasing
 context), and very short (all ancestors produced in non-biasing contexts).
 If a particularly short token is chosen (at random) for production in a
 voiced context it won't be as long, even after lengthening, as other tokens
 in that context have been in the past (on previous iterations).
 Likewise, if a particularly long token is chosen to be produced in a voiceless
 context it will be longer than other tokens in that context have tended
 to be.
 We know that listeners develop expectations about what they should be hearing
 based on context, and can detect when the variant differs from expectation
 (e.g., 
\begin_inset CommandInset citation
LatexCommand citealp
key "krakow1988coarticulatory,gaskell1996phonological"

\end_inset

).
 This mismatch between token and context is therefore a problem for the
 basic exemplar model.
 
\end_layout

\begin_layout Standard
However, all these effects can be seen to arise out of the fact that all
 tokens are taken from, and added back to, the same undifferentiated cloud.
 If tokens of the two allophones were stored separately, then these problems
 could presumably be eliminated.
 Let us consider what that would entail.
 From the perspective of theoretical linguistics, allophones, by definition,
 have no independent representational status.
 They exist only at the surface, only as the realization of a phoneme to
 which some rule, or process, has applied
\begin_inset Foot
status open

\begin_layout Plain Layout
Note that this model is entirely implemented at the phoneme level, allowing
 the presumed forces to act directly on their targets.
 Although exemplar models often assume a word-level representation (explicitly
 or implicitly), most are actually implemented at the phoneme level, and
 lack explicit mechanisms for connecting the two (although see 
\begin_inset CommandInset citation
LatexCommand citet
key "Wedel2008"

\end_inset

 for a model of an indirect biasing relationship).
 Mechanisms such as frequency-based reduction and contrast maintenance are
 defined with respect to the word level.
 Implementing them at the sub-lexical level not only obscures the fact that
 a mapping between the levels is necessary, but eliminates a fundamental
 property of abstraction: the more abstract the unit, the larger the category,
 and the more, and more varied, the tokens.
 The phoneme category 
\begin_inset IPA

\begin_layout Standard
/æ/
\end_layout

\end_inset

 encompasses more than just tokens extracted from the words 
\begin_inset Quotes eld
\end_inset

tag
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

tack
\begin_inset Quotes erd
\end_inset

, but from a large number of words, such as 
\begin_inset Quotes eld
\end_inset

cat
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

lack
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

sag
\begin_inset Quotes erd
\end_inset

, 
\begin_inset Quotes eld
\end_inset

package
\begin_inset Quotes erd
\end_inset

, etc.
 Any changes at the level of the individual word are only one small part
 of what affects the realization of a given phoneme.
 Thus, establishing that a phoneme-level effect follows from a word-level
 interaction requires a significantly more complex model than is usually
 implemented.
 
\end_layout

\end_inset

.
 We are perfectly free to adopt the hypothesis that allophones do, in fact,
 have separate representational status, and create a model in which 
\begin_inset Quotes eld
\end_inset

lengthened
\begin_inset Quotes erd
\end_inset

 allophones are only selected from the 
\begin_inset Quotes eld
\end_inset

lengthened
\begin_inset Quotes erd
\end_inset

 (sub) category, and only 
\begin_inset Quotes eld
\end_inset

unlengthened
\begin_inset Quotes erd
\end_inset

 allophones from the 
\begin_inset Quotes eld
\end_inset

unlengthened
\begin_inset Quotes erd
\end_inset

 (sub) category
\begin_inset Foot
status open

\begin_layout Plain Layout
This is implemented as the State Model in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Models-of-Change"

\end_inset


\end_layout

\end_inset

.
 This would eliminate the context-mismatch problem.
 A paradox arises, however, if we continue to apply the lengthening bias
 to the lengthened tokens.
 By creating a category for these allophones we are effectively encoding
 their context: this category consists of tokens that occur before voiced
 obstruents.
 Applying lengthening to such a token implies that the token was original
 unlengthened (non-biased).
 It also implies that the contextual information was discarded when the
 token was stored, and so must be added during production.
 A model with both explicit representational structure incorporating a biasing
 context, and an actual biasing process, is a strange hybrid.
 This incompatibility between modeling a phenomenon as both stored 
\emph on
and
\emph default
 generated will be discussed in more depth in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Models-of-Change"

\end_inset

.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "subsec:Model-3:-Nasalization"

\end_inset

Model 3: Vowel Nasalization
\end_layout

\begin_layout Standard
While phonemes are taken to be the basic phonological unit for many purposes,
 most phonological rules that operate at the level of individual phonemes,
 in fact, affect only a subset of that phoneme's features.
 Classically, phonemes are taken to be decomposable into a universal set
 of discrete features, and can be uniquely defined by a specific matrix
 of values over those features.
 These features are usually assumed not just to be discrete, but to be binary
 in nature, taking on only one of two possible values, 
\begin_inset Quotes eld
\end_inset

+
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

-
\begin_inset Quotes erd
\end_inset

.
 Thus a partial feature matrix might consist of 
\begin_inset Formula $\left[\begin{array}{c}
+voice\\
-nasal\\
+coronal\\
-del.rel
\end{array}\right]$
\end_inset

, for example, which matches the phonemes /
\emph on
t
\emph default
/, /
\emph on
d
\emph default
/
\emph on
 
\emph default
and /
\emph on
s
\emph default
/, among others.
 Whether considered to be phonetic or phonological, nasalization is a process
 that changes the 
\begin_inset Formula $\left[-nasal\right]$
\end_inset

 specification to 
\begin_inset Formula $\left[+nasal\right]$
\end_inset

.
 In English this rule applies to all vowels occurring in the context of
 a following nasal consonant (e.g., 
\begin_inset IPA

\begin_layout Standard
[læb]
\end_layout

\end_inset

 vs.
 
\begin_inset IPA

\begin_layout Standard
[læ̃m]
\end_layout

\end_inset

).
 Thus it is analogous, in all respects other than binarity, to the vowel
 lengthening example simulated in Model 2, and similarly results in context
 mismatch.
 What Model 3 demonstrates more transparently, however, is that the eventual
 outcome is neutralization of the distinction between the two allophones.
 Once a given token is produced in a nasal context for the first time, all
 its daughter tokens will also be nasal, even when produced in an oral context
 (
\begin_inset IPA

\begin_layout Standard
[læ̃b]
\end_layout

\end_inset

).
 Neutralization occurs because the process of nasalization is uni-directional;
 nasalized tokens produced in oral contexts are not `oralized'.
 In other words, there is only one bias, and only one biasing context, and
 under those circumstances the basic exemplar model will result in biased
 variants in all contexts.
\end_layout

\begin_layout Standard
The nasalization rule, in addition to illustrating a binary process, introduces
 a biasing dimension other then duration.
 This is important because duration is significantly simpler than most phonetic
 variables.
 Additionally, duration possesses what may be a unique property: it is invariant
 under the transformation from production to perception (in the absence
 of error)
\begin_inset Foot
status open

\begin_layout Plain Layout
There is, however, potential ambiguity in attributing duration differences
 to inherent duration, versus differences in speaking rate or prosodic contexts.
 See Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Summary-&-Conclusions"

\end_inset

.
 
\end_layout

\end_inset

.
 Thus, an architecture that can derive the correct results for phonological
 processes acting on duration is not guaranteed to do the same for other
 phonological dimensions.
 
\end_layout

\begin_layout Standard
Vowel nasalization seems to be quite well-understood, and to have a straightforw
ard explanation.
 It arises through an inherent property of normally produced speech: coarticulat
ion.
 The articulation for the nasal consonant, which involves lowering the velum
 so that air can flow through the nasal cavity, is initiated before the
 articulation of the preceding vowel is fully completed.
 As a result, the velum is open for some portion of the end of the vowel,
 meaning that nasal airflow occurs, which, by definition, means that the
 vowel is partially nasalized.
 The evolution from partial to full nasalization seems to be exactly what
 the basic exemplar model should account for: a gradual increase of nasalization
 through an iterative process in which already nasalized (biased) tokens
 are subject to additional nasalization (biasing) as produced tokens are
 converted into stored tokens, which are once again converted into production
 tokens.
 Yet we have already seen that the context-dependent version of the basic
 exemplar model results in a single degenerate outcome.
 
\end_layout

\begin_layout Standard
In fact, there is a deeper representational problem related to the source
 of the bias.
 The degree of vowel nasalization corresponds more or less directly to the
 extent of the vowel during which nasal airflow is present.
 Thus, it is a question only of how early the velum is lowered.
 For nasalization to increase incrementally, the velum lowering must occur
 earlier and earlier.
 There is, however, no mechanism in the basic exemplar model to accomplish
 this.
 The root of the problem is the lack of an explicit production to perception
 mapping.
 That mapping will be the focus of Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Perception-Production"

\end_inset

.
 For now the focus will be on the production side, and the argument will
 be that, on empirical grounds, articulatory parameters cannot depend solely
 on the free evolution of perceptual categories.
 Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Models-of-Change"

\end_inset

 will also show that explicit articulatory targets can be used to prevent
 the collapse and merger problem shared by Models 1-3.
 
\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "sec:Models-of-Change"

\end_inset

Modeling Stability & Change
\end_layout

\begin_layout Standard
In many implemented exemplar models that include production, unbounded iterative
 biasing is prevented via the elimination of tokens that fall in the ambiguous
 region between two existing categories (
\begin_inset CommandInset citation
LatexCommand citealp
key "Wedel2004,Wedel2006,Wedel2008,Blevins2009,DBLP:journals/corr/Tupper14a"

\end_inset

).
 If the categories are taken to be words, then discarding ambiguous tokens
 acts to maintain a meaning distinction that relies on a minimal sound distincti
on along the given phonetic dimension.
 The idea that sound changes that produce homophony are dispreferred in
 some way has existed within historical linguistics for a long time (e.g.,
 
\begin_inset CommandInset citation
LatexCommand citealt
key "Martinet1955"

\end_inset

).
 In recent years this notion has been revived and quantified as an inverse
 correlation between the probability of a sound change that neutralizes
 contrast 
\emph on
x
\emph default
, and the number of words that are differentiated only by contrast 
\emph on
x
\emph default
.
 This is known as the 
\begin_inset Quotes eld
\end_inset

functional load
\begin_inset Quotes erd
\end_inset

 of the contrast
\begin_inset Foot
status open

\begin_layout Plain Layout
There are many other ways one might define functional load.
 However, it turns out that a simple minimal pair count seems to be the
 most useful of these.
\end_layout

\end_inset

 (
\begin_inset CommandInset citation
LatexCommand citealp
key "Surendran2006,wedel2013high"

\end_inset

).
 In 
\begin_inset CommandInset citation
LatexCommand citet
key "Wedel2008"

\end_inset

, contrast maintenance (homophone avoidance) is implemented as a storage
 probability that is proportional to goodness of fit.
 Tokens that are less prototypical members of both categories have a lower
 probability of being retained in either category.
 Thus, as ambiguous tokens are lost, the two categories are effectively
 pushed apart.
 Functional load can be modeled as a weighting factor in this type of model,
 increasing the probability that ambiguous forms will be discarded, and
 effectively strengthening the contrast maintenance effect for certain words
 (
\begin_inset CommandInset citation
LatexCommand citealt
key "Soskuthy2015"

\end_inset

).
\end_layout

\begin_layout Standard
The existence of a second contrasting category along the biasing dimension
 will prevent the biased category from moving past a certain point, allowing
 the basic exemplar models of the previous chapter to converge.
 The assumption of such a category, however, limits the types of sound changes
 that can be modeled; in particular, a sound change in which a new category
 is formed, presumably from the biased variants of an existing category.
 It is exactly this change that is adopted as the modeling gold standard
 in this book.
 Therefore, we will have to consider what other forces can achieve stability,
 forces that, if general, must be included in all models, whether they are
 implementationally necessary or not
\begin_inset Foot
status open

\begin_layout Plain Layout
A goodness-of-fit function that doesn't directly reference contrast is possible
 in this scenario.
 However, it will not produce the desired effect for a single category.
 If prototypicality is determined by distance from the category mean, then
 what is acceptable will change as the mean of the category changes, which
 will occur because of the constant phonetic bias.
 Therefore, the category will move unboundedly.
 On the other hand, if prototypicality depends on some fixed value, then
 the category will not be able to shift beyond the specified limit of `goodness'.
 
\end_layout

\end_inset

.
 In this chapter we will analyze, in detail, the consequences of adding
 production targets to the basic exemplar models of Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:The-Exemplar-Model"

\end_inset

.
 In doing so we will arrive at a subset of models that meet the two criteria
 of boundedness and theoretical coherence.
 The full range of possible outcomes for this set of models will then be
 derived, setting the stage for an investigation of what type of architecture
 would be sufficient (and possibly necessary) to produce (under the appropriate
 conditions) the genesis of a new phoneme category.
\end_layout

\begin_layout Standard
It should be noted at this point that it is widely acknowledged that sociolingui
stic factors play a central role in language change.
 A class of `innovators' may be required, aided by a class of `early adopters'
 in the actuation and spread of a change (
\begin_inset CommandInset citation
LatexCommand citealt
key "milroy1985linguistic"

\end_inset

).
 Change may require those with less social power to pay more attention to
 the speech of those with more power, leading to incorrect inferences about
 the source of phonetic variation (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "Garrett2013"

\end_inset

).
 Change may require systematic differences between individual speakers in
 their analysis of ambiguous data, the degree to which they compensate for
 phonetic biases, or some other facet of speech processing (e.g., 
\begin_inset CommandInset citation
LatexCommand citealp
key "Beddor2009,yu2013socio"

\end_inset

).
 This paper does not explicitly address these aspects of sound change in
 that it focuses on the mental grammar of a single individual.
 The approach taken here, however, is not incompatible, nor inconsistent,
 with a theory of sound change that includes socio-indexical variables.
\end_layout

\begin_layout Section
Articulatory Targets
\end_layout

\begin_layout Standard
Many of the set of proposed universal phonological features specify articulatory
 parameters, such as where in the mouth the tongue tip makes contact during
 the production of the sound.
 Explicit targets of this kind are often assumed to be unnecessary in exemplar
 modeling, where categories are taken to be emergent – dependent only the
 interaction of competing forces.
 This assumption is aided by the practice of treating the initial distribution
 of tokens as arbitrary and independent of the model.
 A number of works have demonstrated that from a single global pressure,
 such as avoidance of homophony, structured categories can evolve (
\begin_inset CommandInset citation
LatexCommand citealt
key "Boer2000,Wedel2006,soskuthy2013phonetic"

\end_inset

).
 However, sound categories, once established, are unlikely to be determined
 solely by the number of contrasts in a given language.
 If this were the case, then a category would be defined only by its individual
 members (the label 
\begin_inset IPA

\begin_layout Standard
/p/
\end_layout

\end_inset

, for example, would be completely arbitrary, and contain no information
 about the use of the lips in the production of the sound).
 Furthermore, we would not expect the consistent phonetic differences that
 are found in the production of phonologically identical sounds across different
 languages (
\begin_inset CommandInset citation
LatexCommand citealt
key "Keating1985"

\end_inset

).
 Distributions would also be predicted to spread out on dimensions lacking
 a contrastive segment distinction.
 Although there is evidence that the absence of contrast leads to greater
 variation in pronunciation along that dimension (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "choi1995acoustic"

\end_inset

), this variation is not unlimited.
 
\begin_inset CommandInset citation
LatexCommand citet
key "Baker2011"

\end_inset

, for example, found a number of differences in how speakers produced American
 English 
\begin_inset Quotes eld
\end_inset

r
\begin_inset Quotes erd
\end_inset

 sounds.
 However, those differences resulted in little to no acoustic difference
 between productions.
 Despite the fact that English does not contrast different types of rhotic
 segments, productions do not expand to fill that large phonetic space.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "subsec:Soft-Targets"

\end_inset

Soft Targets
\end_layout

\begin_layout Standard
From a purely implementational perspective, production targets offer a mechanism
 for avoiding unbounded shift and neutralization.
 Fixed targets, however, will prevent any kind of change, and render the
 exemplar architecture superfluous.
 In this section, what amounts to a semi-fixed target is adopted: a force
 that acts to keep tokens at a fixed location, but from which they can be
 perturbed to some degree by the usual biasing forces.
 
\end_layout

\begin_layout Standard
The semi-fixed, or 
\begin_inset Quotes eld
\end_inset

soft
\begin_inset Quotes erd
\end_inset

, duration target is expressed in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Inertia Force"

\end_inset

), where 
\begin_inset Formula $\beta$
\end_inset

 is a constant between 0 and 1 that determines the strength of the target,
 and 
\emph on
N
\emph default
 is the location of the target along the biasing dimension 
\emph on
x
\emph default
.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
I(x_{i})=\beta(N-x_{i})\label{eq:Inertia Force}
\end{equation}

\end_inset

When the category is instantiated with a mean at 
\emph on
N
\emph default
 (
\begin_inset Formula $z=0$
\end_inset

), 
\begin_inset Formula $I(x)$
\end_inset

 can be conceptualized as a type of inertia, acting to keep tokens in place.
 The further a token moves from the target, the stronger the force pulling
 it back.
 This has the desired effect of bounding movement in either direction, while
 still allowing the category to shift as a whole.
 If 
\emph on
I
\emph default
 is the only force acting, then the tokens will eventually settle at the
 equilibrium point 
\emph on
N
\emph default
, where the change in 
\emph on

\begin_inset Formula $\bar{x}$
\end_inset


\emph default
 is 0.
 
\emph on
N
\emph default
 can also be characterized as the optimum of a function with a positive
 derivative when 
\emph on
x
\emph default
 is less than 
\emph on
N
\emph default
, and a negative derivative, when 
\emph on
x
\emph default
 is greater than 
\emph on
N
\emph default
.
 Regardless of the location of 
\emph on
x
\emph default
, it is always being pushed in the direction of 
\emph on
N
\emph default
.
 
\end_layout

\begin_layout Standard
The Soft Target model is built directly from the gradient context-dependent
 model of Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-2:-Lengthening"

\end_inset

.
 Tokens selected randomly to occur in a biasing context are subjected to
 a bias that increases their value along dimension 
\emph on
x
\emph default
 by a small percentage (
\begin_inset Formula $\alpha$
\end_inset

) of their current value: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
L(x_{i})=\alpha x_{i}\label{eq:Lengthening Force}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Model2:LengtheningProcess"

\end_inset

 shows the effect of adding a soft target to Model 2.
 Change is constrained relative to the basic model.
 This model is also theoretically interpretable; there is a single category,
 with a single production target corresponding to the non-biased segment,
 and the biasing process applies to all tokens of this category with equal
 probability.
 These properties will become important as we continue to explore the modeling
 space in the following sections.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename BaselineModel10000iter.pdf
	scale 30

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Model2:LengtheningProcess"

\end_inset

Soft-Target Model: Increasing bias function.
 Single category:Single target.
 White: tokens produced in non-biasing contexts.
 Black: tokens produced in biasing contexts.
 x-axis: z-normed 
\emph on
x
\emph default
 dimension.
 Observation occurs after 10,000 model cycles.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
For positive 
\emph on
x
\emph default
, the difference between (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Lengthening Force"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Inertia Force"

\end_inset

) is effectively between a monotonically increasing function with an optimum
 at 0, and a non-monotonic function with an optimum at 
\emph on
N
\emph default
.
 Theoretically speaking, however, the former expresses a 
\shape smallcaps
process
\shape default
, while the latter expresses a 
\shape smallcaps
state
\shape default
 (cf.
 
\begin_inset CommandInset citation
LatexCommand citealt
key "Hyman1975"

\end_inset

).
 
\shape smallcaps
process 
\shape default
will be taken to refer to what would be considered an allophonic rule in
 generative phonology, and to which the term `lengthening' (or `shortening')
 can properly apply.
 At the segment level (
\emph on
S
\emph default
), the general process model instantiates the following linguistic relationship:
 
\begin_inset Formula $/S/\rightarrow[S{}^{B}]/$
\end_inset


\emph on
__B
\shape smallcaps
\emph default
, 
\shape default
where
\emph on
 B
\emph default
 stands for the biasing context, and 
\begin_inset Formula $[S^{B}]$
\end_inset

 stands for the allophonic variant that occurs in that context.
 Using the same notation, a 
\noun on
state
\noun default
 model instantiates the following relationship: 
\begin_inset Formula $/S^{B}/\rightarrow[S{}^{B}]$
\end_inset

 in context 
\emph on
B
\emph default
.
 This indicates that 
\begin_inset Formula $S^{B}$
\end_inset

 is stored, or underlying, rather than generated.
 A given 
\shape smallcaps
state
\shape default
 could consist of `long', or `lengthened' tokens, but does not properly
 involve `lengthening'.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Model-Interpretation"

\end_inset

Model Space
\end_layout

\begin_layout Standard
The original lengthening model in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Phrase-Final Lengthening"

\end_inset

 is a 
\shape smallcaps
process
\shape default
 model.
 As we saw previously, a 
\shape smallcaps
process
\shape default
 model that lacks a production target is unbounded, producing no stable
 outcomes.
 As will be shown in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-B:-Lengthening"

\end_inset

, adding a soft target to this model will result in stable outcomes for
 a certain range of parameter values.
 The analogous 
\shape smallcaps
state
\shape default
 model can be created by implementing the bias term itself as a soft target,
 as in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Length Attractor"

\end_inset

) (cf.
 
\begin_inset CommandInset citation
LatexCommand citealt
key "soskuthy2013phonetic"

\end_inset

).
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
L(x_{i})=\alpha(L-x_{i})\label{eq:Length Attractor}
\end{equation}

\end_inset

This model, with one target for non-biased tokens, and one for biased tokens,
 can also be shown to produce stable outcomes.
 The no-target 
\noun on
process
\noun default
 model, the soft-target 
\noun on
process
\noun default
 model, and the 
\noun on
state
\noun default
 model, however, differ with respect to their theoretical consistency, and
 thus, linguistic interpretability.
 
\end_layout

\begin_layout Standard
Model 2, from Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Phrase-Final Lengthening"

\end_inset

, re-labelled as Model A in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab: Model Comparison-1"

\end_inset

, is a linguistically interpretable model.
 There is a single category from which tokens are selected at random, either
 to be produced in biasing contexts, in which case they are lengthened,
 or to be produced in non-biasing contexts, in which case they are unchanged.
 Model B, with a soft target at the location of the non-biased, underlying
 category is also consistent.
 All tokens feel a pull towards this underlying target, but those that happen
 to be produced in a biasing context are also subject to a force that lengthens
 them during production.
 Model C, however, the 
\shape smallcaps
state
\shape default
 model, is not theoretically consistent.
 
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Single-Category Model Space
\begin_inset CommandInset label
LatexCommand label
name "tab: Model Comparison-1"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
resizebox{
\backslash
textwidth}{!}{
\end_layout

\end_inset


\begin_inset Tabular
<lyxtabular version="3" rows="4" columns="7">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Stable
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Consistent
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
A
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\shape smallcaps
Process
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\alpha(1+x)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
No Target
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
–
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Y
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
B
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\shape smallcaps
Process
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\alpha(1+x)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Target
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\beta(N-x)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(Y)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Y
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
C
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\shape smallcaps
State
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\alpha(L-x)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Target
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\beta(N-x)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Y
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In Model C, all tokens have a target at 
\emph on
N
\emph default
, but tokens produced in a biasing context have an additional, conflicting
 target at 
\emph on
L
\emph default
.
 Because there is only a single category in Model C, biased tokens are generated
 from the same pool as non-biased tokens, therefore the second target, 
\emph on
L
\emph default
, exists without an underlying category with which that target can be associated.
 With the distribution initialized at 
\emph on
N
\emph default
, the effect is for biased tokens to be moved an arbitrarily small distance,
 
\begin_inset Formula $\alpha(L-x)$
\end_inset

, towards that second target during production
\begin_inset Foot
status open

\begin_layout Plain Layout
As a 
\noun on
process
\noun default
, incrementality has a straightforward interpretation; a given token is
 shifted, or lengthened, by a fixed proportion of its current length.
 But in a 
\shape smallcaps
state
\shape default
 model, in which all tokens are initialized at one target, it is not clear
 what mechanism would shift certain tokens only a small amount towards another
 target.
 Although, superficially, this effect is similar to that of the entrenchment
 force, 
\begin_inset Formula $\varepsilon(\overline{x}-x)$
\end_inset

, which pushes the tokens of a given category closer together, they are
 different in important ways.
 The use of the category mean in the entrenchment function stands in for
 the sum of the forces that act between individual tokens, maintaining category
 cohesion (the same effect can be achieved by averaging over multiple tokens
 in production (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "Pierrehumbert2000,Wedela"

\end_inset

)).
 The soft target, or inertia force, on the other hand, references a fixed
 target location that is specified independently of the current distribution.
 
\end_layout

\end_inset

.
 Of the three models, only Model B is both stable (bounded), and theoretically
 consistent.
 
\end_layout

\begin_layout Standard
Model C, however, can be made theoretically consistent by introducing a
 second level of representations.
 If the parent category can be split into two sub-categories, then each
 target can be associated with a different sub-category.
 This is Model G in Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab: Model Comparison"

\end_inset

, which is the 2-level model analog of Table 
\begin_inset CommandInset ref
LatexCommand ref
reference "tab: Model Comparison-1"

\end_inset

.
 The remaining models in this table are all theoretically problematic in
 different ways.
 Model D is the two sub-category counterpart of Model B.
 while Model B was theoretically consistent, the introduction of a separate
 sub-category for biased tokens in Model D creates a representational paradox:
 a category with no target, to which lengthening continuously applies.
 Model D is also unbounded.
 Model E re-creates the two-target paradox of Model C.
 And F is the hybrid 
\noun on
process
\noun default
+
\emph on
state
\emph default
 model
\begin_inset Foot
status open

\begin_layout Plain Layout
If double specifications are possible (e.g., 
\shape smallcaps
process
\shape default
 + 
\shape smallcaps
state
\shape default
), then the total set of possible models includes the Single-Category 
\shape smallcaps
process
\shape default
+
\shape smallcaps
state
\shape default
 model, and the set of non-biased No-Target models, among others.
 However, these other models all contain a superset of the representational
 inconsistencies already described, and therefore are not included.
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
2-Level Model Space
\begin_inset CommandInset label
LatexCommand label
name "tab: Model Comparison"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Box Frameless
position "t"
hor_pos "c"
has_inner_box 1
inner_pos "t"
use_parbox 0
use_makebox 0
width "100col%"
special "none"
height "1in"
height_special "totalheight"
thickness "0.4pt"
separation "3pt"
shadowsize "4pt"
framecolor "black"
backgroundcolor "none"
status open

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
resizebox{
\backslash
textwidth}{!}{
\end_layout

\end_inset


\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="7">
<features tabularvalignment="middle">
<column alignment="center" valignment="top" width="0pt">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
biased
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell multicolumn="1" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
non-biased
\end_layout

\end_inset
</cell>
<cell multicolumn="2" alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Stable
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Consistent
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
D
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\shape smallcaps
Process
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\alpha(1+x)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Target
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\beta(N-x)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
E
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\shape smallcaps
2-State
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\beta(N-x)$
\end_inset

; 
\begin_inset Formula $\alpha(L-x)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Target
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\beta(N-x)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Y
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
F
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\shape smallcaps
Process+ State
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\alpha(1+x)$
\end_inset

; 
\begin_inset Formula $\alpha(L-x)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Target
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\beta(N-x)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Y
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
G
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\shape smallcaps
State
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\alpha(L-x)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Target
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\beta(N-x)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Y
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Y
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset ERT
status open

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Only two viable candidates emerge from the full set of models.
 A pure 
\shape smallcaps
process
\shape default
 model with a single target (B), and a pure 
\shape smallcaps
state
\shape default
 model (G).
 In general terms, these results show us that 
\shape smallcaps
process
\shape default
 and 
\shape smallcaps
state
\shape default
 models are incompatible with one another.
 If biased tokens have a separate representational status, this implies
 that only tokens from this sub-category should be chosen to be produced
 in biased contexts.
 Furthermore, since the biased sub-category has a target at 
\emph on
L
\emph default
, those tokens will already be appropriately longer than their non-biased
 counterparts (with a target at 
\emph on
N
\emph default
).
 Therefore, there is no motivation for lengthening them further.
 Effectively, this would be equivalent to a phonological rule of the form:
\end_layout

\begin_layout Numbered Examples (consecutive)
\begin_inset CommandInset label
LatexCommand label
name "Process-+-State:"

\end_inset

Process + State: 
\begin_inset Formula $/S^{B}/\rightarrow[S{}^{B^{B}}]/$
\end_inset


\emph on
__B
\end_layout

\begin_layout Standard
Although (
\begin_inset CommandInset ref
LatexCommand ref
reference "Process-+-State:"

\end_inset

) is linguistically ill-formed, it is equivalent to the feedback loop at
 the heart of the basic exemplar model
\begin_inset Foot
status open

\begin_layout Plain Layout
It should be noted that, as far as I am aware, no one has actually proposed
 the context-dependent exemplar models in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:The-Exemplar-Model"

\end_inset

.
 They are what I take to be the logical extension of the context-free exemplar
 model of 
\begin_inset CommandInset citation
LatexCommand citet
key "Pierrehumbert2000"

\end_inset

.
\end_layout

\end_inset

.
 Cumulativity of small differences is only possible if the bias effects
 in production (contextually determined allophony) are stored (
\noun on
state
\noun default
), rather than being stripped away during perception.
 Storage of allophonic detail implies that the biasing context itself is
 discarded, or at least not used to recover the underlying form.
 In production, however, the 
\shape smallcaps
process
\shape default
 model requires knowledge of the context that triggers biasing.
 In other words, the allophonic rule is available in production, but not
 in perception
\begin_inset Foot
status open

\begin_layout Plain Layout
The alternative is that both the unnormalized surface forms, and their productio
n context are stored, or incorporated into the category label in some way.
 Even if so, it is still not clear why an allophonic rule would continue
 to apply.
 Furthermore, if prior specification of complex sub-structure is required
 (and, in the limit, a unique category for every token) the exemplar framework
 does not seem to offer much, if anything, in terms of explanatory power.
 
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
Another way to characterize this theoretical incompatibility is that a 
\shape smallcaps
process
\shape default
 model implies that normalization takes place, while a 
\shape smallcaps
state
\shape default
 model implies that it does not.
 Thus, inconsistency results when either the production or perception stage
 of a given model assumes normalization, while the other doesn't.
 In the Pure Process Model (B), all tokens are drawn from the same distribution
 in production, with a target, or underlying specification, at 
\emph on
N
\emph default
.
 Lengthening applies as an allophonic rule, but in perception all tokens
 are drawn back to the same underlying target at 
\emph on
N
\emph default
, whether they are lengthened or not.
 Thus, the inertial force acts to partially normalize the effect of lengthening.
 Complete normalization (fixed target) would prevent change entirely.
 In the Pure State Model (G), on the other hand, normalization fails to
 occur in the sense that `lengthened' tokens are assigned to their own sub-categ
ory, and no allophonic rules apply.
 Again, the 
\shape smallcaps
state
\shape default
 aspect is only partial.
 The fact that these are sub-categories rather than completely independent
 categories introduces a connection between the biased and non-biased tokens
 which implies that the relationship between them is known, and therefore,
 that the allophonic transformation is known.
 Two entirely independent categories would preclude change entirely.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Model-Behavior"

\end_inset

Consistent and Convergent Models of Sound Change
\end_layout

\begin_layout Standard
This section is devoted to an exhaustive analysis of the ends states of
 the two theoretically consistent and bounded models: the Pure Process Model
 (B), and the Pure State Model (G).
 These results will be provided in the form of two parameters: the category
 means along the dimension 
\emph on
x
\emph default
, and the difference between the means of the biased and non-biased sub-distribu
tions.
 Because it is not possible to guarantee that simulations will fully sample
 the space of possible outcomes, stable states will be explicitly derived
 as a function of model parameters.
 The derivation will be given in abbreviated terms in the text, with the
 full details provided in the appendices.
 Following 
\begin_inset CommandInset citation
LatexCommand citet
key "soskuthy2013phonetic,Soskuthy2015"

\end_inset

, the percentage of tokens produced in a biasing context (bias proportion)
 will act as the independent variable.
 The term 
\begin_inset Quotes eld
\end_inset

attractor
\begin_inset Quotes erd
\end_inset

 will also be adopted in reference to a soft target, in order to facilitate
 comparison to that work.
 
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Lengthening-as-State"

\end_inset

State Model: Sub-categories
\end_layout

\begin_layout Standard
The Pure State Model contains one target for biased tokens, and a distinct
 target for non-biased tokens.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Model G"

\end_inset

 provides an illustration of the forces acting at some model time 
\emph on
t
\emph default
, on exemplar categories modeled as Normal functions.
 As will be shown below, the means of both sub-categories can be guaranteed
 to lie somewhere between the two targets at 
\emph on
N
\emph default
 and 
\emph on
L
\emph default
.
 Each sub-category is subject to the inertia associated with its own target,
 acting to pull the two apart.
 Membership in a superset category is implemented via the entrenchment force,
 which pulls both in the direction of the global mean, and thus towards
 one another
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand citet
key "soskuthy2013phonetic"

\end_inset

 links sub-categories by applying phonetic biasing probabilistically to
 both, but with the biased sub-category more strongly weighted.
 
\end_layout

\end_inset

.
 In this illustration, the relative number of tokens produced in biasing
 versus non-biasing contexts is represented by the heights of the Normal
 curves.
 Because the proportion of biasing contexts is less than 50% in this example,
 the global mean (indicated by the dashed line) is closer to the mean of
 the non-biased distribution.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Model6Behavior.pdf
	scale 50

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Model G"

\end_inset

Schematic of forces for Pure State Model
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
The equations for each of the model forces have been given previously, but
 are repeated here for ease of reference: (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Entrenchment-2"

\end_inset

) Entrenchment, (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Inertia-2"

\end_inset

) Inertia (attractor) at 
\emph on
N
\emph default
, and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Length Attractor-2"

\end_inset

) Inertia (attractor) at
\emph on
 L
\emph default
.
 A small random error term is also included in all models.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
E(x_{i})=\epsilon(\bar{x}-x_{i})\label{eq:Entrenchment-2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
I(x_{i})=\beta(N-x_{i})\label{eq:Inertia-2}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
L(x_{i})=\alpha(L-x_{i})\label{eq:Length Attractor-2}
\end{equation}

\end_inset

In order to estimate the behavior of this model under various conditions
 we will make the simplifying assumption that each sub-category is specified
 by a Normal curve with variable mean, but fixed standard deviation.
 This allows us to use the mean of each sub-category as a proxy for its
 global behavior.
 To determine the stable model outputs, we use the fact that forces must
 balance at this equilibrium point, meaning that no further changes occur
 in the location of the means.
 Therefore, the sum of all forces is set to zero.
 
\begin_inset Formula $\overline{x_{E}}$
\end_inset

 is defined as the location of the global mean at equilibrium, while 
\begin_inset Formula $\overline{x_{E}^{B}}$
\end_inset

 and 
\begin_inset Formula $\overline{x_{E}^{NB}}$
\end_inset

 are the equilibrium means of the biased and non-biased sub-categories,
 respectively.
 
\end_layout

\begin_layout Standard
The first step is to prove that there is no way for the mean of either sub-categ
ory (and therefore, the global mean) to have a value less than 
\emph on
N
\emph default
, or greater than 
\emph on
L
\emph default
.
 This follows from the mathematical form of the inertial, or attractor,
 forces.
 For values greater than the attractor location, the force is leftward,
 but for values smaller than the attractor location, the force is rightward,
 thus always acting to push the distribution precisely to the attractor
 location.
 If the sub-categories were completely independent (no global entrenchment),
 then they would always stabilize at their respective attractor locations.
 Entrenchment allows the sub-categories to be perturbed from their attractors,
 but only in the direction of the other sub-category.
 Thus, we can be confident that they will end up at equilibrium somewhere
 between 
\emph on
N
\emph default
 and 
\emph on
L
\emph default
.
 The exact location will depend on the parameters 
\begin_inset Formula $\alpha$
\end_inset

 (the strength of the attractor at 
\emph on
L
\emph default
), 
\begin_inset Formula $\beta$
\end_inset

 (the strength of the attractor at 
\emph on
N
\emph default
), 
\begin_inset Formula $\varepsilon$
\end_inset

 (the strength of the entrenchment force), and 
\begin_inset Formula $p$
\end_inset

 (the bias proportion: the percentage of the category consisting of biased
 tokens; in other words, the percentage of tokens produced in a biasing
 context).
 
\end_layout

\begin_layout Standard
The equilibrium location for the non-biased sub-category is determined by
 the point at which global entrenchment (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Entrenchment"

\end_inset

) is perfectly balanced by the attractor at 
\emph on
N
\emph default
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Inertia Force"

\end_inset

).
 Using the equilibrium location of the mean to stand in for the entire sub-categ
ory: 
\begin_inset Formula $\beta(N-\overline{x_{E}^{NB}})+\varepsilon(\overline{x_{E}}-\overline{x_{E}^{NB}})=0$
\end_inset

.
 Therefore, 
\begin_inset Formula $\beta(\overline{x_{E}^{NB}}-N)=\varepsilon(\overline{x_{E}}-\overline{x_{E}^{NB}})$
\end_inset

.
 For the biased distribution, it is the attractor at 
\emph on
L
\emph default
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Length Attractor"

\end_inset

) that will be balanced by global entrenchment (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Entrenchment"

\end_inset

): 
\begin_inset Formula $\alpha(\overline{x_{E}^{B}}-L)=\varepsilon(\overline{x_{E}}-\overline{x_{E}^{B}})$
\end_inset

.
 In order to solve for the three quantities, 
\begin_inset Formula $\overline{x_{E}^{B}}$
\end_inset

, 
\begin_inset Formula $\overline{x_{E}^{NB}}$
\end_inset

 , and 
\begin_inset Formula $\overline{x_{E}}$
\end_inset

 , we need a third equation linking them.
 This is given by the equation that expresses the global mean as a weighted
 average of the two sub-category means.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=(1-p)\overline{x_{E}^{NB}}+p\overline{x_{E}^{B}}\label{eq:weighted mean}
\end{equation}

\end_inset

We can now solve for each of the quantities in turn by substitution.
 Appendix C shows the full derivation, and demonstrates that the global
 mean at equilibrium, 
\begin_inset Formula $\overline{x_{E}}$
\end_inset

 , can be expressed in the following terms:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=\frac{(1-p)\beta N(\alpha+\varepsilon)+p\alpha L(\beta+\varepsilon)}{(\beta+\varepsilon)(\alpha+\varepsilon)-(\alpha+\varepsilon)(1-p)\varepsilon-(\beta+\varepsilon)p\varepsilon}\label{eq: sub-dist}
\end{equation}

\end_inset

as a function of the set of model parameters (
\begin_inset Formula $\alpha,\beta,N,L,p$
\end_inset

), the location of the global mean at equilibrium.
 The other quantity of interest is the distance between the sub-category
 means, which can be expressed as a function of 
\begin_inset Formula $\overline{x_{E}}$
\end_inset

 :
\begin_inset Formula 
\begin{equation}
\Delta\overline{x_{E}}\equiv\overline{x_{E}^{B}}-\overline{x_{E}^{NB}}=\frac{\alpha L+\varepsilon\overline{x_{E}}}{\alpha+\varepsilon}-\frac{\beta N+\varepsilon\overline{x_{E}}}{\beta+\varepsilon}\label{eq:State Model-sep}
\end{equation}

\end_inset

Keeping all other variables constant, we can now derive the behavior of
 these two quantities as a function of the bias proportion, 
\emph on
p
\emph default
.
 
\end_layout

\begin_layout Standard
The change in 
\begin_inset Formula $\overline{x_{E}}$
\end_inset

 that results from a change in 
\emph on
p
\emph default
 is given by taking the partial derivative of Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq: sub-dist"

\end_inset

).
 This turns out to be somewhat unwieldy to calculate in general form.
 In the special case when all forces have the same strength (
\begin_inset Formula $\alpha=\beta=\varepsilon$
\end_inset

), we can show that Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq: sub-dist"

\end_inset

) reduces to 
\begin_inset Formula $\overline{x_{E}}=N+p(L-N)$
\end_inset

.
 Therefore, the global category mean is a positive linear function of 
\emph on
p
\emph default
, and the change in the location of that mean is constant: 
\begin_inset Formula $\frac{\partial\overline{x_{E}}}{\partial p}=L-N$
\end_inset

.
 
\end_layout

\begin_layout Standard
For other cases, it's possible to determine the general behavior of 
\begin_inset Formula $\frac{\partial\overline{x_{E}}}{\partial p}$
\end_inset

 even without an exact solution.
 Assume that the equilibrium state for a given 
\begin_inset Formula $p=p_{j}$
\end_inset

 has already been determined.
 Now increase 
\emph on
p
\emph default
 to 
\begin_inset Formula $p_{k}$
\end_inset

.
 Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:weighted mean"

\end_inset

) entails that if 
\begin_inset Formula $p_{k}>p_{j}$
\end_inset

 (an increase in bias proportion), then the global mean will shift closer
 to the biased sub-category.
 Because the strength of the entrenchment force depends on the distance
 from the global mean, this shift will, in turn, cause the entrenchment
 force on the non-biased sub-category to increase.
 Because the attractor at 
\emph on
N
\emph default
 and the entrenchment force are taken to be perfectly balanced at 
\begin_inset Formula $p=p_{j}$
\end_inset

, an increase in the latter will result in a shift of the non-biased sub-categor
y toward the biased one.
 At the same time, the entrenchment force on the biased sub-category will
 decrease commensurately.
 In this case, a decrease in the entrenchment force causes the balance to
 shift in favor of the attractor at 
\emph on
L
\emph default
, meaning the biased sub-category will also shift in the rightward direction.
 Because the sub-categories shift in the same direction, the equilibrium
 point for the global mean is also guaranteed to shift in that direction,
 and thus to increase as 
\emph on
p
\emph default
 increases: 
\begin_inset Formula $\frac{\partial\overline{x_{E}}}{\partial p}>0$
\end_inset

.
 
\end_layout

\begin_layout Standard
In the special case where 
\begin_inset Formula $\alpha=\beta=\varepsilon$
\end_inset

, we can use the result that 
\begin_inset Formula $\frac{\partial\overline{x_{E}}}{\partial p}=L-N$
\end_inset

, and determine that 
\begin_inset Formula $\frac{\partial\Delta\overline{x_{E}}}{\partial p}=0$
\end_inset

.
 Therefore, the distance between the two sub-categories remains constant
 in this case.
 The general form of the partial derivative of (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:State Model-sep"

\end_inset

) with respect to 
\emph on
p,
\emph default
 
\begin_inset Formula $\frac{\partial\Delta\overline{x_{E}}}{\partial p}$
\end_inset

, can be written as a function of the partial derivative of 
\begin_inset Formula $\overline{x_{E}}$
\end_inset

 with respect to 
\emph on
p
\emph default
 (
\begin_inset Formula $\frac{\partial\overline{x_{E}}}{\partial p}$
\end_inset

):
\begin_inset Formula 
\begin{equation}
\frac{\partial\Delta\overline{x_{E}}}{\partial p}=\frac{\partial\overline{x_{E}}}{\partial p}\varepsilon[\frac{1}{\alpha+\varepsilon}-\frac{1}{\beta+\varepsilon}]\label{eq: Model G: dsep/dp}
\end{equation}

\end_inset

Since we know that 
\begin_inset Formula $\frac{\partial\overline{x_{E}}}{\partial p}$
\end_inset

 is always positive, the sign of 
\begin_inset Formula $\frac{\partial\Delta\overline{x_{E}}}{\partial p}$
\end_inset

 is determined by the sign of 
\begin_inset Formula $\frac{1}{\alpha+\varepsilon}-\frac{1}{\beta+\varepsilon}$
\end_inset

.
 The sign of 
\begin_inset Formula $\frac{1}{\alpha+\varepsilon}-\frac{1}{\beta+\varepsilon}$
\end_inset

 is determined by the relative sizes of the quantities 
\begin_inset Formula $\alpha+\varepsilon$
\end_inset

, and 
\begin_inset Formula $\beta+\varepsilon$
\end_inset

.
 Therefore, if 
\begin_inset Formula $\alpha>\beta$
\end_inset

 then 
\begin_inset Formula $\frac{\partial\Delta\overline{x_{E}}}{\partial p}<0$
\end_inset

; and if 
\begin_inset Formula $\alpha>\beta$
\end_inset

 , then 
\begin_inset Formula $\frac{\partial\Delta\overline{x_{E}}}{\partial p}<0$
\end_inset

.
\end_layout

\begin_layout Subsection
\begin_inset CommandInset label
LatexCommand label
name "subsec:Model-B:-Lengthening"

\end_inset

Process Model: Single Category
\end_layout

\begin_layout Standard
The Pure Process Model contains a single category, and a single target for
 that category.
 Tokens are selected at random, with probability 
\emph on
p,
\emph default
 to be produced in the biasing context.
 This model is identical to the Soft Target Model described in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Soft-Targets"

\end_inset

 (Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Model2:LengtheningProcess"

\end_inset

).
 It will be shown in this section that the Process Model is only stable
 for certain parameter values.
 The behavior of the global mean, and the average separation between biased
 and non-biased productions, will be derived as before.
 The same simplifying assumption that the category can be approximated as
 a Normal distribution with fixed variance will also be made.
 However, it should be noted that this assumption is less justified for
 the one-category model due to the fact that biased and non-biased variants
 will separate, creating a lumpier distribution, and likely an increase
 in variance.
\end_layout

\begin_layout Standard
The derivational steps in this analysis are given graphically in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Derivation"

\end_inset

.
 Panel 1 is a snapshot of the model at some time, 
\emph on

\begin_inset Formula $t$
\end_inset


\emph default
, when the global mean is located at location 
\begin_inset Formula $\overline{x_{t}}$
\end_inset

 along 
\emph on
x
\emph default
.
 Both biased and non-biased tokens are sampled from this distribution, at
 different rates.
 This relationship is indicated by the darker normal curve (subset of tokens
 subjected to bias during production) within the lighter one (subset of
 tokens non-biased during production).
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename Model1Behavior.pdf
	width 75page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Derivation"

\end_inset

Schematic of forces for 
\noun on
process
\noun default
 model
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The production value of any token at any time 
\emph on
t
\emph default
 can be calculated, as long as its current value, and the category mean,
 are known.
 All tokens are subject to the attractor at 
\emph on
N
\emph default
.
 And all tokens are subject to the entrenchment force acting to pull them
 closer to the current category mean (which will be greater than, or equal
 to, 
\emph on
N
\emph default
).
 Additionally, a proportion 
\emph on
p
\emph default
 of randomly selected tokens undergo a lengthening process, moving away
 from the rest of the distribution during production.
\end_layout

\begin_layout Standard
Panels 2-4 of Fig.
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Derivation"

\end_inset

 take us sequentially through the application of forces.
 Panel 2 isolates the effect of applying the attractor and lengthening forces.
 The attractor affects all tokens equally, because all tokens are equally
 far from 
\emph on
N
\emph default
 on average.
 Lengthening, applied only to a subset of tokens, splits the distribution
 apart.
 Before entrenchment applies, the mean values for the observed productions
 (
\begin_inset Formula $\overline{x_{t}^{B}}^{\prime}$
\end_inset

, mean of biased productions in Panel 2; 
\begin_inset Formula $\overline{x_{t}^{NB}}^{\prime}$
\end_inset

 mean of non-biased productions in Panel 2), can each be given as a function
 of the global mean at time 
\begin_inset Formula $t$
\end_inset

, 
\begin_inset Formula $\bar{x_{t}}$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{t}^{B}}^{\prime}=\bar{x_{t}}(1+\alpha)+\beta(N-\bar{x_{t}})\label{eq:Biased}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{t}^{NB}}^{\prime}=\bar{x_{t}}+\beta(N-\bar{x_{t}})\label{eq:Non-biased}
\end{equation}

\end_inset

Applying entrenchment does not affect the global mean, only the absolute
 locations of the sub-distribution means, and their separation.
 Therefore, the mean in Panel 3 is identical to the mean from Panel 2.
 This mean (
\begin_inset Formula $\bar{x_{t}}^{\prime}$
\end_inset

) is given by the weighted average of the means of the observed production
 variants: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\bar{x_{t}}^{\prime}=(1-p)\overline{x_{t}^{NB}}^{\prime}+p\overline{x_{t}^{B}}^{\prime}\label{eq: G weighted mean}
\end{equation}

\end_inset

Equilibrium is achieved when continued iterations fail to change the locations
 of the means.
 This means that they should be unaffected by successive iterations of biasing.
 
\begin_inset Formula $\overline{x_{E}}^{\prime}=\overline{x_{E}}$
\end_inset

 , 
\begin_inset Formula $\overline{x^{B}}^{\prime}=\overline{x^{B}}$
\end_inset

, and 
\begin_inset Formula $\overline{x^{NB}}^{\prime}=\overline{x^{NB}}$
\end_inset

.
 Therefore, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=(1-p)\overline{x_{E}^{NB}}^{\prime}+p\overline{x_{E}^{B}}^{\prime}\label{eq:equilbrium 1}
\end{equation}

\end_inset

and from Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Biased"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Non-biased"

\end_inset

), 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=(1-p)\{\overline{x_{E}}+\beta(N-\overline{x_{E}})\}+p\{\overline{x_{E}}(1+\alpha)+\beta(N-\overline{x_{E}})\}\label{eq:equilbrium 2}
\end{equation}

\end_inset

Solving for 
\begin_inset Formula $\overline{x_{E}}$
\end_inset

 gives
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=\frac{\beta N}{\beta-p\alpha}\label{eq: lengthening process}
\end{equation}

\end_inset

See Appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Appendix D"

\end_inset

 for the full derivation.
 
\end_layout

\begin_layout Standard
The behavior of the global mean as a function of 
\emph on
p
\emph default
 will depend on which region of parameter space we are in: 
\begin_inset Formula $p\alpha<\beta$
\end_inset

, or 
\begin_inset Formula $p\alpha>\beta$
\end_inset

.
 For 
\begin_inset Formula $p\alpha<\beta$
\end_inset

, the denominator in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq: lengthening process"

\end_inset

) is positive.
 Therefore, as 
\emph on
p
\emph default
 increases (but 
\begin_inset Formula $p\alpha$
\end_inset

 stays smaller than 
\begin_inset Formula $\beta$
\end_inset

), the denominator decreases, and the global mean increases (
\begin_inset Formula $\frac{\partial\overline{x_{E}}}{\partial p}>0$
\end_inset

).
 In the limit, as 
\begin_inset Formula $p\alpha$
\end_inset

 goes to 
\begin_inset Formula $\beta$
\end_inset

, the equilibrium mean goes to infinity, and lengthening is unbounded.
 For 
\begin_inset Formula $p\alpha>\beta$
\end_inset

 , the denominator is negative, which also means that the mean is negative,
 and the only equilibrium point is negative.
 Since negative duration values aren't possible, there is no well-defined
 equilibrium in the range in which 
\begin_inset Formula $p\alpha>\beta$
\end_inset

.
 The 
\noun on
process
\noun default
 model is thus only stable if the lengthening strength (
\begin_inset Formula $\alpha$
\end_inset

) is not too great, and the percentage of biasing contexts (
\begin_inset Formula $p$
\end_inset

) is not too large, relative to the attractor strength (
\begin_inset Formula $\beta$
\end_inset

).
\end_layout

\begin_layout Standard
To calculate the second quantity of interest: the dependence of sub-distribution
 separation on 
\emph on
p,
\emph default
 the effect of entrenchment must be included.
 Entrenchment acts to bring all tokens back towards the global mean by an
 amount proportional to their distance from that mean (Panel 3 of Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Derivation"

\end_inset

): 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x^{B}}^{\prime\prime}=\overline{x^{B}}^{\prime}+\varepsilon(\overline{x}^{\prime}-\overline{x^{B}}^{\prime})\label{eq:Panel 3 biased}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x^{NB}}^{\prime\prime}=\overline{x^{NB}}^{\prime}+\varepsilon(\overline{x}^{\prime}-\overline{x^{NB}}^{\prime})\label{eq:Panel 3 unbiased}
\end{equation}

\end_inset

The separation between the two production variants after entrenchment applies
 (
\begin_inset Formula $\triangle\overline{x}^{\prime\prime}$
\end_inset

) can be determined by taking the difference between Equations (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Panel 3 biased"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Panel 3 unbiased"

\end_inset

): 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\triangle\overline{x}^{\prime\prime}\equiv\overline{x^{B}}^{\prime\prime}-\overline{x^{NB}}^{\prime\prime}=(1-\varepsilon)(\overline{x^{B}}^{\prime}-\overline{x^{NB}}^{\prime})
\end{equation}

\end_inset

The final separation depends only on the separation prior to the application
 of entrenchment (
\begin_inset Formula $\triangle\overline{x}^{\prime}$
\end_inset

 in Panel 2), and the strength of the entrenchment term, 
\begin_inset Formula $\varepsilon$
\end_inset

.
 Because the sub-distributions only exist at production, no cumulativity
 in separation is possible (see Panel 4).
 Therefore, at all times 
\emph on
t
\emph default
, the separation in Panel 2, prior to entrenchment, will always be given
 by the lengthening factor: 
\begin_inset Formula $\alpha\overline{x_{t}}$
\end_inset

.
 Therefore, at equilibrium, when 
\begin_inset Formula $\overline{x_{t}}=\overline{x_{E}}$
\end_inset

, the average distance between the two production sub-distributions is given
 by
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\varDelta\overline{x_{E}}=(1-\epsilon)(\alpha\overline{x_{E}}).\label{eq:Cat Sep}
\end{equation}

\end_inset

In the stable parameter range (
\begin_inset Formula $p\alpha>\beta$
\end_inset

), where 
\begin_inset Formula $\overline{x_{E}}$
\end_inset

 increases as 
\emph on
p
\emph default
 increases, the separation of the sub-distributions also increases, but
 more slowly, by a factor of 
\begin_inset Formula $(1-\epsilon)\alpha$
\end_inset

.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Actuation"

\end_inset

Change between Stable States
\end_layout

\begin_layout Standard
Most work in the exemplar framework models either change or stability, but
 not both.
 That is to say, only one stable state is possible, and the model either
 starts in that state, in which case it remains there for all time, or inevitabl
y arrives in that state from any other starting conditions.
 In 
\begin_inset CommandInset citation
LatexCommand citet
key "Garrett2013"

\end_inset

 there are two different modes of processing
\begin_inset Foot
status open

\begin_layout Plain Layout
These are likened to 
\begin_inset Quotes eld
\end_inset

speech
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

non-speech
\begin_inset Quotes erd
\end_inset

 processing modes (
\begin_inset CommandInset citation
LatexCommand citealt
key "liberman1967perception"

\end_inset

); individual speakers may switch between the two modes, or different speakers
 may operate consistently in one or the other mode (e.g.
 
\begin_inset CommandInset citation
LatexCommand citealt
key "yu2013socio"

\end_inset

).
 
\end_layout

\end_inset

, resulting in essentially two different models: one in which normalization
 occurs, which is stable, and one in which normalization is 
\begin_inset Quotes eld
\end_inset

turned off
\begin_inset Quotes erd
\end_inset

, leading to change (the latter model is not implemented, but would lead
 to unbounded shift without an additional mechanism).
 
\begin_inset CommandInset citation
LatexCommand citet
key "Kirby2014"

\end_inset

 is similar in that two different outcomes are possible, one for a `misparsing'
 mode, and one for accurate parsing (In the `misparsing' mode, merger is
 prevented by a stage of hypothesis selection in which a Bayesian learner
 updates phonetic cue weights so as to optimize categorization accuracy).
 
\end_layout

\begin_layout Standard
\begin_inset Quotes eld
\end_inset

Agent-based
\begin_inset Quotes erd
\end_inset

 models (not all implemented using exemplar representations) use the interaction
 among one or more groups of speakers to be the driving mechanism, either
 of the evolution of language itself, or of the evolution of pre-existing
 variants (which may be parameters, or entire grammars).
 Systems are taken to be stable within individual speakers, that is, without
 bias.
 Thus there is no mechanism via which a truly novel form can arise, only
 ways in which an existing distribution can evolve within a heterogeneous
 population (
\begin_inset CommandInset citation
LatexCommand citealt
key "Niyogi1997,Boer2000,nowak2001evolution,Steels2005,baxter2006utterance,oudeyer2006self,fagyal2010centers,stanford2013revisiting,pierrehumbert2014model"

\end_inset

).
 Models that rely on simple `self-organizing' principles, such as random
 selection, or mis-classification, are usually designed to demonstrate that
 a single optimal state will be reached from any starting position (
\begin_inset CommandInset citation
LatexCommand citealp
key "Wedela,ettlinger2007exemplar,Wedel2006,Blevins2009,DBLP:journals/corr/Tupper14a,wedel2017category"

\end_inset

).
 Some additional mechanism would be needed to change such systems further.
 Effectively, actuation is achieved either through speaker contact (in which
 adoption of already existing variants may occur), or by initializing the
 model in an unstable state.
 
\end_layout

\begin_layout Standard
As far as I am aware, 
\begin_inset CommandInset citation
LatexCommand citet
key "soskuthy2013phonetic"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "Soskuthy2015"

\end_inset

 are unique in the literature in that they capture both change and stability
 within a single model.
 Actuation occurs via a completely speaker-internal mechanism that is an
 integral component of the model: allophone frequency.
 Model 1 in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-1:-Context-Free"

\end_inset

 was an instantiation of frequency of use as an instigator of change, in
 the successive reduction of highly frequent words.
 In that model, frequency was a fixed property of a given word type.
 But changes in word frequency, as well as in the relative proportion of
 contextual variants, are possible for independent reasons.
 Words go in and out of style, and the frequency of use of any given word
 is expected to change over time.
 In turn, changes in frequency at the word level also affect the frequency
 of occurrence of the phonemes that make up the word.
 For changes that happen to affect a group of words with the relevant allophonic
 environment, a change in token proportion between multiple allophones of
 a given phoneme could result.
 
\end_layout

\begin_layout Standard
The model of vowel lengthening in 
\begin_inset CommandInset citation
LatexCommand citet
key "soskuthy2013phonetic"

\end_inset

 was the basis for the gradient context-dependent model first introduced
 in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Context-Dependent-Iterativity"

\end_inset

.
 This model was gradually developed, first into a set of possible models
 implementing at least one soft target, then into a subset of those that
 were both stable and theoretically consistent.
 The remaining two models were then implemented with frequency of allophonic
 environment (bias proportion) as the actuator of change.
 
\begin_inset CommandInset citation
LatexCommand citet
key "soskuthy2013phonetic"

\end_inset

 is actually closest to Model E (Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Model-Interpretation"

\end_inset

), as a two-target 
\noun on
state
\noun default
 model.
 The vowel-level category is modeled as a mixture of Gaussians, namely the
 sub-category of variants that occur in the lengthening context, and the
 sub-category of variants that occur in the non-lengthening context.
 Instead of global entrenchment, the link to the superset category is implemente
d by applying lengthening stochastically to tokens chosen from both sub-categori
es, but with the `long' sub-category more strongly weighted.
 Additionally, a 
\begin_inset Quotes eld
\end_inset

centering bias
\begin_inset Quotes erd
\end_inset

, implemented as an attractor at 
\emph on
N
\emph default
, is used to prevent unbounded dispersion
\begin_inset Foot
status open

\begin_layout Plain Layout
This is necessary to counteract the contrast maintenance pressure that pushes
 categories away from one another, via elimination of ambiguous tokens (
\begin_inset CommandInset citation
LatexCommand citet
key "Wedel2008"

\end_inset

 and 
\begin_inset CommandInset citation
LatexCommand citet
key "Blevins2009"

\end_inset

).
 
\end_layout

\end_inset

.
 The mathematical form of the attractor function is equivalent to the soft
 target first introduced in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Soft-Targets"

\end_inset

: an inertial force that applies when tokens are perturbed from an underlyingly
 specified position, acting to pull them back towards that position.
 This results, functionally, in two targets for the biased sub-category
 (one at 
\emph on
N
\emph default
 and one at 
\emph on
L
\emph default
)
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset CommandInset citation
LatexCommand citet
key "Soskuthy2015"

\end_inset

 employs a similar architecture.
 There is an explicit target for only the biased sub-category, but all tokens
 are affected by the same centering force.
 In this model, hard thresholds at 0 and 1 act to force both distributions
 back towards the center, similarly to how a target attracts tokens from
 either direction.
 These attractors are critical to achieving stable states in both models.
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
Sóskuthy's model therefore differs from the Pure State Model implemented
 in Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Lengthening-as-State"

\end_inset

.
 The actual model behavior, however, turns out to be quite similar.
 As we saw in the previous section, changes in bias proportion – how often
 the biasing context occurs relative to the non-biasing context – act to
 shift the model from one stable state to another.
 The global mean of the vowel category always increases with increasing
 
\emph on
p
\emph default
, but the separation between the `lengthened' and `unlengthened' variants
 can increase, decrease, or stay the same, depending on other model parameters
\begin_inset Foot
status open

\begin_layout Plain Layout
In Sóskuthy's models there is a somewhat more complex dependence on 
\emph on
p
\emph default
.
 
\end_layout

\end_inset

.
 Under the assumption that parameter values are fixed within a given speaker,
 only one of those outcomes will actually be possible for each individual.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "subsec:Phoneme-Split"

\end_inset

Phoneme Split
\end_layout

\begin_layout Standard
Existing exemplar models of change are actually models of phonetic, rather
 than phonological, change.
 The framework offers the possibility that low-level synchronic variation,
 like phonetic nasalization, can successively accumulate, leading to large-scale
 change.
 However, the basic framework does not, in and of itself, offer a solution
 to the actuation problem at the phonological level.
 We know that new phonological categories can form over time, and this seems
 to happen when phonetic allophones achieve independence from their parent
 categories.
 Thus, the outcome in which lengthened vowels become contrastive long vowels,
 and nasalized vowels become contrastive nasal vowels, is of particular
 interest.
 It has been proposed that phoneme genesis is triggered by a subset of phonetic
 variants that have shifted sufficiently far from the rest of the distribution
 (
\begin_inset CommandInset citation
LatexCommand citealt
key "Janda2003,Janda2008"

\end_inset

).
 This is essentially what is assumed in 
\begin_inset CommandInset citation
LatexCommand citet
key "Wedel2008"

\end_inset

, with phonemic contrast equated to the emergence of a bi-modal distribution.
 However, as we saw in the 
\noun on
state
\noun default
 model of Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Lengthening-as-State"

\end_inset

, `long' tokens can never get longer than their attractor at 
\emph on
L
\emph default
, and the distance between the two sub-categories is similarly constrained
 by the distance between the two attractors.
 This is problematic if phonetic exaggeration or 
\begin_inset Quotes eld
\end_inset

enhancement
\begin_inset Quotes erd
\end_inset

 is necessary to initiate a new phonological category.
 The 
\noun on
process
\noun default
 model seems to offer more potential for phonological change if lengthening
 can be somehow turned off right after biased tokens achieve sufficient
 separation from the non-biased part of the distribution.
 In fact, what is needed to model phoneme split with the current set of
 models is precisely a mechanism that will enact the necessary representational
 changes needed to convert a 
\noun on
process
\noun default
 model (allophony) to a 
\noun on
state
\noun default
 model (contrast)
\begin_inset Foot
status open

\begin_layout Plain Layout
Going from a 
\noun on
state
\noun default
 to a 
\noun on
process
\noun default
 model, on the other hand, requires that independent categories become linked
 through the inference of a predictable relationship between them.
 In one sense, phoneme merger is clearly the opposite of phoneme split in
 that the former reduces the number of independent categories, while the
 latter increases them.
 However, phoneme merger is not equivalent to (re-)establishing an allophonic
 relationship.
 As far as I am aware, merger is taken to be the result of phonetic overlap
 among distinct categories (that may or may not share allophones) involving
 the wholesale replacement of one category with another occupying the exact
 same phonetic space.
 A change from a 
\noun on
state
\noun default
 to a 
\noun on
process
\noun default
 therefore, may be a different kind of change, and perhaps one that has
 no exact correspondent in the standard taxonomy of sound change.
 This is an intriguing avenue for future work.
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
In addition to the question of how the transition from 
\noun on
process 
\noun default
to
\noun on
 state
\noun default
 can occur, there is the separate question of the level at which the 
\noun on
state
\noun default
 is specified.
 Features, such as [
\emph on
voice
\emph default
], or [
\emph on
nasal
\emph default
], are usually considered to be the universal atoms from which all phonemes
 are constructed.
 However, a given rule, or process, acts over some set of phonemes within
 a given language.
 Each individual phoneme consists of a unique matrix of feature values,
 but the phoneme class is specified by the subset of feature values that
 all members share (comprising a natural class).
 In principle, any combination of feature values for any subset of features
 could be a natural class that is linguistically relevant in some language.
 Yet the number of such classes that are actually used, or active, within
 a given language is much smaller.
 Furthermore, the existence, or activity, of a particular natural class
 within a language is identified only by the fact that all and only the
 phonemes that belong to that class behave identically with respect to some
 rule.
 It is uncontroversial that the rule must be learned by the speaker of the
 language, and therefore, which natural class is associated with the rule
 must also be learned.
 Thus, it is not unlikely that the natural class itself is learned, or formed,
 at the time the rule is learned.
 This view is further supported by the possible existence of 
\begin_inset Quotes eld
\end_inset

unnatural
\begin_inset Quotes erd
\end_inset

 classes (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "Mielke2008"

\end_inset

).
\end_layout

\begin_layout Standard
In the case of vowel lengthening, the relevant class of segments that undergo
 the rule consists of the natural class that specifies all and only vowels.
 The class of segments that act as the trigger, or environment, for the
 rule is the set of all non-continuant non-nasal voiced segments.
 In both the 
\noun on
state
\noun default
 and 
\noun on
process
\noun default
 models this sub-category is explicitly represented, and in fact, the models
 are initialized with this representation
\begin_inset Foot
status open

\begin_layout Plain Layout
It is worth noting that [
\emph on
vowels before everything else
\emph default
] does not actually comprise a natural class due to its disjoint nature,
 consisting of the union of the following natural classes: [
\emph on
vowels before continuant
\emph default
s], [
\emph on
vowels before nasals
\emph default
], and [
\emph on
vowels before voiceless non-continuants
\emph default
].
 In descriptions of the phenomenon, the comparison class is typically non-contin
uants that are voiceless, and this is likely to be assumed as the relevant
 second sub-category for modeling purposes.
\end_layout

\end_inset

.
 This assumption begs the sound change question to a large extent.
 If there was a prior period in which no rule of vowel lengthening existed,
 then the more interesting question might be where it came from in the first
 place.
 In other words, how did precisely this natural class, this sub-category
 of phonological units, become linguistically active in this language.
 But because this is the starting point for these models, there is no mechanism
 for generating new allophonic relationships, or for eliminating them altogether
\begin_inset Foot
status open

\begin_layout Plain Layout
Treating sub-categorization as a phonetic, rather than a phonemic, distinction
 does not solve this problem if the necessary structure is still stipulated,
 and the prior existence of the allophonic rule is assumed (e.g.
 
\begin_inset CommandInset citation
LatexCommand citealp
key "dillon2013single"

\end_inset

).
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
How abstract categories are formed in the first place, and how many, with
 what kinds of sub-structures, are questions that are far from being definitivel
y answered (see, among others, 
\begin_inset CommandInset citation
LatexCommand citet
key "Peperkamp2006,dillon2013single,feldman2009learning,mcmurray2011information,goldsmith2009learning"

\end_inset

).
 It is reasonable to expect that greater knowledge of how categories are
 formed will lead to greater insight into how sound changes occur, and what
 kinds of sound changes are possible.
 It is beyond the scope of this paper to propose a general theory of category
 formation.
 However, in the next chapter, we will explore some models in which the
 basic units to which forces apply are distinct from the featural description
 of the linguistic phenomenon.
 In Chapters 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Perception-Production"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Phoneme-Split"

\end_inset

 we will also modify, or replace, many of the assumptions explicitly laid
 out in Chapters 1- 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Models-of-Change"

\end_inset

, including the very definition of phoneme split.
\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "sec:Perception-Production"

\end_inset

The Relationship between Perception and Production
\end_layout

\begin_layout Standard
In the models examined to this point, the tokens of perception have been
 assumed to be identical to the tokens of production.
 This assumption obscures the fact that targets in production are necessary
 in order for sounds to be produced at all, i.e., that read-out of a stored
 set of acoustic values is not possible.
 It also conflates biases that act in production, with those that act in
 perception, requiring them to act on the same units.
 Furthermore, this assumption requires that complex articulatory dynamics
 be uniquely and transparently realized acoustically.
 For a dimension like segment duration, this assumption may not be too unreasona
ble.
 However, the correspondence between articulation and acoustics is well
 known to be a many-to-many mapping.
 Invariant cues to abstract phonemes have failed to be discovered in either
 domain.
 
\end_layout

\begin_layout Standard
Starting at least with 
\begin_inset CommandInset citation
LatexCommand citet
key "Goldinger1996"

\end_inset

, it has been assumed that experienced exemplars are stored as motor plans
 without intermediate processing.
 While this may be adopted largely as an implementational convenience, it
 is based on the assumption that the true details of the mapping will not
 significantly affect the mechanism of change, or the model outcomes (see
 
\begin_inset CommandInset citation
LatexCommand citealt
key "Pierrehumbert2000"

\end_inset

).
 Perhaps the most critical assumption is that they will not affect the feedback
 loop that is the driving mechanism of such models.
 However, as this chapter will demonstrate, a non-trivial perception to
 production mapping is not just an additive factor that can be slotted into
 existing models, but a shift in perspective that affects all aspects of
 modeling, up to and including what we take to be the source of sound change
 itself.
 The following sections will make these ramifications explicit for three
 cases representing three different types of phonetic bias (two of which
 have been previously modeled): vowel lengthening (duration-based targets);
 vowel nasalization (sequencing of different articulators); and velar palataliza
tion (sequencing of different targets for the same articulator).
 
\end_layout

\begin_layout Section
Duration-based Targets
\end_layout

\begin_layout Standard
The lack of motivation for a process by which a production, at some random
 point along the relevant dimension, is moved only a small amount towards
 its target, was mentioned briefly in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Model-Interpretation"

\end_inset

.
 Failure to completely achieve a target may not seem paradoxical at first
 glance, because it suggests a well-known articulatory phenomenon, known
 as 
\begin_inset Quotes eld
\end_inset

undershoot
\begin_inset Quotes erd
\end_inset

, in which targets fail to be completely achieved (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "Lindblom1963"

\end_inset

).
 But this is not an equivalent process
\begin_inset Foot
status open

\begin_layout Plain Layout
The centering bias in 
\begin_inset CommandInset citation
LatexCommand citet
key "Wedel2008"

\end_inset

 is characterized as a 
\begin_inset Quotes eld
\end_inset

lenition bias towards the center of each segment dimension
\begin_inset Quotes erd
\end_inset

.
 Because Wedel's categories lack underlying targets (they are randomly generated
 and evolve as poor, or ambiguous, tokens are discarded), his lenition bias
 is the mechanism that prevents categories from dispersing indefinitely.
 For a two-dimensional phonetic vowel space composed of the first and second
 formant frequencies, a centralizing bias is fairly consistent with undershoot.
 However, this bias is implemented as a fixed attractor location, rather
 than a process that shifts the vowel formants a small amount towards the
 center of formant space on each production.
 This suggests that there is a target, or ideal, vowel location from which
 all vowels are perturbed by other forces.
 
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Undershoot can occur if over-all speech rate is rapid, not allowing enough
 time to overcome the inertia inherent in the physical articulators, or
 if sequential targets involving the same articulator (e.g., tongue body)
 are far apart in the mouth.
 A duration target, however, cannot be undershot in the same way.
 In the first place, segment duration 
\emph on
per se
\emph default
 is not specified on individual articulators or their configurations.
 Furthermore, duration is not absolute.
 Thus, although a faster speaking rate will lead to shorter vowel durations,
 it will also shorten all segments in all contexts, meaning that the relative
 difference between vowel durations in pre-voiced versus pre-voiceless contexts
 will not necessarily be affected (unless duration values are at floor or
 ceiling).
 A speaking rate transformation effectively changes the location of the
 target itself in absolute terms; it does not affect the speaker's ability
 to reach that target for any given token.
 Length-based features are arguably better modeled by targets that are a
 function of speaking rate.
 
\end_layout

\begin_layout Standard
With respect to the effect of frequency on duration, the mechanism, and
 its interaction with speaking rate, remains somewhat unclear.
 In models of frequency effects, speaking rate does not seem to be considered.
 Yet the parallels between the two are clear.
 The conceptualization of frequency of use as repeated practice suggests
 that there exists a maximally fluent, or optimal, production target.
 While increased frequency should not reduce any word below that target,
 increased speaking rate might.
 If frequency of use translates to higher resting activation, on the other
 hand, and higher resting activation leads to faster production, successive
 shortening should only occur if listeners fail to normalize for speaking
 rate; and if they fail to normalize for speaking rate, then the stored
 distribution will reflect the typical variance in speaking rate.
 This issue will be taken up in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Phoneme-Split"

\end_inset

, with two different implementations of the frequency effect.
\end_layout

\begin_layout Section
Coordination of Independent Articulators
\end_layout

\begin_layout Standard
In the vowel nasalization example of Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-3:-Nasalization"

\end_inset

 it was assumed that nasalization occurred when a given vowel token was
 produced adjacent to a nasal consonant, transforming from completely oral
 (
\begin_inset Formula $[-nasal]$
\end_inset

), to completely nasal (
\begin_inset Formula $[+nasal$
\end_inset

]).
 This simulation was useful for illustrating the context mismatch that would
 result from nasalized tokens being produced in a non-nasal context (which
 occurs whether nasality is considered binary or not).
 Our current purpose, however, is to consider how an articulatory phenomenon
 like nasality could be modeled iteratively with the classic perception-producti
on loop.
 
\end_layout

\begin_layout Standard
In most exemplar models `phonetic bias' is taken to apply without limit,
 and without regard to input values.
 That is, lengthening will occur regardless of how long the vowel already
 is, provided it occurs in a pre-voiced context.
 For the phenomenon of vowel nasalization this requires some partial nasalizatio
n that applies whenever a vowel is produced preceding a nasal consonant,
 a partial nasalization that is additive in nature.
 This is schematized in (
\begin_inset CommandInset ref
LatexCommand ref
reference "Ex: Iterative Nasality"

\end_inset

).
 
\end_layout

\begin_layout Numbered Example (multiline)
\begin_inset CommandInset label
LatexCommand label
name "Ex: Iterative Nasality"

\end_inset


\begin_inset Formula $Nasalization(V)\rightarrow V^{+N}$
\end_inset


\end_layout

\begin_layout Numbered Example (multiline)
\begin_inset Formula $Nasalization(V^{+N})\rightarrow V^{+2N}$
\end_inset


\end_layout

\begin_layout Numbered Example (multiline)
\begin_inset Formula $Nasalization(V^{+2N})\rightarrow V^{+3N}$
\end_inset


\end_layout

\begin_layout Standard
But this type of acoustic cumulativity is only possible under a very specific,
 and unlikely, production model.
 
\end_layout

\begin_layout Standard
As first described in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-3:-Nasalization"

\end_inset

, phonetic vowel nasalization is the product of the coarticulation that
 occurs throughout normal speech.
 Sounds are not produced in strict sequence but overlap considerably with
 their neighbors.
 In the case of a vowel-nasal sequence, the velum, or soft palate, is raised
 in anticipation of the nasal segment before the vowel gesture has completed,
 resulting in airflow through the nasal cavity during at least part of the
 vowel's production.
 To represent the articulatory side of this phenomenon, and draw a clear
 distinction between perceived tokens and their correspondents in production,
 we will make use of the representational tools of Articulatory Phonology
 (AP) (
\begin_inset CommandInset citation
LatexCommand citealt
key "Browman1986,Browman1990"

\end_inset

).
 
\end_layout

\begin_layout Standard
In AP, the abstract representational units of speech are taken to be analogous
 to musical scores, which indicate the coordination and ordering of a series
 of physical movements (articulatory gestures).
 Those gestures involve a set of active articulators – the tongue, velum,
 glottis, etc.
 – usually in relation to a set of passive articulator locations – the teeth,
 lips, hard palate, etc.
 Scores consist of a series of target locations for each active articulator
 (e.g., the alveolar ridge behind the teeth), and timing relations between
 those movements (e.g., begin movement of tongue tip at midpoint of open glottis
 gesture).
 Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Normal nasalization"

\end_inset

 depicts a gestural score for nasal coarticulation, based on the specific
 sequence 
\begin_inset IPA

\begin_layout Standard
/æm/
\end_layout

\end_inset

.
 Time is represented along the x-axis, and the active articulators are shown
 on the y-axis (TB=Tongue Body; VEL=velum).
 The box adjacent to each active articulator represents the time span during
 which that articulator is activated: gradually moving towards it target
 position, then away to a subsequent target, or resting state.
 The interval during which the boxes overlap indicates the period when the
 two articulators are active at the same time.
 This overlap, indicated by the space between the dotted lines in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Normal nasalization"

\end_inset

, is the source of the vowel nasalization of interest.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename nasalization̘1.pdf
	width 25page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Normal nasalization"

\end_inset

Nasalization of underlyingly oral vowel token 
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename nasalization̘2.pdf
	width 25page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:nasalized vowel"

\end_inset

Underlyingly nasalized vowel token (unnormalized)
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename nasalization̘3.pdf
	width 25page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:extra nasalization"

\end_inset

Nasalization of underlyingly nasal vowel token
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Coarticulation"

\end_inset

Coarticulation involving different articulators
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The gestural score indicated in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Normal nasalization"

\end_inset

 produces the acoustic realization 
\begin_inset IPA

\begin_layout Standard
[æ̃m]
\end_layout

\end_inset

.
 The vocalic portion of this token, if stored without normalization, is
 represented by 
\begin_inset IPA

\begin_layout Standard
/æ̃/
\end_layout

\end_inset

.
 The two different types of brackets are used here in exactly their usual
 sense: square brackets indicate a surface form, an instance of speech,
 while forward slashes indicate an underlying form, a form used to generate
 a speech act.
 Before such an acoustic token can be produced, however, it must be converted
 to an articulatory representation.
 This is shown in Fig.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:nasalized vowel"

\end_inset

).
 Note that, despite the fact that the nasalization is now a property of
 the vowel itself, the same two articulatory gestures are still required
 in production.
\end_layout

\begin_layout Standard
At some still later model cycle, when the token represented in (
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:nasalized vowel"

\end_inset

) is chosen for production in the identical nasal context, the combined
 articulatory score is realized as Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:extra nasalization"

\end_inset

.
 Under error-free perception and production, the velum gesture associated
 with the vowel (indicated by diagonal fill lines), and the velum gesture
 associated with the nasal will overlap completely.
 And because there is only one velum, there will be only one velum gesture.
 Acoustically, this will result in exactly the same amount of nasality on
 the vowel as before (
\begin_inset IPA

\begin_layout Standard
[æ̃]
\end_layout

\end_inset

).
 The feedback loop is, in fact, halted after a single iteration.
 The only way that the vowel could become successively more nasalized is
 if the velum were to begin raising earlier and earlier in time – in other
 words, if a successive change in the timing relationship were to occur
\begin_inset Foot
status open

\begin_layout Plain Layout
Differences in the amount of velar opening and degree of velar airflow can
 be found among different types of nasalized vowels (
\begin_inset CommandInset citation
LatexCommand citealt
key "bell1993understanding,hajek2000vowel"

\end_inset

).
 But this is a property of a given vowel.
 There is no reason for the greater degree of velar opening for a low vowel,
 for example, to be increased further each time that vowel is produced preceding
 a nasal.
\end_layout

\end_inset

.
 But a change of this nature requires independent motivation.
 In other words, the iterative result does not come for free when the acoustic-a
rticulatory mapping is no longer an identity relation.
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Competing-targets"

\end_inset

Competing Targets for the Same Articulator
\end_layout

\begin_layout Standard
The final case of perception-to-production mapping considered here is one
 that contains conflicting consecutive specifications for a single articulator.
 A common phenomenon of this type is palatalization, which involves the
 tongue shifting towards the hard palate (either forward or backward) due
 to the influence of a following or preceding segment (
\begin_inset CommandInset citation
LatexCommand citealt
key "Guion1998,Keating1993"

\end_inset

).
 Palatalization often occurs in sequences of obstruent consonants and high
 vowels.
 For example, in the articulation of the sequence 
\begin_inset IPA

\begin_layout Standard
/ki/
\end_layout

\end_inset

, the articulatory target of the 
\begin_inset IPA

\begin_layout Standard
/k/
\end_layout

\end_inset

 is the velum, or soft palate, where the tongue body makes contact, briefly
 creating a complete closure in the oral cavity.
 The articulatory target for the vowel is closer to the hard palate, where
 the tongue body should reach its highest point, but without making contact.
 As a result of the upcoming tongue body specification for the 
\begin_inset IPA

\begin_layout Standard
/i/
\end_layout

\end_inset

, the tongue position for the 
\begin_inset IPA

\begin_layout Standard
/k/
\end_layout

\end_inset

 is shifted forwards – away from the soft palate, and towards the hard palate.
 The result is a 
\begin_inset Quotes eld
\end_inset

blend
\begin_inset Quotes erd
\end_inset

, something that is in between where the two gestures would be in isolation
 (
\begin_inset CommandInset citation
LatexCommand citealt
key "Browman1986,Zsiga2000"

\end_inset

).
 
\end_layout

\begin_layout Standard
The blended production for the palatalized velar is depicted in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:/k+i"

\end_inset

.
 At the bottom of the figure, the boxes represent temporal extent as before,
 this time of the single Tongue Body articulator.
 Diagonal fill lines represent the duration when the 
\begin_inset IPA

\begin_layout Standard
/k/
\end_layout

\end_inset

 target is active, and the semi-opaque white, the duration of the active
 
\begin_inset IPA

\begin_layout Standard
/i/
\end_layout

\end_inset

 articulation.
 Above, the trajectory of the highest point of the tongue body relative
 to the two target locations is indicated by the dotted line.
 The tongue body is assumed to start from a resting position that places
 its highest point somewhere in between the hard and soft palates.
 With the start of the 
\begin_inset IPA

\begin_layout Standard
/k/
\end_layout

\end_inset

 gesture, movement of the tongue body is initiated towards the soft palate.
 However, because the gesture of the following 
\begin_inset IPA

\begin_layout Standard
/i/
\end_layout

\end_inset

 is anticipated, a shift in direction takes place before this target is
 reached, causing both targets to be only partially achieved (The solid
 curves indicate the target trajectories for each segment in isolation).
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename palatalizationa.pdf
	width 40page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:/k+i"

\end_inset


\begin_inset IPA

\begin_layout Standard
/k+i/
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset space \hfill{}
\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename palatalizationb.pdf
	width 40page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:/kʲ+i/"

\end_inset

 
\begin_inset IPA

\begin_layout Standard
/kʲ +i/
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Coarticulation involving the same articulator.
 The dark solid lines represent the trajectories of each segment in isolation.
 The dotted line represents the actual trajectory.
 The Tongue Body (TB) is taken to start and finish in a resting position
 in between the two targets.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
We will take the acoustic counterpart to this production token to correspond
 to a partially palatalized k (
\begin_inset IPA

\begin_layout Standard
[kʲ]
\end_layout

\end_inset

), 
\begin_inset IPA

\begin_layout Standard
[i]
\end_layout

\end_inset

 sequence.
 The articulatory representation associated with the acoustic representation
 
\begin_inset IPA

\begin_layout Standard
/kʲ/
\end_layout

\end_inset

 contains a TB gesture located at the minimum of the dotted curve in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:/k+i"

\end_inset

.
 On a subsequent production cycle in which this token is produced in the
 context of a following 
\begin_inset IPA

\begin_layout Standard
/i/
\end_layout

\end_inset

 (
\begin_inset IPA

\begin_layout Standard
/kʲ+i/
\end_layout

\end_inset

), the 
\begin_inset Quotes eld
\end_inset

palatalizing bias
\begin_inset Quotes erd
\end_inset

 will result in something like the dotted curve in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:/kʲ+i/"

\end_inset

.
 Effectively, the strength of the bias has been reduced.
 This is because the amount of bias depends on the distance between the
 two targets, and the target of the 
\begin_inset IPA

\begin_layout Standard
/kʲ/
\end_layout

\end_inset

 is closer to the target of the 
\begin_inset IPA

\begin_layout Standard
/i/
\end_layout

\end_inset

.
 In fact, it may now be possible to reach both targets via a slight modification
 in the gestural timing.
 In other words, there is no clear necessity of (continuously) shifting
 the target location for the obstruent closer to the hard palate, resulting
 in successively more palatalized tokens.
 
\end_layout

\begin_layout Standard
There is actually more than one plausible acoustic interpretation of the
 output of Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:/k+i"

\end_inset

, and thus more than one articulatory mapping for tokens derived from the
 original production of the 
\begin_inset IPA

\begin_layout Standard
/k+i/
\end_layout

\end_inset

 sequence.
 The target locations of both the consonant and the vowel may be altered,
 or the perceived boundary between the two segments may be shifted, or both.
 Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Palatalizationc"

\end_inset

 depicts a scenario in which the entire sequence has been stored as an exemplar
 of the original 
\begin_inset IPA

\begin_layout Standard
/k/
\end_layout

\end_inset

 category: a composite segment consisting of two sequenced targets
\begin_inset Foot
status open

\begin_layout Plain Layout
Using the IPA to represent acoustic correspondents is not ideal, due not
 only to the conflation of acoustic and articulatory information, but because
 it is not fine-grained enough to capture all the relevant differences among
 the gestural scores.
 The composite analysis could alternatively be represented as 
\begin_inset IPA

\begin_layout Standard
/kʲ/
\end_layout

\end_inset

 (as opposed to the original 
\begin_inset IPA

\begin_layout Standard
/kʲ/+/i/
\end_layout

\end_inset

).
 A change in both targets might look like 
\begin_inset IPA

\begin_layout Standard
/kʲ/+/ɪ/
\end_layout

\end_inset

.
 Other possibilities include: 
\begin_inset IPA

\begin_layout Standard
/k/+/j/+/i/
\end_layout

\end_inset

 , 
\begin_inset IPA

\begin_layout Standard
/k/+/j/
\end_layout

\end_inset

 , 
\begin_inset IPA

\begin_layout Standard
/k͡j/
\end_layout

\end_inset

.
\end_layout

\end_inset

.
 This particular type of mapping is of considerable interest because it
 is not structure-preserving at the phoneme level.
 If the perception to production mapping itself might be the cause of the
 loss or the gain of a phoneme, then phoneme split may be possible without
 an independent change that eliminates conditioning context – may, in fact,
 follow directly from a merger of the allophone with the allophonic context.
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename palatalizationc.pdf
	width 40page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Palatalizationc"

\end_inset

Possible production token derived from perception token [
\begin_inset IPA

\begin_layout Standard
kʲi
\end_layout

\end_inset

]
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "subsec:Misperception-&-Misarticulation"

\end_inset

Misperception & Misarticulation
\end_layout

\begin_layout Standard
A well-established tradition in laboratory phonology attributes phonetic
 and phonological sound change to mishearing and misspeaking on the part
 of individual speakers and listeners (
\begin_inset CommandInset citation
LatexCommand citet
key "Ohala1980,Ohala1981,ohala1983origin,Ohala1990"

\end_inset

).
 Many such changes are traced to coarticulation in production, which can
 create perceptual ambiguity, and the possibility that what the listener
 recovers is not what the speaker intended.
 In certain theories of change, the rarity of sound change is attributed
 to the fact that most speech takes place in a mode where speakers provide
 sufficient cues for listeners, and listeners accurately reverse the effects
 of coarticulation.
 Only rarely do listeners switch to a `non-speech' mode, in which they take
 the perceived forms at face value, or randomly decide to keep a poor category
 exemplar, rather than discard it (e.g., 
\begin_inset CommandInset citation
LatexCommand citealp
key "lindblom1990explaining,Garrett2013"

\end_inset

).
 In other theories, discrepancies between speaker and listener are more
 common, and the rarity of language-wide change is attributed to the listener's
 access to other sources of information about the `correct' form of a word
 (and/or the low likelihood of other speakers adopting and spreading an
 individual's novel variant (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "Ohala1980"

\end_inset

)).
 
\end_layout

\begin_layout Standard
Perception biases emerge when segment 
\emph on
x
\emph default
 is more likely to be misheard as segment 
\emph on
y
\emph default
, than segment 
\emph on
y
\emph default
 to be misheard as segment 
\emph on
x
\emph default
.
 Production biases, in some cases, can be attributed to the masking of overlappi
ng articulatory gestures in rapid or casual speech.
 These biases are of the same kind as those adopted in the preceding models.
 Yet a fundamental aspect of the nature of these misanalyses has been lost
 in implementation.
 Even in models that explicitly invoke the Evolutionary Phonology framework
 (see 
\begin_inset CommandInset citation
LatexCommand citealt
key "Blevins2004"

\end_inset

), the mechanism is typically realized at a very coarse grain
\begin_inset Foot
status open

\begin_layout Plain Layout
Although 
\begin_inset CommandInset citation
LatexCommand citet
key "Boer2000"

\end_inset

 uses a non-trivial mapping from acoustic data to production targets, it
 is not a model of sound change, but of structure emergence in vowel systems.
 In a similar type of model, 
\begin_inset CommandInset citation
LatexCommand citet
key "oudeyer2006self"

\end_inset

 relies on the same units (neurons) being used in perception and production.
 However, this mapping is mediated by the distributed nature of the representati
ons (over a network of neurons), and the fact that neurons are `tuned' by
 experienced input, via a non-linear activation function.
 
\end_layout

\end_inset

.
 For example, the model in 
\begin_inset CommandInset citation
LatexCommand citet
key "wedel2017category"

\end_inset

 (also described in 
\begin_inset CommandInset citation
LatexCommand citet
key "Blevins2009"

\end_inset

) is driven by misperception error (or 
\begin_inset Quotes eld
\end_inset

variant trading
\begin_inset Quotes erd
\end_inset

); this occurs as a binary decision between neighboring lexical categories.
 As already discussed, 
\begin_inset CommandInset citation
LatexCommand citet
key "Garrett2013"

\end_inset

 implement an all-or-nothing normalization mechanism
\begin_inset Foot
status open

\begin_layout Plain Layout
Although they make an explicit distinction between a word-level perceptual
 token space, and a segment-level production token space, no transformation
 algorithm is provided.
 They also suggest that the articulatory `speech' mode is sometimes available
 for perception, so the exact relationship between the two `modes' of processing
 is somewhat unclear.
 In practice, the models seem to be implemented using a single abstract
 phonetic dimension.
\end_layout

\end_inset

.
 In 
\begin_inset CommandInset citation
LatexCommand citet
key "Kirby2014"

\end_inset

, `misparsing' is more gradient; for any given token, a random amount of
 the target segment may be mis-attributed to the preceding segment.
 However, the misparsing doesn't depend on the phonetic properties of the
 input, and different outcomes are only possible by `turning off' the misparsing.
 
\begin_inset CommandInset citation
LatexCommand citet
key "morley2014implications"

\end_inset

 uses a bi-directional misperception term in a model of velar palatalization,
 but misperception only applies to feature parsing, and is segment preserving.
 
\end_layout

\begin_layout Standard
In the next chapter a new model of vowel nasalization is developed, guided
 by the goal of avoiding the theoretical and implementational pitfalls laid
 out in this, and preceding, chapters.
 This model will contain an explicit perception-to-production analysis stage
 in which 
\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:Phoneme-Split"

\end_inset

Phoneme Split
\end_layout

\begin_layout Standard
In this chapter we will develop a model of phoneme split, or genesis, using
 the phenomenon of vowel nasalization as a case study.
 The model will be based on the analyses of the preceding chapters: the
 metric of success will be representational consistency and stability, with
 the ability to achieve multiple stable states under different parameter
 settings.
 The relevant parameters will also be required to serve as testable hypotheses
 about possible actuation mechanisms.
 The first of two model variants, the No-Phoneme Model will contain explicit
 representations only at the word-level, and will be used primarily to illustrat
e a particular implementation of the frequency effect.
 The subsequent model, the Multiple-Parse Model will add a sub-lexical level
 of analysis.
 The major innovation of this model will be an explicit, non-one-to-one,
 perception-to-production mapping in which the likelihood of a given analysis
 depends on the phonetic properties of the input.
 Additionally, no analysis is taken to be more `correct' than any other,
 just as the set of possible sub-lexical units is not taken to determined
 ahead of time.
\end_layout

\begin_layout Section
Representations I
\end_layout

\begin_layout Standard
It has been well-established in both perception and production that a negative
 correlation between degree of vowel nasalization and strength of nasal
 consonant exists (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "kawasaki1978perceived,cohn1990phonetic"

\end_inset

).
 This is consistent with the hypothesis that the final nasal is more likely
 to be lost, the more nasalized the preceding vowel becomes.
 A possible explanation can be found in a listener-oriented theory of change,
 where speakers strive to preserve acoustic cues for ease of listener comprehens
ion.
 Strong nasal cues on the vowel predict the upcoming nasal, which means
 that speakers may expend less effort to preserve the actual nasal, allowing
 it to erode.
 As with other proposals, the question that still remains to be answered
 is how the vowel came to have such strong nasal cues in the first place
 (presumably stronger than the typical range of phonetic nasalization observed
 cross-linguistically).
 
\end_layout

\begin_layout Standard
A different perspective will be adopted here, building on the observation
 of 
\begin_inset CommandInset citation
LatexCommand citet
key "Beddor2009"

\end_inset

 that the negative correlation between vowel nasality and consonant nasality
 follows directly from a single articulatory parameter: the degree of overlap
 of the vowel and nasal gestures.
 The more overlap, the greater the extent of nasalization on the vowel,
 and the shorter the duration of the purely consonantal nasal, and vice
 versa (see Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Normal nasalization"

\end_inset

).
 It will be assumed that successful production requires stored articulatory
 targets, and that these must be inferred from acoustic inputs.
 For simplicity, only a single word type will be modeled, that consisting
 of a tongue body gesture followed by a velum gesture (e.g., 
\begin_inset Quotes eld
\end_inset

am
\begin_inset Quotes erd
\end_inset

).
 The production-perception mapping will take place over three phonetic variables
: duration of tongue body gesture, duration of velum gesture, and duration
 of gestural overlap 
\begin_inset Formula $(x^{V},x^{N},x^{O})$
\end_inset

.
 Categorization will occur at the level of the word, and does not require
 decomposition into phonemes.
 Thus, these models will not assume that there exists an allophonic process
 of vowel nasalization.
 The initial distributions for all tokens will be generated by independent
 sampling from three separate Normal distributions, corresponding to 
\begin_inset Formula $x^{V}$
\end_inset

, 
\begin_inset Formula $x^{N}$
\end_inset

, and 
\begin_inset Formula $x^{O}$
\end_inset

, respectively.
 
\end_layout

\begin_layout Standard
In Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Models-of-Change"

\end_inset

 we saw that (soft) targets were needed to constrain the basic exemplar
 model of change.
 What this did was effectively force an independent production component
 into a model which otherwise equated acoustic and articulatory representations.
 In the current set of models this is not necessary, implementationally,
 or conceptually, because each acoustic token has its own production target.
 This is depicted schematically in Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:P-toP-mapping"

\end_inset

, where stored articulatory parameters (represented by temporally overlapping
 articulatory gestures) are realized as acoustic tokens during production
 (dark patterned rectangles representing sound frequency information over
 time), and acoustic tokens are, in their turn, transformed into stored
 articulatory parameters during perception.
 At the word level, this is a 
\noun on
state
\noun default
 model; the articulatory variables are stored without normalization.
 At the gestural level, however, there is the possibility of an implicit
 
\noun on
process
\noun default
 model in the fact that the overlap dimension represents the concatenation
 of two units, as well as the source of nasalization.
 We will return to this point below.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename SeparateReps.pdf
	width 75text%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:P-toP-mapping"

\end_inset

Graphical depiction of an explicit mapping between articulatory representations
 and acoustic representations.
 
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
As Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Model-Behavior"

\end_inset

 showed, the two-attractor (
\noun on
state
\noun default
 model) had a limited range of output states: the entire distribution always
 stabilizes at some point intermediate between the two attractors.
 Furthermore, that model contained no mechanism for changing the attractor
 locations, or introducing new attractors.
 The current model effectively explodes the number of attractors (or underlying
 representations) to the number of tokens within a category.
 This allows for more complex model dynamics.
 It also allows for changes to occur to the attractors themselves, via independe
nt forces that act, not at the level of the word (or at the level of the
 phoneme), but at the level of the gestural variables.
 Individual tokens of a given word category can thus be altered, with the
 possibility, but not the guarantee, that such changes can spread throughout
 the entire distribution of tokens.
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Frequency-I"

\end_inset

Frequency I
\end_layout

\begin_layout Standard
The first iteration of the phoneme-split model uses a frequency-based attractor
 as the actuator of change.
 Based on the assumption that frequency-based reduction is the result of
 increased fluency, and that to be fluent is to produce some (nearly) ideal
 balance of efficiency and intelligibility, an optimal degree of gestural
 overlap, 
\emph on
T
\emph default
, is defined.
 Relative word frequency is implemented as a parameter (
\begin_inset Formula $\beta$
\end_inset

; 
\begin_inset Formula $0<\beta<1$
\end_inset

) that controls the degree to which each token of the category is shifted
 towards the target, 
\emph on
T
\emph default
, during production.
 Thus, for a stored production token consisting of the duration triple 
\begin_inset Formula $(x_{i}^{V},x_{i}^{N},x_{i}^{O})$
\end_inset

, a fluency effect applies to the ultimate realization of 
\begin_inset Formula $x_{i}^{O}$
\end_inset

, the gestural overlap value, in the following way: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Fluency:x_{i}^{O^{\prime}}=x_{i}^{O}+\beta(T-x_{i}^{O})\label{eq:Frequency attractor}
\end{equation}

\end_inset

Because the absolute duration of the optimal gestural overlap will depend
 on the durations of the gestures for each specific token, 
\begin_inset Formula $T$
\end_inset

 is expressed as a function of 
\begin_inset Formula $x_{i}^{N}$
\end_inset

.
 In the case where 
\begin_inset Formula $T=x_{i}^{N}$
\end_inset

, the fluency pressure always acts to increase 
\begin_inset Formula $x_{i}^{O}$
\end_inset

 (as long as 
\begin_inset Formula $x_{i}^{O}\neq x_{i}^{N}$
\end_inset

), because the duration of overlap can never be larger than the duration
 of the nasal gesture (assuming also that 
\begin_inset Formula $x_{i}^{V}$
\end_inset

 is always greater than 
\begin_inset Formula $x_{i}^{N}$
\end_inset

).
 
\end_layout

\begin_layout Section
Speaking Rate
\end_layout

\begin_layout Standard
As has already been demonstrated several times, a single attractor results
 in a single possible outcome.
 With only the frequency attractor acting on productions, maximal fluency
 is the only possible outcome, regardless of the value of 
\begin_inset Formula $\beta$
\end_inset

.
 Implementationally, a force is needed to counter-act the fluency effect.
 For a fluency target at maximal overlap, that opposing force must act to
 decrease overlap.
 However, in the general case, it may be desirable to include a force that
 can either increase or decrease the relevant parameter values.
 In fact, regardless of the implementational requirements for a successful
 model, there are clear theoretical reasons to include a bi-directional
 force affecting articulatory durations.
 
\end_layout

\begin_layout Standard
Changes in speaking rate, of course, strongly influence the absolute duration
 of speech sounds.
 Furthermore, there are similarities between the reduction effects observed
 in fast speech, and those observed with high-frequency words.
 Therefore, whatever drives changes in speaking rate is clearly relevant
 in a model of change in which duration plays a role.
 Equally important is the fact that changes in speaking rate are not all
 increases in speed, and the effect of slowed speech in potentially disrupting
 cumulative change cannot be selectively ignored.
 
\end_layout

\begin_layout Standard
Changes in speaking rate have been shown to affect both the absolute duration
 as well as the timing between sequential speech units (
\begin_inset CommandInset citation
LatexCommand citealt
key "stetson1928motor,Hardcastle1985"

\end_inset

), and therefore are taken to affect all of 
\begin_inset Formula $(x^{V},x^{N},x^{O})$
\end_inset

 in the phoneme-split model.
 I will adopt the view here that changes in speaking rate are governed by
 forces largely external to the mechanisms of sound change, and that changes
 in rate can therefore be modeled as a stochastic process
\begin_inset Foot
status open

\begin_layout Plain Layout
There is a strand of research that assumes that changes in speaking rate,
 specifically decreases in speaking rate, are driven by a desire to enhance
 or exaggerate a given phonological contrast (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "beckman2011rate"

\end_inset

).
 Although slowed speaking rate often occurs under conditions in which speakers
 are deliberately hyper-articulating their speech, I assume that decreases
 in speaking rate can also occur independently; that is, that speakers can
 control their rate of speech, e.g., when asked to match the beat of a metronome,
 without consciously trying to produce more intelligible speech.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Changes in speaking rate, affecting word duration, are modeled in the following
 way.
 At production, a value is randomly selected from a Normal distribution
 centered about 0.
 This value represents the force (
\emph on
E
\emph default
) that will act on that token: either to expand it (if positive), or to
 compress it (if negative).
 Expansion results in longer words, corresponding to slower speaking rates,
 and compression results in shorter words, corresponding to faster speaking
 rates.
 Each articulatory parameter is independently subjected to this force.
 The degree to which a given gesture is actually expanded or compressed
 depends on how inherently elastic it is.
 This elasticity is implemented as a parameter that controls the steepness
 of a logistic curve.
 For example, the effect of force 
\emph on
E
\emph default
 acting on the overlap variable (
\begin_inset Formula $x^{O}$
\end_inset

) is given in Equation (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Speaking rate transform"

\end_inset

) 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Speaking\overset{}{}Rate:x_{i}^{O^{\prime\prime}}=\frac{A}{(1+e^{kE})}\label{eq:Speaking rate transform}
\end{equation}

\end_inset


\emph on
A
\emph default
 is a normalization factor, and is set to 
\begin_inset Formula $2x_{i}^{Z^{\prime}}$
\end_inset

 for all variables (
\emph on
Z
\emph default
).
 This has the effect of making the adjusted length depend on the current
 length, with 
\begin_inset Formula $E=0$
\end_inset

 resulting in no change.
 Note that for decreases in speaking rate, overlap should decrease – pulling
 the two gestures apart, and thus lengthening the word – , and for increases
 in speaking rate, it should increase.
 Therefore, the dependence of overlap on expansion degree is expressed as
 a positive exponential, while the dependence of the other two duration
 parameters is expressed as a negative exponential.
 For these simulations all three articulatory variables were set to the
 same elasticity (
\begin_inset Formula $k=1$
\end_inset

).
 
\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:No-Phoneme-Phoneme-Split-Model"

\end_inset

No-Phoneme Phoneme-Split Model
\end_layout

\begin_layout Standard
In order to understand the behavior of the phoneme-split models, we first
 create a version with only the speaking rate mechanism included.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SpeakingRateOnly"

\end_inset

 shows the outcome that the speaking rate distribution (
\emph on
E
\emph default
 distribution) selects for, given a particular starting distribution of
 tokens.
 The three articulatory parameters are plotted separately, in different
 colors.
 This model also assumes error-free mapping of acoustics to articulation,
 outside of a small error term in production.
 Entrenchment and memory decay apply as in previous models.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /Users/becca/Dropbox/CurrentWork/PerceptionProduction/VowelNasalization/SpeakingRateI10000.pdf
	width 30page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:SpeakingRateOnly"

\end_inset

Phoneme-Split Model: Speaking rate only
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
With appropriately chosen constants, the speaking rate force is capable
 of disrupting the influence of the frequency-based attractor.
 This allows the equilibrium state of the model to vary as a function of
 word frequency (
\begin_inset Formula $\beta$
\end_inset

).
 For 
\begin_inset Formula $T=x_{i}^{N}$
\end_inset

, the fluency effect acts consistently to shift the overlap longer (
\begin_inset Formula $T=x_{i}^{N}$
\end_inset

).
 If it is too weak (
\begin_inset Formula $\beta$
\end_inset

 too small), then the speaking rate equilibrium shown in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:SpeakingRateOnly"

\end_inset

 prevails.
 If it is strong enough, then it is able to shift the overlap distribution
 to larger values (rightward).
 This is possible because the speaking rate transform depends on the current
 value of the overlap parameter for any given token (expressed in the variable
 
\emph on
A
\emph default
 of Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Speaking rate transform"

\end_inset

)).
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:NasalizationModel1"

\end_inset

 shows the output of the model with both speaking rate and frequency bias,
 run for three different values of 
\begin_inset Formula $\beta$
\end_inset

.
 Note that the number of model iterations is essentially arbitrary.
 Because the number is large (10,000), there is a reasonable expectation
 that a stable state has been reached, but no tests of convergence were
 performed.
 In this section we are more concerned with the qualitative behavior of
 the model, and comparisons in which all but one aspect of the simulations
 are kept constant.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename /Users/becca/Dropbox/CurrentWork/PerceptionProduction/VowelNasalization/SpeakingRateFrequencybeta=.05.pdf
	width 25page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Low-frequency word (
\begin_inset Formula $\beta=.05$
\end_inset

)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename /Users/becca/Dropbox/CurrentWork/PerceptionProduction/VowelNasalization/SpeakingRateFrequencybeta=.2.pdf
	width 25page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Mid-frequency word (
\begin_inset Formula $\beta=.2$
\end_inset

)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /Users/becca/Dropbox/CurrentWork/PerceptionProduction/VowelNasalization/SpeakingRateFrequencybeta=.5.pdf
	width 25page%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
High-frequency word (
\begin_inset Formula $\beta=.5$
\end_inset

)
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:NasalizationModel1"

\end_inset

No-Phoneme Phoneme-Split Model (Frequency Attractor).
 Each model run for 10,000 cycles from the same initial distributions.
 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The No-Phoneme Phoneme-Split model predicts that words with higher frequencies
 should be produced, on average, with vowels that are more nasalized (larger
 degree of overlap between gestures), than lower frequency words.
 It also shows that it is possible to achieve stable phonetic change from
 a change in word frequency.
 The model implements a theory of nasal vowel genesis as an emergent property
 of gradient effects acting directly on articulatory parameters.
 Only in the special case where overlap is roughly equivalent to nasal duration,
 would the data likely be analyzed (by a linguist) as the result of phoneme
 split.
 This state in the model, however, has no special status.
 And distributions that appear intermediate with regard to the average ratio
 of overlap duration to velum gesture duration can be stable.
 Conceptualizing phoneme split in this way allows us to avoid the actuation
 paradox that requires the loss of the conditioning environment, but the
 retention of the conditioned allophone (see Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Actuation-1"

\end_inset

).
 In fact, as there are no phoneme-level representations in this model, there
 are no allophones, and no conditioning environments, in the classical sense.
 Therefore, this model is also a demonstration that phoneme-level representation
s are not necessary (nor is any misperception/misarticulation pressure of
 the kind discussed in the previous Chapter), to achieve a working model
 with the generally correct behavior.
 
\end_layout

\begin_layout Standard
The source of the nasalization effect in this model is the coordination
 between the tongue body and velum gestures.
 This parameter, however, is part of the underlying specification of each
 word token.
 The No-Phoneme model is thus a 
\noun on
state
\noun default
 model.
 Arguably, a 
\noun on
state
\noun default
 model represents a change that has already taken place, in which a process
 of nasalization has been reinterpreted as a static property of a unitary
 representation.
 In the next sections we will turn to a 
\noun on
process
\noun default
 model of vowel nasalization, returning to the misperception/misarticulation
 actuation mechanism.
 This will also involve introducing a sub-lexical level of representation,
 and to revisiting the implementation of word frequency.
\end_layout

\begin_layout Section
Parsing and Misparsing
\end_layout

\begin_layout Standard
In order to recover the meaning of a given speech signal, it is necessary,
 at minimum, to identify the individual lexical items present.
 This, in turn, requires determining where one word ends and the next begins.
 The highly context-sensitive nature of acoustic cues, as well as the lack
 of consistent silence, or other markers, of the boundaries between words
 or sounds, make this a computationally difficult task.
 And this is not just an acquisition problem.
 Signal parsing, or segmentation, is something that must be carried out
 every time speech is perceived.
 
\end_layout

\begin_layout Standard
That segmentation of some kind must also take place at the sub-lexical level
 is evidenced by a large literature on what are known as 
\begin_inset Quotes eld
\end_inset

trading relations
\begin_inset Quotes erd
\end_inset

, in which the value along a given phonetic dimension that separates two
 members of a phonemic contrast is shown to vary depending on the values
 of the other phonetic cues present.
 And those other cues that influence the boundary location are not just
 those that occur within the segment itself.
 For example, a given phone (
\begin_inset IPA

\begin_layout Standard
[t]
\end_layout

\end_inset

) may be ambiguous as to whether it belongs with a preceding or following
 word (e.g., 
\begin_inset Quotes eld
\end_inset

great ship
\begin_inset Quotes erd
\end_inset

 [
\begin_inset IPA

\begin_layout Standard
ɡɹe͡ɪt
\end_layout

\end_inset

#
\begin_inset IPA

\begin_layout Standard
ʃɪp
\end_layout

\end_inset

] versus 
\begin_inset Quotes eld
\end_inset

gray chip
\begin_inset Quotes erd
\end_inset

 [
\begin_inset IPA

\begin_layout Standard
ɡɹe͡ɪ
\end_layout

\end_inset

#
\begin_inset IPA

\begin_layout Standard
t͡ʃɪp
\end_layout

\end_inset

]), and the actual word sequence that is heard will depend on the durations
 of the surrounding segments; longer durations of 
\begin_inset IPA

\begin_layout Standard
[e͡ɪ]
\end_layout

\end_inset

 increase the likelihood of 
\begin_inset Quotes eld
\end_inset

gray
\begin_inset Quotes erd
\end_inset

 over 
\begin_inset Quotes eld
\end_inset

great
\begin_inset Quotes erd
\end_inset

, while shorter durations of 
\begin_inset IPA

\begin_layout Standard
[ʃ]
\end_layout

\end_inset

 increase the likelihood of 
\begin_inset Quotes eld
\end_inset

chip
\begin_inset Quotes erd
\end_inset

 over 
\begin_inset Quotes eld
\end_inset

ship
\begin_inset Quotes erd
\end_inset

.
 An acoustic cue (such as silence itself) may also be ambiguous as to whether
 it originates from a phoneme (
\begin_inset IPA

\begin_layout Standard
/t/
\end_layout

\end_inset

) or a break between words (
\begin_inset Quotes eld
\end_inset

great ship
\begin_inset Quotes erd
\end_inset

 versus 
\begin_inset Quotes eld
\end_inset

gray ship
\begin_inset Quotes erd
\end_inset

 [
\begin_inset IPA

\begin_layout Standard
ɡɹe͡ɪ
\end_layout

\end_inset

#
\begin_inset IPA

\begin_layout Standard
ʃɪp
\end_layout

\end_inset

]; longer durations of silence increase the likelihood of 
\begin_inset Quotes eld
\end_inset

great
\begin_inset Quotes erd
\end_inset

 over 
\begin_inset Quotes eld
\end_inset

gray
\begin_inset Quotes erd
\end_inset

 (
\begin_inset CommandInset citation
LatexCommand citealt
key "repp1978perceptual"

\end_inset

).
 An acoustic feature may also be ambiguous as to whether it belongs to a
 preceding segment, a following segment, or both ([
\begin_inset IPA

\begin_layout Standard
ɹa͡ɪpbɛɹiz
\end_layout

\end_inset

] as either 
\begin_inset Quotes eld
\end_inset

right berries
\begin_inset Quotes erd
\end_inset

 or 
\begin_inset Quotes eld
\end_inset

ripe berries
\begin_inset Quotes erd
\end_inset

) (
\begin_inset CommandInset citation
LatexCommand citealt
key "Gow2003"

\end_inset

).
 
\end_layout

\begin_layout Standard
In all of the preceding examples the ambiguity exists because of the existence
 of multiple real-word alternatives.
 Without those alternatives, or competitors, phonetically ambiguous input
 quickly becomes perceptually unambiguous (e.g., 
\begin_inset CommandInset citation
LatexCommand citealp
key "warren1970perceptual,ganong1980phonetic"

\end_inset

).
 The strong susceptibility of low-level perception to high-level expectations
 also speaks to the amount of noise, or essentially unpredictable variability,
 in the acoustic realization of a given abstract category.
 Speech perception involves the complex integration of multiple cues, each
 of which, in isolation, may be relatively uninformative, in order to arrive
 at a single parse, a single percept, of what is heard.
 This percept is presumably the best alternative among those available to
 the listener (see 
\begin_inset CommandInset citation
LatexCommand citet
key "davis2007hearing"

\end_inset

 for a review of the literature).
 Although speech perception appears extremely robust due to the fact that
 the meaning intended by the speaker is usually recoverable by the listener,
 that robustness is a property of the entire set of cues available, not
 of acoustic features alone, and certainly not of individual acoustic features.
 Rather than conceptualizing sound change as the relatively rare event in
 which the listener mishears, or the speaker misspeaks, it may be the case
 that what we typically think of as the 
\begin_inset Quotes eld
\end_inset

changed
\begin_inset Quotes erd
\end_inset

 variants are already present within the distribution of stored tokens,
 as one of multiple possible parses of each inherently ambiguous input signal.
\end_layout

\begin_layout Section
Multiple Parses
\end_layout

\begin_layout Standard
The classical way in which sound change is conceptualized is based on the
 assumption that there exists a unique, correct, sub-lexical representation
 for each word.
 It is meaningless to speak of phoneme-level 
\begin_inset Quotes eld
\end_inset

errors
\begin_inset Quotes erd
\end_inset

 unless this is the case.
 Consider the following hypothetical example (where 
\emph on

\begin_inset Formula $x>y$
\end_inset


\emph default
 indicates an historical change from 
\emph on
x
\emph default
 to 
\emph on
y
\emph default
):
\end_layout

\begin_layout Numbered Examples (consecutive)
\begin_inset CommandInset label
LatexCommand label
name "anpa>ampa"

\end_inset


\begin_inset IPA

\begin_layout Standard
anpa > ampa
\end_layout

\end_inset


\end_layout

\begin_layout Standard
(
\begin_inset CommandInset ref
LatexCommand ref
reference "anpa>ampa"

\end_inset

) is a common type of change known as nasal place assimilation.
 In this example, the coronal feature of the nasal 
\begin_inset Formula $/n/$
\end_inset

 is assimilated to (or replaced by) the labial feature of the following
 
\begin_inset Formula $/p/$
\end_inset

.
 Speakers of a language that undergoes this change presumably had an earlier
 allophonic rule specifying that 
\begin_inset Formula $/n/$
\end_inset

's preceding stops take on the place features of that stop.
 Therefore, the change could only have occurred if they uncharacteristically
 failed to account for this rule, or they made the 
\begin_inset Quotes eld
\end_inset

wrong
\begin_inset Quotes erd
\end_inset

 choice for a production that was especially strongly assimilated.
 In either case, listeners are assumed to parse their acoustic input into
 a sequence of discrete phones, deciding for each segment whether to normalize
 or accept at face value.
 Thus, for a change from 
\begin_inset IPA

\begin_layout Standard
/anpa/
\end_layout

\end_inset

 to 
\begin_inset IPA

\begin_layout Standard
/ampa/
\end_layout

\end_inset

 to have actually occurred in the way it is denoted here, it must be the
 case that listeners used to routinely segment continuous acoustic tokens
 of this word into the sequence of units 
\begin_inset IPA

\begin_layout Standard
/a/
\end_layout

\end_inset

, 
\begin_inset IPA

\begin_layout Standard
/n/
\end_layout

\end_inset

, 
\begin_inset IPA

\begin_layout Standard
/p/
\end_layout

\end_inset

, 
\begin_inset IPA

\begin_layout Standard
/a/
\end_layout

\end_inset

, until they switched to segmenting those tokens into the sequence 
\begin_inset IPA

\begin_layout Standard
/a/
\end_layout

\end_inset

, 
\begin_inset IPA

\begin_layout Standard
/m/
\end_layout

\end_inset

, 
\begin_inset IPA

\begin_layout Standard
/p/
\end_layout

\end_inset

, 
\begin_inset IPA

\begin_layout Standard
/a/
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
Of course, we know that a discrete series of abstract symbols (either 
\begin_inset IPA

\begin_layout Standard
[anpa]
\end_layout

\end_inset

 or 
\begin_inset IPA

\begin_layout Standard
[ampa]
\end_layout

\end_inset

 ) is not present in the acoustic signal in any objective sense.
 The abstract notation also implies that this change occurs once, simultaneously
, for all words, and for all word tokens.
 However, adopting the hypothesis that multiple experienced instances of
 speech are stored implies that change would have to occur over individual
 tokens.
 In fact, the multiple-parse hypothesis is a logical consequence of the
 basic tenet of the exemplar framework.
 The conflation of perception and production that we saw in in the the exemplar
 models of Chapters 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Feedback Loop"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Models-of-Change"

\end_inset

 is borrowed directly from the standard generative notation.
 Once a transformation from perceptual tokens to production tokens is required,
 it becomes clear 1) that parsing is necessary in the first place, and 2)
 that it must occur for each experienced token.
 Recognizing that acoustic tokens are inherently ambiguous with respect
 to their decomposition into discrete units suggests, in turn, that variable
 parses might be the norm rather than the exception
\begin_inset Foot
status open

\begin_layout Plain Layout
This is closely related to the proposal that stored lexical items can have
 more than one representation (see, e.g., 
\begin_inset CommandInset citation
LatexCommand citealp
key "hooper1976word,Janda2008,Bybee2001"

\end_inset

).
 Split representations are also assumed to be the outcome of discontinuous
 articulatory change in the model of 
\begin_inset CommandInset citation
LatexCommand citet
key "Garrett2013"

\end_inset

.
\end_layout

\end_inset

.
 
\end_layout

\begin_layout Standard
In the nasal assimilation example, there are two obvious alternative parses,
 differing in whether they contain the phoneme 
\begin_inset Formula $/n/$
\end_inset

 or 
\begin_inset Formula $/m/$
\end_inset

, thus the word-level category 
\begin_inset Quotes eld
\end_inset

anpa
\begin_inset Quotes erd
\end_inset

 is hypothesized to be composed of at least some tokens specified with productio
n targets for 
\begin_inset Formula $/n/$
\end_inset

, and some for 
\begin_inset Formula $/m/$
\end_inset

.
 However, additional possible parses exist if we do not assume the available
 phoneme inventory 
\emph on
a priori
\emph default
.
 In fact, if we allow all universally possible segments into the analysis
 space, then we avoid the actuation paradox of the classical diachronic
 approach.
 As the next section will show, this re-framing of the change question allows
 synchronic variation to be linked to diachronic change in a way that is
 not dependent on either stopping or starting the model at a critical point
 in time.
 
\end_layout

\begin_layout Section
Representations II
\end_layout

\begin_layout Standard
The Multiple-Parse model adds a 
\noun on
process
\noun default
 component to the No-Phoneme model.
 The process is implemented at the level of the articulatory gesture, but
 conceptually requires the existence of abstract categories intermediate
 between the word and the gesture.
 As before, the change occurs in the distribution of variants that already
 exist, rather than in the genesis of entirely novel forms.
 This aspect bears some similarity to the proposal in 
\begin_inset CommandInset citation
LatexCommand citet
key "Baker2011"

\end_inset

, based on misanalysis of the signal, but the current model is not abrupt,
 nor does it require 
\begin_inset Quotes eld
\end_inset

extreme
\begin_inset Quotes erd
\end_inset

 variants to be adopted.
\end_layout

\begin_layout Standard
The conversion from perception to production is the locus of sub-lexical
 parsing, mapping every continuous acoustic token into a series of categorical
 units.
 In principle, these units can consist of any contiguous set as long as
 it is phonetically plausible, and exhaustively parses the input signal.
 However, in the case of vowel nasalization, we will be concerned with two
 particular possibilities: the one-sublexical-unit analysis, and the two-sublexi
cal-unit analysis.
 These are of special interest, of course, because they bear considerable
 similarity to the classical analyses of the phenomenon before change (two
 units), and after change (one unit).
 However, it is important to be very careful in how these units are described,
 because the traditional notational system essentially forces enforces an
 analysis more general than the word level.
 In order not to assume generalization, and remain representationally consistent
, the following notation will be adopted for the two sub-lexical parses
 of the word in question (
\begin_inset Quotes eld
\end_inset

am
\begin_inset Quotes erd
\end_inset

): 
\begin_inset Formula $/\tilde{V}_{am}/$
\end_inset

 (Analysis 1), and 
\begin_inset Formula $/V_{am}/+/N_{am}/$
\end_inset

 (Analysis 2).
 The desired implication is that only after generalization across multiple
 words could something similar to the abstract categories 
\begin_inset Formula $/\widetilde{V}/$
\end_inset

 and 
\begin_inset Formula $/V/+/N/$
\end_inset

 arise.
\end_layout

\begin_layout Standard
For the articulatory parameters already defined, a single-unit parse means
 that all three values will be stored on the production side.
 
\begin_inset Formula $/\tilde{V}_{am}/$
\end_inset

 is a 3-dimensional cloud, and entrenchment applies over each dimension
 (note that this is what was assumed for all tokens in the No-Phoneme Model.
 Which, therefore, implicitly assumed a single-unit analysis).
 The two-unit parse, however, is explicitly a 
\noun on
process
\noun default
 analysis, entailing that one token is drawn from a one-dimensional 
\emph on

\begin_inset Formula $/V_{am}/$
\end_inset


\emph default
 cloud, one from a one-dimensional 
\emph on

\begin_inset Formula $/N_{am}/$
\end_inset


\emph default
 cloud, with concatenation occurring at the time of production.
 In other words, the overlap between the two gestures is not stored, but
 determined online.
 
\end_layout

\begin_layout Section
Multiple-Parse Phoneme-Split Model
\end_layout

\begin_layout Standard
Either analysis is possible for any given token, but, critically, depends
 on the acoustic properties of that token.
 In this set of simulations it will be assumed that word-level categorization
 is correct, and that the three duration quantities (
\begin_inset Formula $x_{i}^{V},x_{i}^{N},x_{i}^{O}$
\end_inset

), are accurately recovered in perception, although this is not critical
\begin_inset Foot
status open

\begin_layout Plain Layout
If the error term is symmetrical, then it will have no qualitative effect
 on the model dynamics.
\end_layout

\end_inset

.
 Analysis 1, the single-segment analysis, is more likely to be selected,
 the more highly overlapped the gestures that produced that token, while
 Analysis 2, the 2-segments-in-sequence analysis, is more likely for less
 overlapped gestures.
 The specific dependence is on the quantity 
\begin_inset Formula $Q_{i}=\frac{x_{i}^{O}}{x_{i}^{N}}$
\end_inset


\emph on
 
\emph default
.
 Larger values of 
\begin_inset Formula $x_{i}^{O}$
\end_inset

 lead to larger values of 
\begin_inset Formula $Q_{i}$
\end_inset

, as do smaller values of 
\begin_inset Formula $x_{i}^{N}$
\end_inset

.
 Selecting for large 
\emph on
Q
\emph default
 thus selects both for larger overlap and shorter word durations.
 That duration should correlate with number of constituents is a reasonable
 hypothesis.
 It can also be hypothesized that articulatory gestures will tend to be
 more tightly coordinated within, than across, segments, if shared constituency
 promotes greater merger
\begin_inset Foot
status open

\begin_layout Plain Layout
I am not aware of evidence for this specific relationship, but there is
 evidence for different types of gestural coordination across different
 domains: between the onset and nucleus of a syllable, versus the nucleus
 and coda (
\begin_inset CommandInset citation
LatexCommand citealt
key "Browman1988,byrd1996influences"

\end_inset

); and within, versus across, morpheme boundaries (
\begin_inset CommandInset citation
LatexCommand citealt
key "Cho2001"

\end_inset

).
\end_layout

\end_inset

.
 The probability of Analysis 1, 
\begin_inset Formula $P(a=1)$
\end_inset

, depends on 
\emph on
Q
\emph default
 in the following way (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:segmentation-1"

\end_inset

).
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P(a=1)=Ae^{-b(1-Q)}-C\label{eq:segmentation-1}
\end{equation}

\end_inset

Probability increases with increasing 
\emph on
Q
\emph default
 because of the negative exponential in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:segmentation-1"

\end_inset

).
 The largest possible value for 
\emph on
Q
\emph default
 is 1, therefore 
\begin_inset Formula $1-Q$
\end_inset

 is always positive.
 When 
\begin_inset Formula $Q=1$
\end_inset

 , 
\begin_inset Formula $P(a=1)$
\end_inset

 reaches its maximum at 
\begin_inset Formula $A-C$
\end_inset

.
 How quickly the probability decreases as a function of decreasing 
\emph on
Q
\emph default
 is controlled by the variable 
\emph on
b.
 
\emph default
The larger 
\emph on
b
\emph default
, the larger the negative exponential, and the more quickly 
\begin_inset Formula $P(a=1)$
\end_inset

 decreases, selecting for larger mean 
\emph on
Q
\emph default
 values (and fewer tokens).
 See Appendix 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Appendix E"

\end_inset

 for additional details.
\end_layout

\begin_layout Standard
If Analysis 1 is chosen in perception, based on the value of 
\begin_inset Formula $P(a=1)$
\end_inset

, then all three values of the token are stored.
 If Analysis 2 is chosen, then the duration of the tongue body gesture (
\begin_inset Formula $x_{i}^{V}$
\end_inset

), and the duration of the velum gesture (
\begin_inset Formula $x_{i}^{N}$
\end_inset

), are each stored in separate categories, and the overlap value is discarded.
 Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:MultiParse-Reps"

\end_inset

 provides a schematic depiction of these relationships.
 Note that the dimensions are not accurately represented here; two dimensions
 are used for all categories to make the membership relationships easier
 to see.
 Individual tokens are drawn as schematic gestural scores: extent represents
 time, and fill type represents active articulator.
 The horizontal alignment of the two bars in the tokens of the category
 are meant to indicate the stored gestural overlap parameter.
 The thin lines drawn between tokens of the and sub-categories indicate
 that they are stored together, and will be produced together.
 Overlap must be determined via a separate distribution.
 Entrenchment happens only within individual sub-lexical categories
\begin_inset Foot
status open

\begin_layout Plain Layout
If entrenchment at the word-level is added it will have the effect of pushing
 values back towards the means of the Analysis 2 categories, since the model
 is initiated with those values, and the Analysis 2 parse is always more
 likely.
 
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Graphics
	filename MultiParseModel.pdf
	width 75text%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:MultiParse-Reps"

\end_inset

Schematic depiction of the relationships between the word-level category
 (
\begin_inset Quotes eld
\end_inset

am
\begin_inset Quotes erd
\end_inset

) and the sub-lexical level categories of its constituents.
 Production-side representations.
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Tokens are chosen randomly for production from among all stored values.
 Once selected, the token is subject to the same speaking rate transformation
 used in the No-Phoneme model.
 The overlap degree for an Analysis-2 token defaults to a fixed percentage
 of the current average value of 
\begin_inset Formula $x^{N}$
\end_inset

 (with some variance).
 There is no phonetic bias in this model.
 The selection bias that drives the feedback loop resides in the choice
 of underlying analysis – the parsing of the input signal.
 Eq.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:segmentation-1"

\end_inset

 selects for large values of 
\begin_inset Formula $x^{O}$
\end_inset

, and for small values of 
\begin_inset Formula $x^{N}$
\end_inset

, both of which will increase the size of 
\emph on
Q
\emph default
, and thus increase the probability of Analysis 1.
 There is no cumulativity such that these values grow more extreme, but
 there is a constant pressure to sort tokens with the largest overlap proportion
 into this sub-lexical category.
 Thus, if an independent mechanism resulted in an increase in 
\emph on
Q
\emph default
 for some tokens, those tokens would raise the average 
\emph on
Q
\emph default
 value of the 
\begin_inset Formula $/\tilde{V}_{am}/$
\end_inset

 sub-category.
\end_layout

\begin_layout Standard
A mechanism that shortens word duration will have this effect: shortening
 
\begin_inset Formula $x^{N}$
\end_inset

 (and 
\begin_inset Formula $x^{V}$
\end_inset

), and lengthening 
\begin_inset Formula $x^{O}$
\end_inset

.
 If higher frequency is taken to result in faster productions, then an increase
 in frequency would lead to more, and higher-valued 
\emph on
Q
\emph default
, tokens.
 For the simulations reported below, frequency was implemented as a negative
 perturbation to the mean value of the expansion force distribution.
 As a result, higher frequency (higher resting activation) results in shorter
 words, and thus shorter tongue body and velum gestures (and longer overlap)
 under all speaking rates.
 This implementation will be discussed in more detail in the following section.
 
\end_layout

\begin_layout Standard
Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Multiple-Parse-Results"

\end_inset

 shows the model results as a function of frequency.
 Each point is the result of running the model for 10,000 iterations.
 Mean values for the three duration parameters, as well as the proportion
 overlap (
\emph on
Q
\emph default
) are given for each of the categories: Panel 1: word-level; Panel 2: Analysis
 1 tokens; Panel 3: Analysis 2 tokens.
 Note that the overlap proportion in Panel 3 shows the constraint that overlap
 proportion stay fixed with respect to 
\begin_inset Formula $\overline{x^{N}}$
\end_inset

.
 Whereas, in Panel 2, as resting activation (frequency) increases, the proportio
n overlap increases.
 Because the number of tokens parsed into the 
\begin_inset Formula $/\tilde{V}_{am}/$
\end_inset

 category also increases with increasing resting activation (from approximately
 31% to 50%), the overlap proportion increases for the word-level category
 as a whole (Panel 1).
 
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Graphics
	filename /Users/becca/Dropbox/CurrentWork/PerceptionProduction/VowelNasalization/MultipleParseResults.pdf
	width 75text%

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Multiple-Parse-Results"

\end_inset

Multiple-Parse Model: Results as a function of resting activation (word
 frequency).
 Each point corresponds to the mean after 10,000 model iterations.
 Mean articulatory durations are plotted in black.
 Proportion overlap (
\begin_inset Formula $Q=\frac{\bar{x}^{O}}{\bar{x}^{N}}$
\end_inset

) is plotted in red.
 Panel 1: all word tokens; Panel 2: Analysis 1 tokens only; Panel 3: Analysis
 2 tokens only.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
\begin_inset CommandInset label
LatexCommand label
name "sec:Frequency-II"

\end_inset

Frequency II
\end_layout

\begin_layout Standard
In the No-Phoneme Phoneme-Split model frequency was implemented as a fixed
 attractor on overlap duration (Section 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Frequency-I"

\end_inset

).
 The assumption was that there existed an optimal (most fluent) production
 of a given word with precisely the degree of gestural overlap given by
 the attractor target.
 At the same time, in order to generate productions that more closely resembled
 nasal vowels, it was necessary to set the target quite high – in the reported
 simulations it was set to the entire duration of the accompanying velum
 gesture.
 However, it is not clear why the optimal production of the 2-gesture word
 should exhibit such a large degree of overlap.
 And, in general, there is no clear reason for greater practice, or increased
 fluency, to always result in shortened, or reduced, articulations, especially
 to the point where distinctiveness may be lost at the word and/or phoneme
 level.
 Yet this seems to be the case with frequency effects.
 It has been shown, for example, that individual segments within high-frequency
 words are shorter, and that there are more likely to exhibit 
\begin_inset Quotes eld
\end_inset

deletions
\begin_inset Quotes erd
\end_inset

 (dropping, or masking of a consonant, or unstressed vowel) (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "Bell2003,Raymond2006,Bybee2008"

\end_inset

).
 The realizations of segments in higher-frequency words tend also to be
 less extreme, or more 
\begin_inset Quotes eld
\end_inset

centralized
\begin_inset Quotes erd
\end_inset

, perhaps failing to reach the usual articulatory target (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "munson2004effect,Scarborough2004,gahl2008time"

\end_inset

).
\end_layout

\begin_layout Standard
The listener-based account of frequency effects explains these phenomena
 as a consequences of contextual predictability.
 It is actually the less predictable, less easy to access, more confusable,
 forms that are produced with particular care (hyper-articulated) by the
 speaker in order to aid intelligibility (e.g.
 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "Aylett2004"

\end_inset

).
 In the absence of that pressure, articulations are reduced to the degree
 possible, facilitating the task of the speaker.
 Factors that have been shown to affect predictability, as well as word
 form, include sentence, or discourse, context, bigram frequency, and unigram
 frequency, among others.
 Nevertheless, there are a number of results that are not compatible with
 a strictly listener-based theory, studies that have shown that speakers
 do not always alter their productions in such a way as to facilitate listener
 comprehension (see 
\begin_inset CommandInset citation
LatexCommand citet
key "turnbull2015assessing"

\end_inset

 for a review of the literature).
\end_layout

\begin_layout Standard
As mentioned briefly in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Word-Frequency"

\end_inset

, the speaker-based approach attributes frequency effects to automatic productio
n-side mechanisms.
 This is usually couched in terms of activation levels, within some kind
 of lexical network model where different representations 
\begin_inset Quotes eld
\end_inset

compete
\begin_inset Quotes erd
\end_inset

 in both perception and production (e.g., 
\begin_inset CommandInset citation
LatexCommand citealp
key "mcclelland1981interactive,Dell1986"

\end_inset

).
 In terms of word retrieval, the successful candidate is the one that achieves
 a given threshold of activation first.
 Every time a word is accessed, or produced, it is activated to this level.
 Repeated activations, within some time period, are taken to result in some
 level of residual activation that persists even when the word is not selected.
 This 
\begin_inset Quotes eld
\end_inset

resting
\begin_inset Quotes erd
\end_inset

 activation level is naturally higher in higher frequency words, giving
 them a head start against lower-frequency competitors.
 
\end_layout

\begin_layout Standard
The resting-activation account is in line with results establishing that
 higher-frequency words are produced earlier than lower-frequency ones in
 a variety of tasks, such as picture naming, and word or sentence reading
 – even with delays.
 Higher-frequency words also lead to faster response times in lexical decision
 and other speeded response tasks, as well as to greater accuracy in word
 recognition (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "howes1951visual,balota1985locus,Luce1986,Marslen-Wilson1990"

\end_inset

).
 However, it is not at all obvious that higher resting activation alone
 can account for articulatory or temporal reduction (hypo-articulation).
\end_layout

\begin_layout Standard
In fact, it has been argued 
\emph on
both
\emph default
 that a higher activation level should lead to hyper-articulation (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "Baese-Berk2009"

\end_inset

), and that it should lead to hypo-articulation (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "gahl2012reduce"

\end_inset

)
\begin_inset Foot
status open

\begin_layout Plain Layout
Note that different results were obtained in these studies, one based on
 laboratory data, and one on conversational corpus data.
\end_layout

\end_inset

.
 In works that adopt the latter position the connection seems to be assumed.
 For example, 
\begin_inset CommandInset citation
LatexCommand citeauthor
key "gahl2012reduce"

\end_inset

(2012: p.79) write that 
\begin_inset Quotes eld
\end_inset

Production-based accounts...would lead one to expect that words that are retrieved
 quickly tend to be phonetically reduced – 
\emph on
provided that fast retrieval speed translates into fast production speed
\begin_inset Quotes erd
\end_inset


\emph default
 (emphasis mine).
\end_layout

\begin_layout Standard
The fact that there does not appear to be a well-worked out mechanism for
 this result raises the possibility that we have yet to find the right model
 for frequency.
 Empirically, however, the correlation between shorter/faster productions
 and higher word frequency seems quite robust.
 In the Multiple-Parse Model, a frequency-based increase in production speed
 is taken to be an additive effect, acting to effectively shift the speaking
 rate distribution.
 If we continue to assume that speaking rate acts independently of other
 model forces, then words will continue to be pulled in both directions,
 expanding or compressing in turn.
 Subject to the same large positive force, both low and high frequency words
 will be produced more slowly, and will thus be longer than if no force
 had applied.
 However, the high-frequency word will be somewhat shorter than its low
 frequency counterpart, due to the difference in its resting state.
 The same will be found under compression (unless floor is reached).
\end_layout

\begin_layout Standard
The dependence on frequency (resting activation) in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Multiple-Parse-Results"

\end_inset

 shows the effect of progressively shifting the speaking rate distribution;
 shorter average word durations result in shorter 
\begin_inset Formula $x^{N}$
\end_inset

, and longer 
\begin_inset Formula $x^{O}$
\end_inset

, and thus lead to a greater proportion of Analysis 1 tokens
\begin_inset Foot
status open

\begin_layout Plain Layout
Because the duration of overlap cannot exceed the length of the shortest
 gesture, the longest 
\emph on
absolute
\emph default
 overlap durations can only occur with the longest word tokens, thus there
 is also some selection pressure towards longer 
\begin_inset Formula $x^{N}$
\end_inset

 inherent in the 
\begin_inset Formula $x^{O}$
\end_inset

 measure.
 Reducing the length of 
\begin_inset Formula $x^{N}$
\end_inset

 will therefore eliminate some of the largest 
\begin_inset Formula $x^{O}$
\end_inset

 tokens.
\end_layout

\end_inset

.
 This shift is not unbounded, because speaking rate is a bi-directional
 force; high-frequency words can also be lengthened, just not lengthened
 quite as much as their lower-frequency counterparts.
 Note that the frequency effect in this model acts on all tokens, both stored
 and generated.
 In the latter case, we must assume that some type of motor plan involving
 the concatenation of 
\begin_inset Formula $/V_{am}/$
\end_inset

 and 
\begin_inset Formula $/N_{am}/$
\end_inset

 is associated with a resting activation value that affects the duration
 of the resulting word.
 
\end_layout

\begin_layout Section
Actuation 
\end_layout

\begin_layout Standard
As in the No-Phoneme Phoneme Split model, there is no single moment at which
 a sound change occurs in the Multiple-Parse Phoneme-Split Model.
 Every instance of perception involves a decision about parsing which is
 based on existing synchronic variation.
 And every available parse is a possibility at any time, for any token;
 it is the probabilities of those parses which change over time.
 Although the nasal vowel parse is assumed in the sense that it is one possible
 analysis for a given token, this model, in fact, avoids many limiting assumptio
ns about the nature of sound change inherent to the classical view.
 For example, the 
\emph on
/V+N/
\emph default
 analysis is not privileged, beyond having a higher probability of selection,
 given the starting distribution.
 Additional analyses can be added to the set of parsing hypotheses, if motivated
 by general-purpose properties of speech perception.
 It is consistently the word level at which all forces act in this model,
 and at the level of articulatory gesture that changes are realized.
 In particular, this model does not rely on the allophonic level at which
 the synchronic rule, and the diachronic change, are assumed to occur.
 As a result, the normalization/lack of normalization question becomes a
 basic element of speech processing: the analysis that must occur when perceptua
l values are transformed into production values.
 
\end_layout

\begin_layout Standard
If classification occurs at the word level, and words have articulatory
 representations something like gestural scores, then it is not necessary
 to first identify a series of abstract phonemes in order to identify individual
 words.
 Thus, the problem of 
\begin_inset Quotes eld
\end_inset

compensating
\begin_inset Quotes erd
\end_inset

 for (the feature of) nasalization at the level of the phoneme disappears.
 The ambiguity remains regarding the proper articulatory realization of
 a given acoustic input, but there is no longer a unique, correct sub-lexical
 analysis.
 The possible analyses available to the listener, based on their phonetic
 experience, should always include, at minimum, both the 
\begin_inset Quotes eld
\end_inset

normalized
\begin_inset Quotes erd
\end_inset

 option, as well as the 
\begin_inset Quotes eld
\end_inset

unnormalized
\begin_inset Quotes erd
\end_inset

 one.
\end_layout

\begin_layout Standard
In the specific simulations reported in the previous section, the average
 percentage of the velum lowering gesture that occurred simultaneously with
 the tongue body gesture varied from 30%, for the lowest frequency word,
 to about 50% for the highest-frequency word.
 It was also the case that the absolute gesture durations were considerably
 shorter at the higher frequencies.
 These results describe a diachronic change under the scenario in which
 a single word comes to be used more, or less, frequently over time.
 Under the scenario in which frequencies are fixed, but there exists a set
 of words with a range of different frequencies, these results describe
 a synchronic distribution.
 The model thus generates at least two testable predictions: 1) that a differenc
e in the degree of vowel nasalization should be observed across words of
 different frequencies (provided the relevant phonological context is sufficient
ly similar among those words), and 2) that the highest-frequency words should
 approximate the degree of vowel nasalization observed in languages that
 are described as having phonemically nasal vowels.
 In other words, no exaggeration, or enhancement, of the effect is required
 in this model.
 Lexicon-wide change is assumed to start with change at the individual word
 level.
\end_layout

\begin_layout Standard
It is widely acknowledged that change (of certain kinds, at least) happens
 on a word by word basis (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "Phillips1984,Bybee2002,Pierrehumbert2002"

\end_inset

), and that some words can be `further along' in the change than others.
 With regards to nasalized vowels in particular, 
\begin_inset CommandInset citation
LatexCommand citet
key "malecot1960vowel"

\end_inset

 offers evidence that the distinction between English words like 
\begin_inset Quotes eld
\end_inset

cap
\begin_inset Quotes erd
\end_inset

 and 
\begin_inset Quotes eld
\end_inset

camp
\begin_inset Quotes erd
\end_inset

 is primarily that between a nasal and oral vowel (
\begin_inset IPA

\begin_layout Standard
[kæp]
\end_layout

\end_inset

 vs.
 
\begin_inset IPA

\begin_layout Standard
[kæ̃p]
\end_layout

\end_inset

), rather than the presence versus absence of a nasal consonant (
\begin_inset IPA

\begin_layout Standard
[kæp]
\end_layout

\end_inset

 vs.
 
\begin_inset IPA

\begin_layout Standard
[kæ̃mp]
\end_layout

\end_inset

).
 The English segmental inventory is not usually analyzed as containing an
 abstract nasal vowel (although see 
\begin_inset CommandInset citation
LatexCommand citet
key "Sole1992"

\end_inset

 for an argument that nasal vowels are phonologically specified).
 Nevertheless, individual tokens, or individual words, or even classes of
 words, may have phonetic realizations that are indistinguishable from those
 generated from an underlying nasal vowel.
 
\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "subsec:Summary-&-Conclusions"

\end_inset

Discussion & Conclusions
\end_layout

\begin_layout Standard
The aim of Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Phoneme-Split"

\end_inset

 was to develop an explanatory model of a specific type of sound change:
 phoneme split, or phoneme genesis.
 Yet, in the course of developing that model, the change being modeled itself
 underwent a certain kind of transformation.
 When phoneme split was first introduced in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Actuation-1"

\end_inset

 it was described as allophone becoming phoneme.
 The implication, particularly in the case of vowel nasalization, was that
 a completely new phoneme category had to be created, something that had
 not been previously modeled.
 The classical representations for the synchronic and diachronic rules are
 given below.
\end_layout

\begin_layout Numbered Example (multiline)
\begin_inset CommandInset label
LatexCommand label
name "allophonic rule"

\end_inset


\begin_inset Formula $/V/\rightarrow[\tilde{V}]/$
\end_inset

__
\begin_inset Formula $N$
\end_inset


\end_layout

\begin_layout Subexample
\begin_inset CommandInset label
LatexCommand label
name "split-1"

\end_inset


\begin_inset Formula $/VN/>/\tilde{V}/$
\end_inset


\end_layout

\begin_layout Subexample
\begin_inset CommandInset label
LatexCommand label
name "split-2"

\end_inset


\begin_inset Formula $[\tilde{V}]>/\tilde{V}/$
\end_inset


\end_layout

\begin_layout Standard
In scenario (
\begin_inset CommandInset ref
LatexCommand ref
reference "split-1"

\end_inset

), the loss of the nasal context 
\emph on
(N)
\emph default
 is the precipitating event, critical to the emergence of the phoneme.
 In scenario (
\begin_inset CommandInset ref
LatexCommand ref
reference "split-2"

\end_inset

) the loss of the nasal context is irrelevant; the phoneme arises through
 some other mechanism.
\end_layout

\begin_layout Standard
Immediately, the actuation problem arises – the problem of determining why
 phoneme split sometimes happens and sometimes doesn't (
\begin_inset CommandInset citation
LatexCommand citealt
key "Labov1968"

\end_inset

).
 If the conditioning context can be lost without phoneme genesis, then it
 cannot be the loss alone that creates the phoneme (
\begin_inset CommandInset ref
LatexCommand ref
reference "split-1"

\end_inset

).
 But if the loss of context is irrelevant, and coincidental, then contextually
 predictable phonemes are possible, and we have no way to determine – or
 predict – the status of such sounds (
\begin_inset CommandInset ref
LatexCommand ref
reference "split-2"

\end_inset

).
\end_layout

\begin_layout Standard
The solution to this impasse that is suggested by the Multiple-Parse Model
 is that phonemes are nothing other than hypotheses made by individual listener/
speakers about how to break up word-level units, hypotheses that may change
 from moment to moment, and from token to token.
 Once such a hypothesis is made it acquires its own representational reality
 – at least for that speaker.
 Because allophonic relationships only exist as corollaries of a given phonemic
 analysis, they are automatically generated under one hypothesis, and automatica
lly missing under the other.
 
\end_layout

\begin_layout Standard
However, even under the 
\begin_inset Quotes eld
\end_inset

allophonic
\begin_inset Quotes erd
\end_inset

 analysis, allophones never actually surface in this model The process that
 generates what linguists would label as an allophone does not occur at
 the same representational level as the phoneme; it occurs in the region
 shared between two adjacent phonemes
\begin_inset Foot
status open

\begin_layout Plain Layout
Incidentally, this reveals another hidden assumption of the generative notation:
 the fact that coarticulation appears to only affect one of the segments
 involved.
 Nasalization occurs on the vowel, but vocalization should also occur on
 the nasal.
 This bias is most likely based in perception, but articulation-wise, the
 allophonic relationship may be relatively symmetric.
\end_layout

\end_inset

.
 It is predictable in the sense that nasality is predictable when the velum
 is lowered.
 But it is meaningless to talk about bigram predictability – the predictability
 of vowel nasality from the subsequent nasal – because the listener does
 not hear a sequence of phones.
 Under one parsing of their input that nasality will be attributed to gestural
 overlap between adjacent phonemes, under another it will be attributed
 to gestural overlap within a single phoneme.
 In either case it will be entirely predictable.
 
\end_layout

\begin_layout Standard
The sound change in question, therefore, does not actually involve the generatio
n of a new phoneme category.
 If we assume that all possible hypotheses are entertained for all ambiguous
 inputs, then all phonemes exist at all times, and it is only their probabilitie
s that might change over time
\begin_inset Foot
status open

\begin_layout Plain Layout
This does not preclude the merger of phonetic values in the pronunciation
 of two sounds that were previously distinct (e.g., the so-called PIN/PEN
 merger in certain dialects of American English).
 By hypothesis, the number of sub-lexical units that are posited to comprise
 the relevant words is not relevant in this case.
\end_layout

\end_inset

.
 This re-framing avoids the representational paradoxes discussed earlier.
 Actuation now pertains to factors that affect the probability distribution
 over the hypothesis space.
 Such factors are likely to be numerous, and undoubtedly include aspects
 of language processing not explored here.
 In the same vein, the vowel nasalization model is not to be taken as applicable
 to all types of sound change, nor even as a model of all aspects of the
 vowel nasalization change.
 In the next two sections some other factors are briefly discussed, along
 with possible extensions of the current work.
\end_layout

\begin_layout Section
Additional Implications & Future Work
\end_layout

\begin_layout Standard
The Multiple-Parse Model is a model of the internal dynamics of a single
 word category in isolation.
 In this model the assumption is that sub-lexical categories are derived
 from words, rather than the other way around (see, e.g., 
\begin_inset CommandInset citation
LatexCommand citealp
key "beckman2000ontogeny"

\end_inset

).
 Once such categories arise, however, they are expected to exert influence
 in the other direction (English orthography is likely to produce a similar
 effect).
 Even without the influence of explicit phoneme categories, we expect word-level
 representations to be linked in some way that reflects their similarity
 to each other.
 Therefore, evolution of single words cannot truly occur in isolation.
 
\end_layout

\begin_layout Standard
Sound change is typically taken to mean change at the phoneme level.
 In the Multiple-Parse Model change is taken to occur at a less abstract
 level: sub-lexical, but specific to an individual word.
 I assume that a generalization is necessary, likely requiring multiple,
 semi-independent changes at the word level
\begin_inset Foot
status open

\begin_layout Plain Layout
Not to mention the spread of change to all members of a speech community
 – something which has not been touched on at all in this work.
\end_layout

\end_inset

.
 The dynamics of such a model are not trivial, and require, among other
 constraints, that the phoneme-to-word feedback bias be strong enough to
 allow generalization to occur across all words containing that phoneme,
 but not so strong as to prevent changes at the level of the individual
 word.
 
\end_layout

\begin_layout Standard
One interesting consequence of adopting the position that word categories
 precede phoneme categories, is that phonetic regularities must begin as
 
\noun on
states
\noun default
 (stored articulatory variables), rather than 
\noun on
processes
\noun default
 (the result of combining two or more linguistic units), in the infant learner.
 Processes are potentially inferred gradually, over sufficient amounts of
 variable data (e.g., 
\begin_inset CommandInset citation
LatexCommand citealp
key "goodman1997inseparability"

\end_inset

), but individual 
\noun on
state
\noun default
 representations might persist, as 
\noun on
process
\noun default
 ones do in the simulations of the previous section.
 
\end_layout

\begin_layout Standard
The opposite course of development might be expected to occur in the domain
 of morphology, where explicit concatenation requires a 
\noun on
process
\noun default
 model, but 
\noun on
state
\noun default
 analyses become available over time.
 In fact, the 
\begin_inset Quotes eld
\end_inset

competition
\begin_inset Quotes erd
\end_inset

 between the Analysis 1 parse and the Analysis 2 parse bears a high degree
 of similarity to dual-route theories of morphology (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "caramazza1988lexical,frauenfelder1992constraining"

\end_inset

).
 Classically, transparent morphological alternations are assumed to be rule-base
d, analogously to allophonic alternations.
 However, it is evident from the historical record that morphological affixation
s that were once productive can fall out of use, resulting in a few artifactual
 forms that are unlikely to be decomposed into their constituents by modern
 speakers.
 Additionally, some highly frequent forms, although transparently decomposable,
 may behave as though they have unique lexical entries (e.g., 
\begin_inset CommandInset citation
LatexCommand citet
key "baayen1997singulars"

\end_inset

.
 See 
\begin_inset CommandInset citation
LatexCommand citet
key "levelt1999theory"

\end_inset

 for a review of frequency-based storage, and 
\begin_inset CommandInset citation
LatexCommand citet
key "burani1987representation,baayen1993frequency"

\end_inset

 for further discussion of factors affecting morphological storage).
 This parallelism does not seem to be coincidental, and is especially relevant
 to allophonic alternations that occur precisely at morpheme boundaries.
\end_layout

\begin_layout Standard
Morphophonological alternations are, in fact, often taken to comprise the
 best evidence of an active phonological rule.
 This is because the morphological process involved is assumed to be productive.
 That is, it is assumed to be a 
\noun on
process
\noun default
.
 Yet, the change that led to the phonological alternation may only have
 come about due to representations becoming more 
\noun on
state
\noun default
-like, as is implied by the behavior of the Multiple-Parse Model.
 If this is on the right track, then truly phonological, truly productive
 alternations may only arise when 
\noun on
state
\noun default
 and 
\noun on
process
\noun default
 representations are balanced in such a way as to preserve this tension.
 Determining the necessary conditions for this to happen presents an interesting
 area for future research.
\end_layout

\begin_layout Section
Types of Sound Change
\end_layout

\begin_layout Standard
In the modeling of sound change, the term 
\begin_inset Quotes eld
\end_inset

phonetic bias
\begin_inset Quotes erd
\end_inset

 seems to have been used as a cover term to refer to phonetically-based
 sound change of more or less any kind.
 Thus it has been (or can be) applied to word reduction, vowel lengthening,
 vowel nasalization, and nasal place assimilation (or loss), among others.
 There is no 
\emph on
a priori
\emph default
 reason to expect all phonetically-based sound change to operate in the
 same way, however.
 And part of an ultimate theory of sound change will include a taxonomy
 both of the source of a given change, as well as its actuation mechanism.
 
\end_layout

\begin_layout Standard
The Multiple-Parse Model of vowel nasalization presented in the previous
 chapter is based on the hypothesis that coarticulatory nasalization is
 
\emph on
not
\emph default
 best analyzed as a phonetic bias; that is, as a constant pressure acting
 in a fixed direction.
 Instead, the source of nasalization is taken to be an inherent property
 of motor planning involving the temporal overlap between adjacent articulatory
 gestures.
 Synchronically, overlap degree is assumed to vary as a function of speaking
 rate, among potentially many factors, all of them contributing to a stable
 distribution with a certain degree of variance.
 In the implemented model, a change in the resting activation of a word-level
 category acts to shift both the absolute durations of the articulatory
 parameters, as well as the proportion of overlap.
 Words become shorter, with a higher degree of overlap, as resting activation
 increases.
 This follows from the assumption that activation level directly affects
 not only the speed with which words are accessed and initiated, but also
 the speed at which articulation unfolds.
 The utility of this model is only as good as this assumption, and will
 need to be revised if our understanding of the frequency effect changes
\begin_inset Foot
status open

\begin_layout Plain Layout
The correlation between speaking rate and degree of coarticulation, as well
 as the correlation between word-frequency and degree of coarticulation,
 appear to be quite robust.
 It is less clear, however, what the exact mechanism is that mediates between
 activation level and degree of coarticulation.
 Without this link, we run the risk of modeling an epiphenomenon, rather
 than the phenomenon itself.
\end_layout

\end_inset

.
 However, actuation is achievable by any mechanism that can shift the overlap
 distribution as a whole.
\end_layout

\begin_layout Standard
The Multiple-Parse Model, of course, is meant to be not just a model of
 vowel nasalization, but of all linguistic phenomena that are functionally
 equivalent to vowel nasalization.
 Establishing this class is not trivial, and I will only hypothesize here
 that phenomena involving articulatory overlap, articulatory blending, and
 articulatory masking will generally be possible to model in this way.
 True phonetic biases can also be incorporated into the general model.
 Consonants occurring before other consonants (rather than vowels) can be
 considered to be in a perceptually disadvantaged position.
 This is especially true of stops, since most of the cues to their identity
 actually occur in the transitions to a following vowel (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "liberman1954role"

\end_inset

), but likely holds to some extent for most consonants.
 Articulatorily, the velum gesture attributed to the nasal in a word like
 
\begin_inset Quotes eld
\end_inset

camp
\begin_inset Quotes erd
\end_inset

 will be overlapped to some extent not only with the preceding vowel, but
 also with the following consonant.
 The overlap with the preceding vowel is highly audible, while the overlap
 with the following stop is much less so, due to the complete closure in
 the oral cavity.
 The stop context, relative to a vowel context (such as in the word 
\begin_inset Quotes eld
\end_inset

camo
\begin_inset Quotes erd
\end_inset

), can be thought of as biasing for nasal deletion (or a nasal vowel).
 This can be implemented as a factor that raises the probability of the
 single-segment parse
\begin_inset Foot
status open

\begin_layout Plain Layout
In fact, the word-final context modeled in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Phoneme-Split"

\end_inset

 does not constitute a homogeneous phonetic environment.
 Unless the target word is in absolute phrase-final position it will be
 followed by another word, beginning with either a consonant or a vowel.
 Because the two different possibilities present different perceptual environmen
ts, segment loss might only occur in the former, resulting in a type of
 liaison (e.g.
 
\begin_inset CommandInset citation
LatexCommand citealt
key "Tranel1981"

\end_inset

).
 There is also some evidence to suggest that changes restricted to specific
 words can be attributed to their historically higher occurrence rates in
 the perceptually disadvantaged environment (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "brown2012discourse"

\end_inset

)
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Velar palatalization was briefly discussed in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Competing-targets"

\end_inset

 as an example of gesture blending.
 Faster productions will result in more overlap between consonant and vowel,
 which should merge the two gestures more completely, as well as render
 the combined production shorter.
 Both phonetic properties should lead to an increase in the probability
 of the single-segment analysis.
 The many different ways in which palatalization can be realized in different
 languages (e.g., 
\begin_inset IPA

\begin_layout Standard
k>t͡ʃ
\end_layout

\end_inset

, 
\begin_inset IPA

\begin_layout Standard
k>kʲ
\end_layout

\end_inset

, 
\begin_inset IPA

\begin_layout Standard
kj>kʲ
\end_layout

\end_inset

, etc.), suggests a number of possible influencing factors, as well as an
 inherently larger space of possible parses.
 One such parse results in a two-segment analysis, with an intermediate
 tongue position for the consonantal gesture (see Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:/kʲ+i/"

\end_inset

); another results in a single-segment analysis with a complex two-target
 gesture (see Fig.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Palatalizationc"

\end_inset

).
 Perceptual asymmetries have been found with respect to the rate of misidentific
ation of [ki] sequences in noise and fast speech (as [ti] and 
\begin_inset IPA

\begin_layout Standard
[t͡ʃi]
\end_layout

\end_inset

, most commonly) suggesting that phonetic bias plays a role in this change
 (
\begin_inset CommandInset citation
LatexCommand citealt
key "Guion1998,Chang2001"

\end_inset

).
\end_layout

\begin_layout Standard
In contrast, the phenomenon of vowel lengthening (Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-2:-Lengthening"

\end_inset

) does not appear to be the direct result of overlap, blending, or masking.
 There is no consensus in the literature, however, regarding the phonetic
 source of this effect.
 In fact, there is not even agreement about whether the process is one of
 lengthening before voiced obstruents, or shortening before voiceless ones
 (
\begin_inset CommandInset citation
LatexCommand citealt
key "gimson1970introduction,wells1982accents"

\end_inset

).
 Of the hypotheses proposed, most have an articulatory basis (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "belasco1958variations,delattre1962some,chen1970vowel,lisker1974explaining,Klatt1976,moreton2004realization,schwartz2010phonology"

\end_inset

), but auditory/perceptual accounts have been offered as well (e.g., 
\begin_inset CommandInset citation
LatexCommand citealt
key "lisker1957closure,javkin1977phonetic,Kluender1988"

\end_inset

).
 None of these have been firmly established empirically, and strong arguments
 have been made against many of them.
 Without some idea of what the mechanism for the actual increase (or decrease)
 in length is, it is not possible to produce an insightful model.
 Work in progress suggests, in fact, that the apparent lengthening effect
 may be epiphenomenal: the result of partial temporal compensation, resulting
 from an upper limit on the duration of voiced obstruents (Morley & Smith
 
\emph on
in prep.
\emph default
).
 If this is right then it suggests another type of misparsing that can occur
 when multiple sources affect the same phonetic dimension in roughly the
 same way.
 In the case of vowel length, contributing sources include phrase-final
 lengthening, lengthening due to slowed speaking rate, and greater length
 due to an inherently longer vowel, creating ambiguity as to how the observed
 duration should be attributed
\begin_inset Foot
status open

\begin_layout Plain Layout
This story requires that some type of normalization be carried out – even
 if it is just a comparison between neighboring segments.
 If pure duration is the dimension of contrast, then it is hard to see how
 segments could be classified as `long' or `short' unless speaking rate,
 at minimum, is taken into account.
\end_layout

\end_inset

.
\end_layout

\begin_layout Standard
Other kinds of change, such as transphonologization, or chain shifting,
 suggest still other potential sources, but it is beyond the scope of this
 paper to speculate about their exact nature.
 However, given a hypothesis regarding the source of the phenomenon and
 the representational level at which it acts, it is possible to create an
 implemented model.
 Such a model may, or may not, bear much resemblance to those proposed in
 this work, yet the basic questions about the relationship between theory
 and model, and between model and implementation will remain the same.
 
\end_layout

\begin_layout Section
Summary & Conclusions
\end_layout

\begin_layout Standard
Computational models allow us to run experiments with language that are
 not possible in the real world, such as those at the timescale of diachronic
 change.
 This is a powerful and useful tool for making explicit tests of our current
 theories.
 Computational models can be used to establish existence proofs; demonstrating
 that it is possible to solve a problem in a particular way.
 On the flip side, however, modeling requires extensive simplification of
 the complex factors at play in language use and comprehension.
 And there is never any guarantee that the simplifications have not altered
 the problem to the point that the results no longer shed light on the phenomeno
n of interest.
 Implemented models are often tailored to specific problems, and may prove
 to be inconsistent with other known aspects of language.
 In order to get a model to run there are various implementational choices
 that must be made, choices that may, in fact, contain hidden theoretical
 assumptions.
 Thus, the interpretation of modeling results, just like the interpretation
 of the more traditional type of experimental results, must include serious
 consideration of potential confounds.
 
\end_layout

\begin_layout Standard
The purpose of the present work has been to bring the theoretical issues
 to the fore via explicit links between different implementational approaches
 and the types of representational structures they embody.
 In this way a number of representational inconsistencies, or paradoxes,
 were uncovered.
 The more transparent of these were the cases in which tokens were assigned
 two different underlying representations, or where an explicitly separate
 (i.e.
 stored) category was also subject to a process, giving the phenomenon a
 hybrid 
\noun on
state-process
\noun default
 status.
 In fact, there may be a paradox lurking in applying a process (e.g., knowing
 that tokens should be lengthened in a particular context) but failing to
 account for the effects of that process (lengthening) when adding the produced
 token back to the perceptual exemplar cloud.
 
\end_layout

\begin_layout Standard
Two apparent successes of the basic iterative exemplar model – accounting
 for frequency-based lenition, and phonetic similarity effects – were called
 into question.
 Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-1:-Context-Free"

\end_inset

 demonstrated that, depending on the specifics of how word frequency is
 represented, successive reduction of tokens does not necessarily produce
 the observed negative correlation between frequency and word length.
 Retention of fine-grained phonetic detail (without retention of production
 context) was shown to actually disrupt predictable phonetic allophony.
 Depending on other representational decisions, the result was either a
 single variant that occurred in all contexts, multiple variants that occurred
 unpredictably, or a continuously moving target (Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Context-Dependent-Iterativity"

\end_inset

).
 
\end_layout

\begin_layout Standard
Developing exemplar models that make the right kinds of predictions requires
 some force for constraining the powerful iterativity mechanism of the perceptio
n-production feedback loop.
 This is often accomplished in practice by filtering out tokens that fall
 between two existing, contrastive categories.
 But in the absence of contrast, something else is required to keep the
 categories bounded.
 There seems to be a common misconception that exemplar models do not require
 underlying representations, or targets.
 But models may in fact implement what amounts functionally to a soft target,
 or attractor, even if it is not identified as such (Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Soft-Targets"

\end_inset

).
 Such a target may, in fact, be necessary to produce bounded behavior.
\end_layout

\begin_layout Standard
Furthermore, the standard assumption of an identity mapping between perception
 and production obscures the complexity of the speech processing problem.
 In fact, differences between what speakers intend to produce, and what
 listeners perceive, are likely to play a large role in diachronic change
 that arises from synchronic variation, the very thing these models are
 trying to explain.
 Nor is it the case that iterative application of an articulation-based
 bias (such as anticipatory feature spread) can be assumed to lead to cumulativi
ty on the acoustic side (Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:Perception-Production"

\end_inset

).
 To produce the type of gradual increase that is desired, a change in the
 relative timing of articulators may be required.
 The explanation as to why such a change would occur is the answer to the
 actuation question itself.
\end_layout

\begin_layout Standard
A proposal was offered in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "chap:Phoneme-Split"

\end_inset

 for one way to account for phoneme genesis arising from allophonic split.
 The model was designed in a way that prioritized representational consistency,
 capturing both change and stability, and implementing a plausible mechanism
 for change at both local and global scales.
 The 
\begin_inset Quotes eld
\end_inset

correct
\begin_inset Quotes erd
\end_inset

 sub-lexical representations were not assumed, and therefore, neither was
 the allophonic rule (or production bias).
 Instead, the equivalent of an articulatory representation was decided independe
ntly for each token.
 Feedback occurred in the dependence of the parsing probabilities on the
 values of the articulatory parameters.
 In the reported simulations there was only one choice to be made by the
 speaker/listener, whether to store or generate the degree of overlap between
 the two articulatory gestures.
 Stability was achieved by a general-purpose force (speaking rate), acting
 bi-directionally, to both lengthen and shorten tokens.
 Different stable states resulted from different resting activation levels,
 which affected the rapidity with which the words were produced.
 This result hinged on two properties of the model: the dependence of the
 speaking rate effect on word duration (longer tokens were lengthened more
 than shorter tokens for the same decrease in rate), and the implementation
 of resting activation as a shift in the mean of the speaking rate distribution.
 Numerous other implementational choices are possible, but only a small
 fraction of them lead to a theoretically coherent, cognitively plausible,
 empirically adequate outcome.
 Thus, the existence proof embodied in the Multiple-Parse Model has merit
 in and of itself.
 The results also raise the possibility that certain consistently intractable
 problems in the study of change and actuation may be artifacts of the overt
 and covert assumptions of the traditional notational system.
\end_layout

\begin_layout Standard
On the one hand, the work in this book represents relatively minor variations
 on existing proposals and models: the basic exemplar architecture in which
 sub-phonemic detail is retained; the role of word frequency in sound change;
 ambiguity in surface forms as the driver of variation; etc.
 Its primary innovation may be in bringing together and explicitly implementing
 those elements.
 Yet, the result is a radical re-conceptualization of basic phonological
 tenets: that phoneme split is neither phoneme creation, nor allophone loss;
 that, in fact, neither allophonic rules, nor phonemic inventories exist
 as traditionally described; that phonological rules as we typically understand
 them may only arise under restricted conditions, requiring morphological
 antecedents and a more explicit stage of learner generalization.
 This conceptual shift was largely a consequence of forcing diachronic and
 synchronic representations to match, revealing that questions about how
 sound categories change are really questions about what sound categories
 are – how they are mentally represented –, and that neither question can
 be adequately answered without the other.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
bibfiles "/Users/becca/Dropbox/CurrentWork/Technical/All_Refs"
options "/Users/becca/Dropbox/CurrentWork/Technical/linquiry2abstract"

\end_inset


\end_layout

\begin_layout Standard
\start_of_appendix
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
numberwithin{equation}{section}
\end_layout

\begin_layout Plain Layout


\backslash
numberwithin{table}{section}
\end_layout

\begin_layout Plain Layout


\backslash
numberwithin{figure}{section}
\end_layout

\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:Appendix A"

\end_inset

Model Parameters: Chapters 1 - 4
\end_layout

\begin_layout Standard
For each of the basic exemplar models for which simulations were run, the
 following parameter values were used:
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
Simulation parameter values
\end_layout

\end_inset


\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="8">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\varepsilon$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma_{error}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\alpha$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
p
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\beta$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
N
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Baseline Model Ch.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "sec:The-Exemplar-Model"

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(.3)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
–
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
–
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
–
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model 1: Context-Free Ch.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-1:-Context-Free"

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
.3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
.5
\begin_inset Formula $\sigma$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
–
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
–
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
–
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model 2: Context-Dependent (Gradient) Ch.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Phrase-Final Lengthening"

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
.3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
.25
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
–
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
–
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Model 3: Context-Dependent (Discrete) Ch.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-3:-Categorical"

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
.3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
–
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
.5
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
–
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
–
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Soft-Target Model Ch.
 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Soft-Targets"

\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
.3
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\sigma$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
.1
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
.25
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
.6
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
2
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
50
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:Appendix B"

\end_inset

The Frequency Effect
\end_layout

\begin_layout Standard
This material is supplemental to Chapters 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-1:-Context-Free"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Word-Frequency"

\end_inset

 of the main text.
\end_layout

\begin_layout Standard
The iterative model implies that the frequency effect must arise in the
 lifetime of the speaker, and only after they have had sufficient exposure
 to a given (high frequency) category.
 This may happen very quickly.
 However, the less time it takes, the more opportunities there will be for
 lower-frequency categories to 
\begin_inset Quotes eld
\end_inset

catch up
\begin_inset Quotes erd
\end_inset

.
 Therefore, in order to give the best chance to the basic model, we will
 assume the largest possible time period in which the effect could arise:
 the age of the experimental population for which frequency effects are
 found.
 As the pool of participants for psychology and linguistics experiments
 is most often university undergraduates, we will take 20 years to be the
 maximum amount of time necessary to produce a reduction in duration comparable
 to what has been reported in the literature.
\end_layout

\begin_layout Standard
We don't know how many model iterations correspond to 20 years.
 But we will define the number of productions during this time, for a word
 of frequency, 
\emph on
f
\emph default
, as 
\begin_inset Formula $n_{f}$
\end_inset

, and the proportion by which it is reduced, as 
\begin_inset Formula $\delta_{n_{f}}$
\end_inset

, from an initial average duration of 
\begin_inset Formula $\overline{d_{0}}$
\end_inset

.
 This period of time will be called an epoch (
\emph on
e
\emph default
).
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{d_{n_{f}}}=\overline{d_{0}}-\delta_{n_{f}}\overline{d_{0}}\label{eq:epoch-dur}
\end{equation}

\end_inset

To simplify the problem, we will consider a scenario in which there is only
 a single token belonging to each category, located at the category mean,
 which is replaced, each time production occurs, by a token reduced by a
 fixed proportion of the current duration.
 With this simplification all categories will reduce faster, since it is
 always the most reduced token that is chosen in production.
 However, since all measures are comparisons between categories of different
 frequencies (rather than absolute values), this should not affect the result.
 Low and high frequency categories are also of exactly the same size token-wise
 in this simplified scenario, and only update-rate differentiates them.
 Equalizing low- and high -frequency categories in this way does affect
 the outcome, as we saw in Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-1:-Context-Free"

\end_inset

, but it advantages the basic model by ensuring that higher-frequency words
 are always shorter than lower-frequency ones.
\end_layout

\begin_layout Standard
In the simplified scenario, each generation is exponentially more reduced
 than the last.
 From Eq (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:linear bias"

\end_inset

): 
\begin_inset Formula $x_{o(+n)}=x_{o}\left(1-\alpha\right)^{n}$
\end_inset

, we can derive Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Reduction"

\end_inset

), which expresses the duration, after 1 epoch, for a word category of frequency
, 
\emph on
f
\emph default
, and an initial average duration of 
\begin_inset Formula $\overline{d_{0}}$
\end_inset

.
 Rewriting Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:linear bias"

\end_inset

) in terms of these variables:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{d_{n_{f}}}=\overline{d_{0}}(1-\alpha)^{n_{f}}
\end{equation}

\end_inset

Substituting in from Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:epoch-dur"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{d_{0}}-\delta_{n_{f}}\overline{d_{0}}=\overline{d_{0}}(1-\alpha)^{n_{f}}
\end{equation}

\end_inset

And, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\delta_{n_{f}}=1-(1-\alpha)^{n_{f}}\label{eq:Reduction}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We don't know what the amount of reduction over 1 epoch is.
 But we do have an idea of the size of the frequency effect: word duration
 as a function of frequency (log frequency is typically what is plotted
 in order to make the frequency distribution closer to Normal (see, e.g.,
 
\begin_inset CommandInset citation
LatexCommand citet
key "gahl2012reduce"

\end_inset

)).
 If we assume a linear relation between word duration and log frequency,
 then for each unit change in log frequency, the difference in word duration
 should be equal to a constant value (
\emph on
b
\emph default
).
 Thus, the predicted difference in duration between a low frequency and
 high frequency word is related to the difference in frequencies by the
 following formula: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{\Delta d_{e}}{log(f_{L})-log(f_{H})}=b\label{eq:log-linear}
\end{equation}

\end_inset

If speaker/listeners begin at birth with equal experience of all words –
 meaning, none –, then the differences in duration that accrue over the
 course of an epoch will be due entirely to the amount of reduction that
 occurs over that epoch.
 By the time that one epoch has passed, the higher frequency word of any
 pair will have reduced more than its counterpart.
 Assuming that the two words in question are otherwise identical, for our
 purposes, that they have the same original duration, then the difference
 in absolute duration at that time will be given by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\Delta d_{e}=\delta_{n_{L}}-\delta_{n_{H}}\label{eq:duration-diff}
\end{equation}

\end_inset

Combining (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:log-linear"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:duration-diff"

\end_inset

),
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\delta_{n_{L}}-\delta_{n_{H}}=b[log(\frac{f_{L}}{f_{H}})]
\end{equation}

\end_inset

Substituting in Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Reduction"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
1-(1-\alpha)^{n_{L}}-[1-(1-\alpha)^{n_{H}}]=b[log(\frac{f_{L}}{f_{H}})]
\end{equation}

\end_inset

Simplifying:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
(1-\alpha)^{n_{H}}-(1-\alpha)^{n_{L}}=b[log(\frac{f_{L}}{f_{H}})]\label{eq:reduction-to-freq}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The higher the frequency of a given word, the more times it should be produced
 within a given time period.
 And if reduction is proportional to the log frequency, with every production
 resulting in a given amount of reduction, then the number of productions
 should also be proportional to log frequency.
 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
n_{f}=rlog(f)\label{eq:n-productions}
\end{equation}

\end_inset

Substituting (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:n-productions"

\end_inset

) into (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:reduction-to-freq"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
(1-\alpha)^{rlog(f_{H})}-(1-\alpha)^{rlog(f_{L})}=b[log(\frac{f_{L}}{f_{H}})]\label{eq:boundary cond}
\end{equation}

\end_inset

Assuming that it is possible to find values for 
\begin_inset Formula $\alpha$
\end_inset

 and 
\emph on
r
\emph default
 that satisfy Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:boundary cond"

\end_inset

) for all frequencies, the additional reduction that will occur over the
 lifetime of the speaker can then be determined.
 
\end_layout

\begin_layout Standard
If 1 epoch corresponds to about 20 years, then there will be about 4 over
 the lifetime of an individual.
 If we assume a constant rate of production for each category proportional
 to its frequency, then lifetime (
\emph on
E
\emph default
) average reduction is given by 
\begin_inset Formula $\delta_{E_{f}}=1-(1-\alpha)^{4n_{f}}$
\end_inset

, which can be rewritten as:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\delta_{E_{f}}=1-(1-\alpha)^{4rlog(f)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
With the necessary constants, we can now determine the difference in reduction
 between the same two word categories after 4 epochs.
 If we assume that there exists a floor beyond which words cannot reduce
 further, then we will need to determine if any words are predicted to reach
 floor in the lifetime of the speaker, and what effect that will have on
 the behavior of the frequency dependence – either entirely neutralizing
 the duration difference between certain words, or decreasing that difference
 to some extent.
 
\end_layout

\begin_layout Standard
The exact predictions of the linearly biased frequency model will depend
 on a host of implementational details.
 As already discussed in the text, the choice of whether lower-frequency
 categories should have proportionally fewer tokens than higher-frequency
 categories will affect the outcome.
 Other parameters that have the potential to alter the outcome include whether
 or not each individual experience is automatically added to memory – or
 only a certain minimum number, or some average of recent experience –,
 and how quickly older memories decay, being replaced by new experiences.
 It may be possible, if unlikely, that at least one set of parameter values
 exists that will prevent any words reaching floor within the lifetime of
 the speaker.
 However, under any parameter settings, all words are predicted to continue
 reducing over the lifetime of the speaker.
 This prediction is empirically testable.
\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:Appendix C"

\end_inset

Derivation of State Model
\end_layout

\begin_layout Standard
This material is supplemental to Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Lengthening-as-State"

\end_inset

 of the main text.
\end_layout

\begin_layout Standard
For the Pure State Model (G), with 2-targets, each sub-category is subject
 to two forces: entrenchment, and inertia.
 Under the simplifying assumption that each sub-category can be treated
 as a Normal distribution with constant variance, the equilibrium locations
 of the sub-category means can be derived in the following way.
 At equilibrium the entrenchment force is balanced by the inertial force
 due to each sub-category's attractor.
 The location of the sub-category mean is the location at which the displacement
 that would occur due to the entrenchment force is exactly counter-acted
 by the displacement that would occur due to the inertia force.
 For the non-biased sub-category this equilibrium occurs under the following
 conditions:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\beta(\overline{x_{E}^{NB}}-N)=\varepsilon(\overline{x_{E}}-\overline{x_{E}^{NB}})\label{eq:xNB-state}
\end{equation}

\end_inset

For the biased sub-category, equilibrium occurs when:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\alpha(\overline{x_{E}^{B}}-L)=\varepsilon(\overline{x_{E}}-\overline{x_{E}^{B}})\label{eq:xB-State}
\end{equation}

\end_inset

Because the entrenchment force depends on the global mean, so too do the
 two equilibrium equations.
 In turn, the global mean can be expressed as a function of the sub-category
 means (where the proportion of biased tokens is given by 
\emph on
p
\emph default
): 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=(1-p)\overline{x_{E}^{NB}}+p\overline{x_{E}^{B}}\label{eq:weighted-means}
\end{equation}

\end_inset

With three equations, we can solve for the three distribution means.
 Solving for 
\begin_inset Formula $\overline{x_{E}^{NB}}$
\end_inset

 in Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:xNB-state"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}^{NB}}=\frac{\beta N+\varepsilon\overline{x_{E}}}{\beta+\varepsilon}
\end{equation}

\end_inset

Solving for 
\begin_inset Formula $\overline{x_{E}^{B}}$
\end_inset

 in Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:xB-State"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}^{B}}=\frac{\alpha L+\varepsilon\overline{x_{E}}}{\alpha+\varepsilon}
\end{equation}

\end_inset

Substituting these two values into Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:weighted-means"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=(1-p)\frac{\beta N+\varepsilon\overline{x_{E}}}{\beta+\varepsilon}+p\frac{\alpha L+\varepsilon\overline{x_{E}}}{\alpha+\varepsilon}
\end{equation}

\end_inset

Solving for 
\begin_inset Formula $\overline{x_{E}}$
\end_inset

 as a function of 
\emph on
p, 
\emph default
and collecting terms:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=\frac{(1-p)\beta N}{\beta+\varepsilon}+\frac{(1-p)\varepsilon\overline{x_{E}}}{\beta+\varepsilon}+\frac{p\alpha L}{\alpha+\varepsilon}+\frac{p\varepsilon\overline{x_{E}}}{\alpha+\varepsilon}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}-\frac{(1-p)\varepsilon\overline{x_{E}}}{\beta+\varepsilon}-\frac{p\varepsilon\overline{x_{E}}}{\alpha+\varepsilon}=\frac{(1-p)\beta N}{\beta+\varepsilon}+\frac{p\alpha L}{\alpha+\varepsilon}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{\overline{x_{E}}(\beta+\varepsilon)(\alpha+\varepsilon)-(\alpha+\varepsilon)(1-p)\varepsilon\overline{x_{E}}-(\beta+\varepsilon)p\varepsilon\overline{x_{E}}}{(\beta+\varepsilon)(\alpha+\varepsilon)}=\frac{(1-p)\beta N}{\beta+\varepsilon}+\frac{p\alpha L}{\alpha+\varepsilon}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{\overline{x_{E}}(\beta+\varepsilon)(\alpha+\varepsilon)-(\alpha+\varepsilon)(1-p)\varepsilon\overline{x_{E}}-(\beta+\varepsilon)p\varepsilon\overline{x_{E}}}{(\beta+\varepsilon)(\alpha+\varepsilon)}=\frac{(1-p)\beta N(\alpha+\varepsilon)+p\alpha L(\beta+\varepsilon)}{(\alpha+\varepsilon)(\beta+\varepsilon)}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}[(\beta+\varepsilon)(\alpha+\varepsilon)-(\alpha+\varepsilon)(1-p)\varepsilon-(\beta+\varepsilon)p\varepsilon]=(1-p)\beta N(\alpha+\varepsilon)+p\alpha L(\beta+\varepsilon)
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=\frac{(1-p)\beta N(\alpha+\varepsilon)+p\alpha L(\beta+\varepsilon)}{(\beta+\varepsilon)(\alpha+\varepsilon)-(\alpha+\varepsilon)(1-p)\varepsilon-(\beta+\varepsilon)p\varepsilon}\label{eq:Model G-eq}
\end{equation}

\end_inset

Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Model G-eq"

\end_inset

) is a complex function of 
\begin_inset Formula $\alpha,\beta,\varepsilon,N,L$
\end_inset


\family roman
\series medium
\shape up
\size normal
\emph off
\bar no
\strikeout off
\uuline off
\uwave off
\noun off
\color none
, and 
\family default
\series default
\shape default
\size default
\emph on
\bar default
\strikeout default
\uuline default
\uwave default
\noun default
\color inherit
p
\emph default
, the derivative of which is not trivially calculated.
 For known values of 
\begin_inset Formula $\alpha,\beta,\varepsilon,N$
\end_inset

, and 
\emph on
L
\emph default
 , 
\begin_inset Formula $\overline{x_{E}}(p)$
\end_inset

 can be determined exactly.
 The general behavior of this function, however, can be understood via the
 following chain of reasoning.
 
\end_layout

\begin_layout Standard
For a given 
\begin_inset Formula $p=p_{i}$
\end_inset

 (for 
\begin_inset Formula $p_{i}<1$
\end_inset

), the equilibrium location of the global mean can be found using Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Model G-eq"

\end_inset

).
 Now imagine that p increases from 
\begin_inset Formula $p_{i}$
\end_inset

 to 
\begin_inset Formula $p_{j}$
\end_inset

.
 This will result in the global mean moving closer to the biased sub-category
 (Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:weighted-means"

\end_inset

)).
 A change in the global mean will cause a change in the entrenchment force
 for both sub-categories.
 It will increase for the non-biased sub-category, which is now farther
 from the global mean; and it will decrease in exactly the same degree for
 the biased sub-category, which is now closer to the global mean.
\end_layout

\begin_layout Standard
Because inertia does not depend on 
\emph on
p
\emph default
, the lefthand sides of Eqs.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:xNB-state"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:xB-State"

\end_inset

) will remain constant.
 Thus, the non-biased sub-category will shift in the direction of the mean
 – rightward – as a result of the increase in 
\emph on
p
\emph default
.
 The decrease in the entrenchment force on the biased sub-category, conversely,
 will cause a shift away from the mean, and towards the attractor at 
\emph on
L
\emph default
.
 This is also a rightward shift, however.
 The net effect will be to perturb the sub-categories from their former
 equilibrium locations to points farther to the right, and closer to 
\emph on
L
\emph default
.
 As 
\emph on
p
\emph default
 increases, 
\begin_inset Formula $\overline{x_{E}}$
\end_inset

 will always increase (as long as both sub-categories are located between
 
\emph on
N
\emph default
 and 
\emph on
L
\emph default
).
\end_layout

\begin_layout Standard
The distance between the means of the two sub-categories can also be written
 as a function of 
\emph on
p
\emph default
.
 Once equilibrium has been reached, the separation can be derived from Eqs.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:xNB-state"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:xB-State"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\Delta\overline{x_{E}}\equiv\overline{x_{E}^{B}}-\overline{x_{E}^{NB}}=\frac{\alpha L+\varepsilon\overline{x_{E}}}{\alpha+\varepsilon}-\frac{\beta N+\varepsilon\overline{x_{E}}}{\beta+\varepsilon}
\end{equation}

\end_inset

Collecting terms and simplifying:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
=\frac{\alpha L}{\alpha+\varepsilon}-\frac{\beta N}{\beta+\varepsilon}+\frac{\varepsilon\overline{x_{E}}}{\alpha+\varepsilon}-\frac{\varepsilon\overline{x_{E}}}{\beta+\varepsilon}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
=\frac{\alpha L}{\alpha+\varepsilon}-\frac{\beta N}{\beta+\varepsilon}+\varepsilon\overline{x_{E}}[\frac{1}{\alpha+\varepsilon}-\frac{1}{\beta+\varepsilon}]
\end{equation}

\end_inset

The change in sub-category separation as a function of changing 
\emph on
p
\emph default
 is thus given by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{\partial\Delta\overline{x_{E}}}{\partial p}=\frac{\partial\overline{x_{E}}}{\partial p}\varepsilon[\frac{1}{\alpha+\varepsilon}-\frac{1}{\beta+\varepsilon}]\label{eq:Model G-sep}
\end{equation}

\end_inset

In order to determine 
\begin_inset Formula $\frac{\partial\Delta\overline{x_{E}}}{\partial p}$
\end_inset

, we must be able to calculate 
\begin_inset Formula $\frac{\partial\overline{x_{E}}}{\partial p}$
\end_inset

.
 For the special case in which all forces have the same strength (
\begin_inset Formula $\alpha=\beta=\varepsilon$
\end_inset

), it is straightforward to calculate the derivative of Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Model G-eq"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=\frac{2\alpha^{2}N+p(2\alpha^{2}L-2\alpha^{2}N)}{4\alpha^{2}-2\alpha^{2}}
\end{equation}

\end_inset

Collecting terms and simplifying:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=\frac{2\alpha^{2}[N+pL-pN]}{2\alpha^{2}[2-1]}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=N+p(L-N)\label{eq: State-special case}
\end{equation}

\end_inset

This gives the expected behavior; for 
\begin_inset Formula $p=0$
\end_inset

, there is only the non-biased distribution, which is stable at 
\emph on
N
\emph default
, and for 
\begin_inset Formula $p=1$
\end_inset

, there is only the biased distribution, which is stable at 
\emph on
L
\emph default
.
 For equal numbers of biased and non-biased variants, each sub-category
 stabilizes at the same distance from its attractor, and the global mean
 is halfway between the two.
 The change in the global category mean as a function of 
\emph on
p
\emph default
 is a positive, fixed value: 
\begin_inset Formula $L-N$
\end_inset

, the derivative of (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq: State-special case"

\end_inset

).
 Plugging this value for 
\begin_inset Formula $\frac{\partial\overline{x_{E}}}{\partial p}$
\end_inset

 into Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Model G-sep"

\end_inset

) gives:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\frac{\partial\Delta\overline{x_{E}}}{\partial p}=(L-N)\alpha[\frac{1}{2\alpha}-\frac{1}{2\alpha}]=0\label{eq:separation-special case}
\end{equation}

\end_inset

Thus, while the overall category mean gets larger as 
\emph on
p
\emph default
 increases, the separation between the categories remains constant.
 
\end_layout

\begin_layout Standard
In the general case, the separation between the two sub-categories will
 show different behavior for different parameter values.
 Because 
\begin_inset Formula $\frac{\partial\overline{x_{E}}}{\partial p}>0$
\end_inset

, the sign of 
\begin_inset Formula $\frac{\partial\Delta\overline{x_{E}}}{\partial p}$
\end_inset

 depends on the 
\begin_inset Formula $\varepsilon[\frac{1}{\alpha+\varepsilon}-\frac{1}{\beta+\varepsilon}]$
\end_inset

 term.
 When 
\begin_inset Formula $\alpha<\beta$
\end_inset

, the separation increases with increasing 
\emph on
p
\emph default
.
 This follows from the fact that 
\begin_inset Formula $\frac{\partial\Delta\overline{x_{E}}}{\partial p}$
\end_inset

 is positive only when 
\begin_inset Formula $\varepsilon[\frac{1}{\alpha+\varepsilon}-\frac{1}{\beta+\varepsilon}]>0$
\end_inset

.
 For 
\begin_inset Formula $\varepsilon[\frac{1}{\alpha+\varepsilon}-\frac{1}{\beta+\varepsilon}]$
\end_inset

 to be greater than zero it must be the case that 
\begin_inset Formula $\frac{1}{\alpha+\varepsilon}>\frac{1}{\beta+\varepsilon}$
\end_inset

.
 This, in turn, requires that 
\begin_inset Formula $\alpha<\beta$
\end_inset

.
 By the same reasoning, the separation decreases as a function of increasing
 
\emph on
p
\emph default
 when 
\begin_inset Formula $\alpha>\beta$
\end_inset

.
 Finally, the separation remains constant when 
\begin_inset Formula $\alpha=\beta$
\end_inset

, because this entails that 
\begin_inset Formula $\varepsilon[\frac{1}{\alpha+\varepsilon}-\frac{1}{\beta+\varepsilon}]=0$
\end_inset

, verifying the result in Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:separation-special case"

\end_inset

).
\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:Appendix D"

\end_inset

Derivation of Process Model
\end_layout

\begin_layout Standard
This material is supplemental to Chapter 
\begin_inset CommandInset ref
LatexCommand ref
reference "subsec:Model-B:-Lengthening"

\end_inset

 of the main text.
\end_layout

\begin_layout Standard
For The Pure Process Model, there is a single category, and all tokens are
 subject to the same inertial force, in proportion to their distance from
 the single attractor at 
\emph on
N
\emph default
.
 Additionally, a proportion 
\emph on
p
\emph default
 of randomly selected tokens undergo a lengthening process, moving away
 from the rest of the distribution during production.
 The simplifying assumption, that each sub-distribution can be treated as
 a Normal distribution with constant variance, is adopted.
 To derive the model behavior we will look at the contribution of the different
 forces in stages.
 This derivation references the stages depicted in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Derivation"

\end_inset

.
\end_layout

\begin_layout Standard
First we apply the lengthening process, at time 
\begin_inset Formula $t$
\end_inset

, to tokens drawn from a distribution with a global mean of 
\begin_inset Formula $\overline{x_{t}}$
\end_inset

.
 These tokens are simultaneously subjected to an inertial force.
 Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Model-B B-Prime"

\end_inset

) gives the mean of the biased sub-distribution at time 
\emph on
t,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{t}^{B}}^{\prime}=\overline{x_{t}}(1+\alpha)+\beta(N-\overline{x_{t}})\label{eq:Model-B B-Prime}
\end{equation}

\end_inset

and Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Model-B NB-Prime"

\end_inset

) give the means of the non-biased sub-distribution at time 
\emph on
t
\emph default
.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{t}^{NB}}^{\prime}=\overline{x_{t}}+\beta(N-\overline{x_{t}})\label{eq:Model-B NB-Prime}
\end{equation}

\end_inset

On average, a proportion 
\emph on
p
\emph default
 of the distribution will be lengthened, thus the location of the global
 mean after lengthening and inertia apply, can be expressed as
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{t}}^{\prime}=(1-p)\overline{x_{t}^{NB}}^{\prime}+p\overline{x_{t}^{B}}^{\prime}\label{eq:Model-B weight mean}
\end{equation}

\end_inset

Entrenchment must also be applied in order to determine the final outcome,
 but entrenchment does not affect the location of the global mean, only
 the locations of the sub-distribution means, and their separation.
 To see this, we can compare the global mean before and after entrenchment
 applies.
 After entrenchment, the means of each sub-distribution are given by:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{t}^{B}}^{\prime\prime}=\overline{x_{t}^{B}}^{\prime}-\varepsilon(\overline{x_{t}}^{\prime}-\overline{x_{t}^{B}}^{\prime})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{t}^{NB}}^{\prime\prime}=\overline{x_{t}^{NB}}^{\prime}-\varepsilon(\overline{x_{t}}^{\prime}-\overline{x_{t}^{NB}}^{\prime})
\end{equation}

\end_inset

Substituting into Eq.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Model-B weight mean"

\end_inset

), gives
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x}_{t}^{''}=(1-p)[\overline{x_{t}^{NB}}^{\prime}-\varepsilon(\overline{x_{t}}^{\prime}-\overline{x_{t}^{NB}}^{\prime})]+p[\overline{x_{t}^{B}}^{\prime}-\varepsilon(\overline{x}_{t}^{\prime}-\overline{x_{t}^{B}}^{\prime})]
\end{equation}

\end_inset

Simplifying and collecting terms:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
=\overline{x_{t}}^{\prime}-\varepsilon(1-p)(\overline{x_{t}}^{\prime}-\overline{x_{t}^{NB}}^{\prime})-p\varepsilon(\overline{x_{t}}^{\prime}-\overline{x_{t}^{B}}^{\prime})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
=\overline{x_{t}}^{\prime}+\varepsilon(1-p)\overline{x_{t}^{NB}}^{\prime}-[\varepsilon(1-p)+p\varepsilon]\overline{x_{t}}^{\prime}+p\varepsilon\overline{x_{t}^{B}}^{\prime}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
=\overline{x_{t}}^{\prime}-\varepsilon\overline{x_{t}}^{\prime}+\varepsilon[(1-p)\overline{x_{t}^{NB}}^{\prime}+p\overline{x_{t}^{B}}^{\prime}]
\end{equation}

\end_inset

The term 
\begin_inset Formula $(1-p)\overline{x_{t}^{NB}}^{\prime}+p\overline{x_{t}^{B}}^{\prime}$
\end_inset

 is equivalent to 
\begin_inset Formula $\overline{x_{t}}^{\prime}$
\end_inset

 by Eq (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Model-B weight mean"

\end_inset

).
 Therefore
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{t}}^{\prime\prime}=\overline{x_{t}}^{\prime}-\varepsilon\overline{x_{t}}^{\prime}+\varepsilon\overline{x_{t}}^{\prime}=\overline{x_{t}}^{\prime}
\end{equation}

\end_inset

Because it does not depend on entrenchment, the global mean at equilibrium
 can be determined directly from (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Model-B B-Prime"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Model-B NB-Prime"

\end_inset

).
 Equilibrium occurs when the two sub-distributions are also at equilibrium,
 and the global mean stops changing: 
\begin_inset Formula $\overline{x_{E}}=\overline{x_{E}}^{\prime}$
\end_inset

, 
\begin_inset Formula $\overline{x_{E}^{NB}}^{\prime}=\overline{x_{E}^{NB}}$
\end_inset

, and 
\begin_inset Formula $\overline{x_{E}^{B}}^{\prime}=\overline{x_{E}^{B}}$
\end_inset

.
 Therefore,
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=(1-p)\overline{x_{E}^{NB}}^{\prime}+p\overline{x_{E}^{B}}^{\prime}
\end{equation}

\end_inset

Substituting in Eqs.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Model-B B-Prime"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Model-B NB-Prime"

\end_inset

):
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=[\overline{x_{E}}+\beta(N-\overline{x_{E}})]-p[\overline{x_{E}}+\beta(N-\overline{x_{E}})]+p[\overline{x_{E}}+\overline{x_{E}}\alpha+\beta(N-\overline{x_{E}})]
\end{equation}

\end_inset

Simplifying and collecting terms:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=\overline{x_{E}}+\beta(N-\overline{x_{E}})+p\overline{x_{E}}\alpha-p[\overline{x_{E}}+\beta(N-\overline{x_{E}})]+p[\overline{x_{E}}-\beta(N-\overline{x_{E}})]
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=\overline{x_{E}}+\beta(N-\overline{x_{E}})+p\overline{x_{E}}\alpha
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=\overline{x_{E}}(1-\beta+p\alpha)+\beta N
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}(1-1+\beta-p\alpha)=\beta N
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}}=\frac{\beta N}{\beta-p\alpha}\label{eq:equ-mean-state}
\end{equation}

\end_inset

For the case when 
\begin_inset Formula $p\alpha<\beta$
\end_inset

, the denominator in (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:equ-mean-state"

\end_inset

) is positive.
 As 
\emph on
p
\emph default
 increases (but 
\begin_inset Formula $p\alpha$
\end_inset

 remains smaller than 
\begin_inset Formula $\beta$
\end_inset

), the denominator decreases, and the global mean increases.
 As 
\begin_inset Formula $p\alpha$
\end_inset

 approaches 
\begin_inset Formula $\beta$
\end_inset

, the global mean goes to infinity; lengthening is unbounded.
 For 
\begin_inset Formula $p\alpha>\beta$
\end_inset

 the only stable point is negative, and thus there is no well-defined equilibriu
m.
 The 
\noun on
process
\noun default
 model is thus only stable if the lengthening strength is not too great,
 and the percentage of biasing contexts is not too large.
 
\end_layout

\begin_layout Standard
To calculate the dependence of the sub-distribution separation on 
\emph on
p,
\emph default
 the effect of entrenchment must be included.
 The equilibrium separation is defined as: 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\triangle\overline{x_{E}}^{\prime\prime}\equiv\overline{x_{E}^{B}}^{\prime\prime}-\overline{x_{E}^{NB}}^{\prime\prime}
\end{equation}

\end_inset

And 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}^{B}}^{\prime\prime}=\overline{x_{E}^{B}}^{\prime}+\varepsilon(\overline{x_{E}}^{\prime}-\overline{x_{E}^{B}}^{\prime})
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}^{NB}}^{\prime\prime}=\overline{x_{E}^{NB}}^{\prime}+\varepsilon(\overline{x_{E}}^{\prime}-\overline{x_{E}^{NB}}^{\prime})
\end{equation}

\end_inset

Therefore, 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}^{B}}^{\prime\prime}-\overline{x_{E}^{NB}}^{\prime\prime}=\overline{x_{E}^{B}}^{\prime}-\overline{x_{E}^{NB}}^{\prime}+\varepsilon(\overline{x_{E}}^{\prime}-\overline{x_{E}^{B}}^{\prime})-\varepsilon(\overline{x_{E}}^{\prime}-\overline{x_{E}^{NB}}^{\prime})
\end{equation}

\end_inset

Collecting terms:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
=\overline{x_{E}^{B}}^{\prime}-\overline{x_{E}^{NB}}^{\prime}-\varepsilon(\overline{x_{E}^{B}}^{\prime}-\overline{x_{E}^{NB}}^{\prime})
\end{equation}

\end_inset

and
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\triangle\overline{x_{E}}^{\prime\prime}=(1-\varepsilon)(\overline{x_{E}^{B}}^{\prime}-\overline{x_{E}^{NB}}^{\prime})
\end{equation}

\end_inset

The observed separation at equilibrium depends on the separation due to
 prior model forces.
 From Eqs.
 (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Model-B B-Prime"

\end_inset

) and (
\begin_inset CommandInset ref
LatexCommand ref
reference "eq:Model-B NB-Prime"

\end_inset

), 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\overline{x_{E}^{B}}^{\prime}-\overline{x_{E}^{NB}}^{\prime}=\overline{x_{E}}(1+\alpha)+\beta(N-\overline{x_{E}})-[\overline{x_{E}}+\beta(N-\overline{x_{E}})]
\end{equation}

\end_inset

This reduces to 
\begin_inset Formula $\alpha\overline{x_{E}}$
\end_inset

.
 Note that this is exactly the amount that biased tokens are shifted away
 from the mean at equilibrium.
 Because this is a 
\noun on
process
\noun default
 model, the separation created by the lengthening bias only exists transiently,
 and it is not possible for any specific subset of tokens to continue to
 increase their separation from the rest of the distribution.
 Therefore, the prior separation between the sub-distribution means is always
 given by the lengthening bias applied to that mean.
 And the total separation, by 
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\varDelta\overline{x_{E}}=(1-\epsilon)(\alpha\overline{x_{E}}).\label{eq:Cat Sep-1}
\end{equation}

\end_inset

In the stable parameter range, where 
\begin_inset Formula $\overline{x_{E}}$
\end_inset

 increases as 
\emph on
p
\emph default
 increases, the separation of the sub-distributions also increases, but
 more slowly, by a factor of 
\begin_inset Formula $\alpha(1-\epsilon)$
\end_inset

.
\end_layout

\begin_layout Chapter
\begin_inset CommandInset label
LatexCommand label
name "chap:Appendix E"

\end_inset

Nasalization Model Parameters
\end_layout

\begin_layout Standard
The following parameters were identical for the two models:
\end_layout

\begin_layout Itemize
The entrenchment strength is set to 
\begin_inset Formula $\varepsilon=.2$
\end_inset


\end_layout

\begin_layout Itemize
The production error on each articulatory dimension is drawn from the distributi
on 
\begin_inset Formula $\mathcal{\mathscr{N}\left(\mathrm{0,.25\sigma_{x^{z}}}\right)}$
\end_inset

, where 
\begin_inset Formula $\sigma_{x^{Z}}$
\end_inset

 indicates the standard deviation of the current distribution of stored
 tokens on dimension 
\begin_inset Formula $x^{Z}$
\end_inset


\end_layout

\begin_layout Itemize
Speaking Rate:
\end_layout

\begin_deeper
\begin_layout Itemize
Expansion force (
\emph on
E
\emph default
) is a random variable distributed according to 
\begin_inset Formula $\mathcal{\mathscr{N}\left(\mathrm{0,.25}\right)}$
\end_inset

.
 
\end_layout

\begin_layout Itemize
The speaking rate transformation lengthens or shortens a given duration
 parameter, according to the following dependence on 
\emph on
E:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
x_{i}^{O^{\prime}}=\frac{2x_{i}^{O}}{(1+e^{k_{O}E})}\label{eq:Speaking rate transform-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
x_{i}^{V^{\prime}}=\frac{2x_{i}^{V}}{(1+e^{-k_{V}E})}\label{eq:Speaking rate transform-1-1}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
x_{i}^{N\prime}=\frac{2x_{i}^{N}}{(1+e^{-k_{N}E})}\label{eq:Speaking rate transform-1-1-1}
\end{equation}

\end_inset


\end_layout

\end_deeper
\begin_layout Standard
For these simulations all gestures are set to the same elasticity (
\begin_inset Formula $k_{O}=k_{N}=k_{V}=1$
\end_inset

).
 
\end_layout

\end_deeper
\begin_layout Itemize
Model outputs are reported after 10,000 iterations
\end_layout

\begin_layout Itemize
\begin_inset Formula $x_{i}^{O}$
\end_inset

 is never allowed to fall below 0, or to exceed the shorter of the two values
 
\begin_inset Formula $(x_{i}^{N},x_{i}^{V})$
\end_inset


\end_layout

\begin_layout Itemize
The duration of 
\begin_inset Formula $x_{i}^{V}$
\end_inset

 is never allowed to fall below 50 ms, or to exceed 600 ms
\end_layout

\begin_layout Itemize
The duration of 
\begin_inset Formula $x_{i}^{N}$
\end_inset

 is never allowed to fall below 25 ms, or to exceed 500 ms
\end_layout

\begin_layout Section
No-Phoneme Model
\end_layout

\begin_layout Standard
The fluency attractor affects overlap duration according to the following
 formula:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
x_{i}^{O^{\prime}}=x_{i}^{O}+\beta(T-x_{i}^{O})\label{eq:Frequency attractor-1}
\end{equation}

\end_inset

The target overlap duration for these simulations is set at 
\begin_inset Formula $T=x_{i}^{N}$
\end_inset

.
 
\begin_inset Formula $\beta$
\end_inset

 parameterizes frequency on a scale between 0 and 1.
 
\end_layout

\begin_layout Section
Multiple-Parse Model
\end_layout

\begin_layout Itemize
Resting activation acts as a perturbation to the expansion force, 
\emph on
E
\emph default
.
 The mean of the expansion function is shifted 
\begin_inset Formula $\frac{1}{4}$
\end_inset

 of a standard deviation for each unit of 
\emph on
f
\emph default
, where f parameterizes frequency:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\bar{E}^{\prime}=\bar{E}-f(.25\sigma_{E})
\]

\end_inset


\end_layout

\begin_layout Itemize
The overlap duration for Analysis 2 tokens is a random variable distributed
 according to 
\begin_inset Formula $\mathcal{\mathscr{N}\left(\mathrm{.25\bar{x}^{N},\sigma_{x^{N}}}\right)}$
\end_inset


\end_layout

\begin_layout Itemize
The probability of Analysis 1 is given by:
\end_layout

\begin_deeper
\begin_layout Standard
\begin_inset Formula 
\begin{equation}
P(a=1)=Ae^{-b(1-Q)}-C\label{eq:segmentation-1-1}
\end{equation}

\end_inset

where 
\begin_inset Formula $Q=\frac{x_{i}^{O}}{x_{i}^{N}}$
\end_inset


\emph on
 
\emph default
.
 For all simulations, the constants are set to: 
\begin_inset Formula $A=1$
\end_inset

, 
\begin_inset Formula $b=2$
\end_inset

, and 
\begin_inset Formula $C=0$
\end_inset


\end_layout

\end_deeper
\end_body
\end_document
